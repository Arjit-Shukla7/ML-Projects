{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d56a2f3c",
   "metadata": {},
   "source": [
    "## **Setting Up the environment and basic understanding of python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a8c8947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (1.1.7)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.6 in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from langchain-openai) (1.2.7)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.109.1 in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from langchain-openai) (2.15.0)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.6.2)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (25.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (2.12.5)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.13.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.6->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.25.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (4.12.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.16.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain-openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.4.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (2.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\arjit shukla\\desktop\\iit\\ml books\\.venv\\lib\\site-packages (from tqdm>4->openai<3.0.0,>=1.109.1->langchain-openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb37b83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key loaded successfully\n",
      "<class 'os._Environ'>\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "if os.environ.get(\"GROQ_API_KEY\"):\n",
    "    print(\"API Key loaded successfully\")\n",
    "else:\n",
    "    print(\"API Key not found\")\n",
    "\n",
    "print(type(os.environ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fb989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"qwen/qwen3-32b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa7330a",
   "metadata": {},
   "source": [
    "When ever i see any new object i need to ask 5 questions\\\n",
    "1.What is this object?\\\n",
    "2.What can this object do?\\\n",
    "3.How do i talk to it?\\\n",
    "4.What these constructor parameters actually do?\\\n",
    "5.What is not obvious from the signature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "302d368f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_groq.chat_models.ChatGroq'>\n"
     ]
    }
   ],
   "source": [
    "print(type(llm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d245fb14",
   "metadata": {},
   "source": [
    "So answer of 1st question is , it is a class inheritance of langchain_groq.chat_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4a979cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['InputType', 'OutputType', '__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__orig_bases__', '__parameters__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_on_complete__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_setattr_handlers__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__ror__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abatch_with_config', '_abc_impl', '_acall_with_config', '_agenerate', '_agenerate_with_cache', '_astream', '_atransform_stream_with_config', '_batch_with_config', '_calculate_keys', '_call_async', '_call_with_config', '_combine_llm_outputs', '_convert_cached_generations', '_convert_input', '_copy_and_set_values', '_create_chat_result', '_create_message_dicts', '_default_params', '_generate', '_generate_with_cache', '_get_invocation_params', '_get_llm_string', '_get_ls_params', '_get_value', '_identifying_params', '_iter', '_llm_type', '_serialized', '_set_model_profile', '_setattr_handler', '_should_stream', '_stream', '_transform_stream_with_config', 'abatch', 'abatch_as_completed', 'agenerate', 'agenerate_prompt', 'ainvoke', 'as_tool', 'assign', 'astream', 'astream_events', 'astream_log', 'async_client', 'atransform', 'batch', 'batch_as_completed', 'bind', 'bind_tools', 'build_extra', 'cache', 'callbacks', 'client', 'config_schema', 'config_specs', 'configurable_alternatives', 'configurable_fields', 'construct', 'copy', 'custom_get_token_ids', 'default_headers', 'default_query', 'dict', 'disable_streaming', 'from_orm', 'generate', 'generate_prompt', 'get_config_jsonschema', 'get_graph', 'get_input_jsonschema', 'get_input_schema', 'get_lc_namespace', 'get_name', 'get_num_tokens', 'get_num_tokens_from_messages', 'get_output_jsonschema', 'get_output_schema', 'get_prompts', 'get_token_ids', 'groq_api_base', 'groq_api_key', 'groq_proxy', 'http_async_client', 'http_client', 'input_schema', 'invoke', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'map', 'max_retries', 'max_tokens', 'metadata', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_kwargs', 'model_name', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'n', 'name', 'output_schema', 'output_version', 'parse_file', 'parse_obj', 'parse_raw', 'pick', 'pipe', 'profile', 'rate_limiter', 'reasoning_effort', 'reasoning_format', 'request_timeout', 'schema', 'schema_json', 'service_tier', 'set_verbose', 'stop', 'stream', 'streaming', 'tags', 'temperature', 'to_json', 'to_json_not_implemented', 'transform', 'update_forward_refs', 'validate', 'validate_environment', 'verbose', 'with_alisteners', 'with_config', 'with_fallbacks', 'with_listeners', 'with_retry', 'with_structured_output', 'with_types']\n"
     ]
    }
   ],
   "source": [
    "print(dir(llm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f27bc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on ChatGroq in module langchain_groq.chat_models object:\n",
      "\n",
      "class ChatGroq(langchain_core.language_models.chat_models.BaseChatModel)\n",
      " |  ChatGroq(*args: Any, name: str | None = None, cache: langchain_core.caches.BaseCache | bool | None = None, verbose: bool = <factory>, callbacks: list[langchain_core.callbacks.base.BaseCallbackHandler] | langchain_core.callbacks.base.BaseCallbackManager | None = None, tags: list[str] | None = None, metadata: dict[str, typing.Any] | None = None, custom_get_token_ids: collections.abc.Callable[[str], list[int]] | None = None, rate_limiter: langchain_core.rate_limiters.BaseRateLimiter | None = None, disable_streaming: Union[bool, Literal['tool_calling']] = False, output_version: str | None = <factory>, profile: langchain_core.language_models.model_profile.ModelProfile | None = None, client: Any = None, async_client: Any = None, model: str, temperature: float = 0.7, stop_sequences: list[str] | str | None = None, reasoning_format: Optional[Literal['parsed', 'raw', 'hidden']] = None, reasoning_effort: str | None = None, model_kwargs: dict[str, typing.Any] = <factory>, api_key: pydantic.types.SecretStr | None = <factory>, base_url: str | None = <factory>, groq_proxy: str | None = <factory>, timeout: float | tuple[float, float] | typing.Any | None = None, max_retries: int = 2, streaming: bool = False, n: int = 1, max_tokens: int | None = None, service_tier: Literal['on_demand', 'flex', 'auto'] = 'on_demand', default_headers: collections.abc.Mapping[str, str] | None = None, default_query: collections.abc.Mapping[str, object] | None = None, http_client: typing.Any | None = None, http_async_client: typing.Any | None = None) -> None\n",
      " |\n",
      " |  Groq Chat large language models API.\n",
      " |\n",
      " |  To use, you should have the\n",
      " |  environment variable `GROQ_API_KEY` set with your API key.\n",
      " |\n",
      " |  Any parameters that are valid to be passed to the groq.create call\n",
      " |  can be passed in, even if not explicitly saved on this class.\n",
      " |\n",
      " |  Setup:\n",
      " |      Install `langchain-groq` and set environment variable\n",
      " |      `GROQ_API_KEY`.\n",
      " |\n",
      " |      ```bash\n",
      " |      pip install -U langchain-groq\n",
      " |      export GROQ_API_KEY=\"your-api-key\"\n",
      " |      ```\n",
      " |\n",
      " |  Key init args — completion params:\n",
      " |      model:\n",
      " |          Name of Groq model to use, e.g. `llama-3.1-8b-instant`.\n",
      " |      temperature:\n",
      " |          Sampling temperature. Ranges from `0.0` to `1.0`.\n",
      " |      max_tokens:\n",
      " |          Max number of tokens to generate.\n",
      " |      reasoning_format:\n",
      " |          The format for reasoning output. Groq will default to `raw` if left\n",
      " |          undefined.\n",
      " |\n",
      " |          - `'parsed'`: Separates reasoning into a dedicated field while keeping the\n",
      " |              response concise. Reasoning will be returned in the\n",
      " |              `additional_kwargs.reasoning_content` field of the response.\n",
      " |          - `'raw'`: Includes reasoning within think tags (e.g.\n",
      " |              `<think>{reasoning_content}</think>`).\n",
      " |          - `'hidden'`: Returns only the final answer content. Note: this only\n",
      " |              suppresses reasoning content in the response; the model will still perform\n",
      " |              reasoning unless overridden in `reasoning_effort`.\n",
      " |\n",
      " |          See the [Groq documentation](https://console.groq.com/docs/reasoning#reasoning)\n",
      " |          for more details and a list of supported models.\n",
      " |      model_kwargs:\n",
      " |          Holds any model parameters valid for create call not\n",
      " |          explicitly specified.\n",
      " |\n",
      " |  Key init args — client params:\n",
      " |      timeout:\n",
      " |          Timeout for requests.\n",
      " |      max_retries:\n",
      " |          Max number of retries.\n",
      " |      api_key:\n",
      " |          Groq API key. If not passed in will be read from env var `GROQ_API_KEY`.\n",
      " |      base_url:\n",
      " |          Base URL path for API requests, leave blank if not using a proxy\n",
      " |          or service emulator.\n",
      " |      custom_get_token_ids:\n",
      " |          Optional encoder to use for counting tokens.\n",
      " |\n",
      " |  See full list of supported init args and their descriptions in the params\n",
      " |  section.\n",
      " |\n",
      " |  Instantiate:\n",
      " |      ```python\n",
      " |      from langchain_groq import ChatGroq\n",
      " |\n",
      " |      model = ChatGroq(\n",
      " |          model=\"llama-3.1-8b-instant\",\n",
      " |          temperature=0.0,\n",
      " |          max_retries=2,\n",
      " |          # other params...\n",
      " |      )\n",
      " |      ```\n",
      " |\n",
      " |  Invoke:\n",
      " |      ```python\n",
      " |      messages = [\n",
      " |          (\"system\", \"You are a helpful translator. Translate the user sentence to French.\"),\n",
      " |          (\"human\", \"I love programming.\"),\n",
      " |      ]\n",
      " |      model.invoke(messages)\n",
      " |      ```\n",
      " |      ```python\n",
      " |      AIMessage(content='The English sentence \"I love programming\" can\n",
      " |      be translated to French as \"J\\'aime programmer\". The word\n",
      " |      \"programming\" is translated as \"programmer\" in French.',\n",
      " |      response_metadata={'token_usage': {'completion_tokens': 38,\n",
      " |      'prompt_tokens': 28, 'total_tokens': 66, 'completion_time':\n",
      " |      0.057975474, 'prompt_time': 0.005366091, 'queue_time': None,\n",
      " |      'total_time': 0.063341565}, 'model_name': 'llama-3.1-8b-instant',\n",
      " |      'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop',\n",
      " |      'logprobs': None}, id='run-ecc71d70-e10c-4b69-8b8c-b8027d95d4b8-0')\n",
      " |      ```\n",
      " |\n",
      " |  Stream:\n",
      " |      ```python\n",
      " |      # Streaming `text` for each content chunk received\n",
      " |      for chunk in model.stream(messages):\n",
      " |          print(chunk.text, end=\"\")\n",
      " |      ```\n",
      " |\n",
      " |      ```python\n",
      " |      content='' id='run-4e9f926b-73f5-483b-8ef5-09533d925853'\n",
      " |      content='The' id='run-4e9f926b-73f5-483b-8ef5-09533d925853'\n",
      " |      content=' English' id='run-4e9f926b-73f5-483b-8ef5-09533d925853'\n",
      " |      content=' sentence' id='run-4e9f926b-73f5-483b-8ef5-09533d925853'\n",
      " |      ...\n",
      " |      content=' program' id='run-4e9f926b-73f5-483b-8ef5-09533d925853'\n",
      " |      content='\".' id='run-4e9f926b-73f5-483b-8ef5-09533d925853'\n",
      " |      content='' response_metadata={'finish_reason': 'stop'}\n",
      " |      id='run-4e9f926b-73f5-483b-8ef5-09533d925853\n",
      " |      ```\n",
      " |\n",
      " |      ```python\n",
      " |      # Reconstructing a full response\n",
      " |      stream = model.stream(messages)\n",
      " |      full = next(stream)\n",
      " |      for chunk in stream:\n",
      " |          full += chunk\n",
      " |      full\n",
      " |      ```\n",
      " |\n",
      " |      ```python\n",
      " |      AIMessageChunk(content='The English sentence \"I love programming\"\n",
      " |      can be translated to French as \"J\\'aime programmer\". Here\\'s the\n",
      " |      breakdown of the sentence: \"J\\'aime\" is the French equivalent of \"\n",
      " |      I love\", and \"programmer\" is the French infinitive for \"to program\".\n",
      " |      So, the literal translation is \"I love to program\". However, in\n",
      " |      English we often omit the \"to\" when talking about activities we\n",
      " |      love, and the same applies to French. Therefore, \"J\\'aime\n",
      " |      programmer\" is the correct and natural way to express \"I love\n",
      " |      programming\" in French.', response_metadata={'finish_reason':\n",
      " |      'stop'}, id='run-a3c35ac4-0750-4d08-ac55-bfc63805de76')\n",
      " |      ```\n",
      " |\n",
      " |  Async:\n",
      " |      ```python\n",
      " |      await model.ainvoke(messages)\n",
      " |      ```\n",
      " |\n",
      " |      ```python\n",
      " |      AIMessage(content='The English sentence \"I love programming\" can\n",
      " |      be translated to French as \"J\\'aime programmer\". The word\n",
      " |      \"programming\" is translated as \"programmer\" in French. I hope\n",
      " |      this helps! Let me know if you have any other questions.',\n",
      " |      response_metadata={'token_usage': {'completion_tokens': 53,\n",
      " |      'prompt_tokens': 28, 'total_tokens': 81, 'completion_time':\n",
      " |      0.083623752, 'prompt_time': 0.007365126, 'queue_time': None,\n",
      " |      'total_time': 0.090988878}, 'model_name': 'llama-3.1-8b-instant',\n",
      " |      'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop',\n",
      " |      'logprobs': None}, id='run-897f3391-1bea-42e2-82e0-686e2367bcf8-0')\n",
      " |      ```\n",
      " |\n",
      " |  Tool calling:\n",
      " |      ```python\n",
      " |      from pydantic import BaseModel, Field\n",
      " |\n",
      " |\n",
      " |      class GetWeather(BaseModel):\n",
      " |          '''Get the current weather in a given location'''\n",
      " |\n",
      " |          location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
      " |\n",
      " |\n",
      " |      class GetPopulation(BaseModel):\n",
      " |          '''Get the current population in a given location'''\n",
      " |\n",
      " |          location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
      " |\n",
      " |\n",
      " |      model_with_tools = model.bind_tools([GetWeather, GetPopulation])\n",
      " |      ai_msg = model_with_tools.invoke(\"What is the population of NY?\")\n",
      " |      ai_msg.tool_calls\n",
      " |      ```\n",
      " |\n",
      " |      ```python\n",
      " |      [\n",
      " |          {\n",
      " |              \"name\": \"GetPopulation\",\n",
      " |              \"args\": {\"location\": \"NY\"},\n",
      " |              \"id\": \"call_bb8d\",\n",
      " |          }\n",
      " |      ]\n",
      " |      ```\n",
      " |\n",
      " |      See `ChatGroq.bind_tools()` method for more.\n",
      " |\n",
      " |  Structured output:\n",
      " |      ```python\n",
      " |      from typing import Optional\n",
      " |\n",
      " |      from pydantic import BaseModel, Field\n",
      " |\n",
      " |\n",
      " |      class Joke(BaseModel):\n",
      " |          '''Joke to tell user.'''\n",
      " |\n",
      " |          setup: str = Field(description=\"The setup of the joke\")\n",
      " |          punchline: str = Field(description=\"The punchline to the joke\")\n",
      " |          rating: int | None = Field(description=\"How funny the joke is, from 1 to 10\")\n",
      " |\n",
      " |\n",
      " |      structured_model = model.with_structured_output(Joke)\n",
      " |      structured_model.invoke(\"Tell me a joke about cats\")\n",
      " |      ```\n",
      " |\n",
      " |      ```python\n",
      " |      Joke(\n",
      " |          setup=\"Why don't cats play poker in the jungle?\",\n",
      " |          punchline=\"Too many cheetahs!\",\n",
      " |          rating=None,\n",
      " |      )\n",
      " |      ```\n",
      " |\n",
      " |      See `ChatGroq.with_structured_output()` for more.\n",
      " |\n",
      " |  Response metadata:\n",
      " |      ```python\n",
      " |      ai_msg = model.invoke(messages)\n",
      " |      ai_msg.response_metadata\n",
      " |      ```\n",
      " |\n",
      " |      ```python\n",
      " |      {\n",
      " |          \"token_usage\": {\n",
      " |              \"completion_tokens\": 70,\n",
      " |              \"prompt_tokens\": 28,\n",
      " |              \"total_tokens\": 98,\n",
      " |              \"completion_time\": 0.111956391,\n",
      " |              \"prompt_time\": 0.007518279,\n",
      " |              \"queue_time\": None,\n",
      " |              \"total_time\": 0.11947467,\n",
      " |          },\n",
      " |          \"model_name\": \"llama-3.1-8b-instant\",\n",
      " |          \"system_fingerprint\": \"fp_c5f20b5bb1\",\n",
      " |          \"finish_reason\": \"stop\",\n",
      " |          \"logprobs\": None,\n",
      " |      }\n",
      " |      ```\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      ChatGroq\n",
      " |      langchain_core.language_models.chat_models.BaseChatModel\n",
      " |      langchain_core.language_models.base.BaseLanguageModel[AIMessage]\n",
      " |      langchain_core.language_models.base.BaseLanguageModel\n",
      " |      langchain_core.runnables.base.RunnableSerializable[Union[PromptValue, str, Sequence[Union[BaseMessage, list[str], tuple[str, str], str, dict[str, Any]]]], TypeVar]\n",
      " |      langchain_core.runnables.base.RunnableSerializable\n",
      " |      langchain_core.load.serializable.Serializable\n",
      " |      pydantic.main.BaseModel\n",
      " |      langchain_core.runnables.base.Runnable\n",
      " |      abc.ABC\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  bind_tools(self, tools: 'Sequence[dict[str, Any] | type[BaseModel] | Callable | BaseTool]', *, tool_choice: 'dict | str | bool | None' = None, **kwargs: 'Any') -> 'Runnable[LanguageModelInput, AIMessage]'\n",
      " |      Bind tool-like objects to this chat model.\n",
      " |\n",
      " |      Args:\n",
      " |          tools: A list of tool definitions to bind to this chat model.\n",
      " |\n",
      " |              Supports any tool definition handled by [`convert_to_openai_tool`][langchain_core.utils.function_calling.convert_to_openai_tool].\n",
      " |          tool_choice: Which tool to require the model to call.\n",
      " |              Must be the name of the single provided function,\n",
      " |              `'auto'` to automatically determine which function to call\n",
      " |              with the option to not call any function, `'any'` to enforce that some\n",
      " |              function is called, or a dict of the form:\n",
      " |              `{\"type\": \"function\", \"function\": {\"name\": <<tool_name>>}}`.\n",
      " |          **kwargs: Any additional parameters to pass to the\n",
      " |              `langchain.runnable.Runnable` constructor.\n",
      " |\n",
      " |  validate_environment(self) -> 'Self'\n",
      " |      Validate that api key and python package exists in environment.\n",
      " |\n",
      " |  with_structured_output(self, schema: 'dict | type[BaseModel] | None' = None, *, method: \"Literal['function_calling', 'json_mode', 'json_schema']\" = 'function_calling', include_raw: 'bool' = False, **kwargs: 'Any') -> 'Runnable[LanguageModelInput, dict | BaseModel]'\n",
      " |      Model wrapper that returns outputs formatted to match the given schema.\n",
      " |\n",
      " |      Args:\n",
      " |          schema: The output schema. Can be passed in as:\n",
      " |\n",
      " |              - An OpenAI function/tool schema,\n",
      " |              - A JSON Schema,\n",
      " |              - A `TypedDict` class,\n",
      " |              - Or a Pydantic class.\n",
      " |\n",
      " |              If `schema` is a Pydantic class then the model output will be a\n",
      " |              Pydantic instance of that class, and the model-generated fields will be\n",
      " |              validated by the Pydantic class. Otherwise the model output will be a\n",
      " |              dict and will not be validated.\n",
      " |\n",
      " |              See `langchain_core.utils.function_calling.convert_to_openai_tool` for\n",
      " |              more on how to properly specify types and descriptions of schema fields\n",
      " |              when specifying a Pydantic or `TypedDict` class.\n",
      " |\n",
      " |              !!! warning \"Behavior changed in `langchain-groq` 0.3.8\"\n",
      " |\n",
      " |                  Added support for Groq's dedicated structured output feature via\n",
      " |                  `method=\"json_schema\"`.\n",
      " |\n",
      " |          method: The method for steering model generation, one of:\n",
      " |\n",
      " |              - `'function_calling'`:\n",
      " |                  Uses Groq's tool-calling [API](https://console.groq.com/docs/tool-use)\n",
      " |              - `'json_schema'`:\n",
      " |                  Uses Groq's [Structured Output API](https://console.groq.com/docs/structured-outputs).\n",
      " |                  Supported for a subset of models, including `openai/gpt-oss`,\n",
      " |                  `moonshotai/kimi-k2-instruct-0905`, and some `meta-llama/llama-4`\n",
      " |                  models. See [docs](https://console.groq.com/docs/structured-outputs)\n",
      " |                  for details.\n",
      " |              - `'json_mode'`:\n",
      " |                  Uses Groq's [JSON mode](https://console.groq.com/docs/structured-outputs#json-object-mode).\n",
      " |                  Note that if using JSON mode then you must include instructions for\n",
      " |                  formatting the output into the desired schema into the model call\n",
      " |\n",
      " |              Learn more about the differences between the methods and which models\n",
      " |              support which methods [here](https://console.groq.com/docs/structured-outputs).\n",
      " |\n",
      " |          method:\n",
      " |              The method for steering model generation, either `'function_calling'`\n",
      " |              or `'json_mode'`. If `'function_calling'` then the schema will be converted\n",
      " |              to an OpenAI function and the returned model will make use of the\n",
      " |              function-calling API. If `'json_mode'` then JSON mode will be used.\n",
      " |\n",
      " |              !!! note\n",
      " |                  If using `'json_mode'` then you must include instructions for formatting\n",
      " |                  the output into the desired schema into the model call. (either via the\n",
      " |                  prompt itself or in the system message/prompt/instructions).\n",
      " |\n",
      " |              !!! warning\n",
      " |                  `'json_mode'` does not support streaming responses stop sequences.\n",
      " |\n",
      " |          include_raw:\n",
      " |              If `False` then only the parsed structured output is returned.\n",
      " |\n",
      " |              If an error occurs during model output parsing it will be raised.\n",
      " |\n",
      " |              If `True` then both the raw model response (a `BaseMessage`) and the\n",
      " |              parsed model response will be returned.\n",
      " |\n",
      " |              If an error occurs during output parsing it will be caught and returned\n",
      " |              as well.\n",
      " |\n",
      " |              The final output is always a `dict` with keys `'raw'`, `'parsed'`, and\n",
      " |              `'parsing_error'`.\n",
      " |\n",
      " |          kwargs:\n",
      " |              Any additional parameters to pass to the `langchain.runnable.Runnable`\n",
      " |              constructor.\n",
      " |\n",
      " |      Returns:\n",
      " |          A `Runnable` that takes same inputs as a\n",
      " |              `langchain_core.language_models.chat.BaseChatModel`. If `include_raw` is\n",
      " |              `False` and `schema` is a Pydantic class, `Runnable` outputs an instance\n",
      " |              of `schema` (i.e., a Pydantic object). Otherwise, if `include_raw` is\n",
      " |              `False` then `Runnable` outputs a `dict`.\n",
      " |\n",
      " |              If `include_raw` is `True`, then `Runnable` outputs a `dict` with keys:\n",
      " |\n",
      " |              - `'raw'`: `BaseMessage`\n",
      " |              - `'parsed'`: `None` if there was a parsing error, otherwise the type\n",
      " |                  depends on the `schema` as described above.\n",
      " |              - `'parsing_error'`: `BaseException | None`\n",
      " |\n",
      " |      Example: schema=Pydantic class, method=\"function_calling\", include_raw=False:\n",
      " |\n",
      " |      ```python\n",
      " |      from typing import Optional\n",
      " |\n",
      " |      from langchain_groq import ChatGroq\n",
      " |      from pydantic import BaseModel, Field\n",
      " |\n",
      " |\n",
      " |      class AnswerWithJustification(BaseModel):\n",
      " |          '''An answer to the user question along with justification for the answer.'''\n",
      " |\n",
      " |          answer: str\n",
      " |          # If we provide default values and/or descriptions for fields, these will be passed\n",
      " |          # to the model. This is an important part of improving a model's ability to\n",
      " |          # correctly return structured outputs.\n",
      " |          justification: str | None = Field(default=None, description=\"A justification for the answer.\")\n",
      " |\n",
      " |\n",
      " |      model = ChatGroq(model=\"openai/gpt-oss-120b\", temperature=0)\n",
      " |      structured_model = model.with_structured_output(AnswerWithJustification)\n",
      " |\n",
      " |      structured_model.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n",
      " |\n",
      " |      # -> AnswerWithJustification(\n",
      " |      #     answer='They weigh the same',\n",
      " |      #     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'\n",
      " |      # )\n",
      " |      ```\n",
      " |\n",
      " |      Example: schema=Pydantic class, method=\"function_calling\", include_raw=True:\n",
      " |\n",
      " |      ```python\n",
      " |      from langchain_groq import ChatGroq\n",
      " |      from pydantic import BaseModel\n",
      " |\n",
      " |\n",
      " |      class AnswerWithJustification(BaseModel):\n",
      " |          '''An answer to the user question along with justification for the answer.'''\n",
      " |\n",
      " |          answer: str\n",
      " |          justification: str\n",
      " |\n",
      " |\n",
      " |      model = ChatGroq(model=\"openai/gpt-oss-120b\", temperature=0)\n",
      " |      structured_model = model.with_structured_output(\n",
      " |          AnswerWithJustification,\n",
      " |          include_raw=True,\n",
      " |      )\n",
      " |\n",
      " |      structured_model.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n",
      " |      # -> {\n",
      " |      #     'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Ao02pnFYXD6GN1yzc0uXPsvF', 'function': {'arguments': '{\"answer\":\"They weigh the same.\",\"justification\":\"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.\"}', 'name': 'AnswerWithJustification'}, 'type': 'function'}]}),\n",
      " |      #     'parsed': AnswerWithJustification(answer='They weigh the same.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'),\n",
      " |      #     'parsing_error': None\n",
      " |      # }\n",
      " |      ```\n",
      " |\n",
      " |      Example: schema=TypedDict class, method=\"function_calling\", include_raw=False:\n",
      " |\n",
      " |      ```python\n",
      " |      from typing_extensions import Annotated, TypedDict\n",
      " |\n",
      " |      from langchain_groq import ChatGroq\n",
      " |\n",
      " |\n",
      " |      class AnswerWithJustification(TypedDict):\n",
      " |          '''An answer to the user question along with justification for the answer.'''\n",
      " |\n",
      " |          answer: str\n",
      " |          justification: Annotated[str | None, None, \"A justification for the answer.\"]\n",
      " |\n",
      " |\n",
      " |      model = ChatGroq(model=\"openai/gpt-oss-120b\", temperature=0)\n",
      " |      structured_model = model.with_structured_output(AnswerWithJustification)\n",
      " |\n",
      " |      structured_model.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n",
      " |      # -> {\n",
      " |      #     'answer': 'They weigh the same',\n",
      " |      #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'\n",
      " |      # }\n",
      " |      ```\n",
      " |\n",
      " |      Example: schema=OpenAI function schema, method=\"function_calling\", include_raw=False:\n",
      " |\n",
      " |      ```python\n",
      " |      from langchain_groq import ChatGroq\n",
      " |\n",
      " |      oai_schema = {\n",
      " |          'name': 'AnswerWithJustification',\n",
      " |          'description': 'An answer to the user question along with justification for the answer.',\n",
      " |          'parameters': {\n",
      " |              'type': 'object',\n",
      " |              'properties': {\n",
      " |                  'answer': {'type': 'string'},\n",
      " |                  'justification': {'description': 'A justification for the answer.', 'type': 'string'}\n",
      " |              },\n",
      " |              'required': ['answer']\n",
      " |          }\n",
      " |\n",
      " |          model = ChatGroq(model=\"openai/gpt-oss-120b\", temperature=0)\n",
      " |          structured_model = model.with_structured_output(oai_schema)\n",
      " |\n",
      " |          structured_model.invoke(\n",
      " |              \"What weighs more a pound of bricks or a pound of feathers\"\n",
      " |          )\n",
      " |          # -> {\n",
      " |          #     'answer': 'They weigh the same',\n",
      " |          #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'\n",
      " |          # }\n",
      " |      ```\n",
      " |\n",
      " |      Example: schema=Pydantic class, method=\"json_schema\", include_raw=False:\n",
      " |\n",
      " |      ```python\n",
      " |      from typing import Optional\n",
      " |\n",
      " |      from langchain_groq import ChatGroq\n",
      " |      from pydantic import BaseModel, Field\n",
      " |\n",
      " |\n",
      " |      class AnswerWithJustification(BaseModel):\n",
      " |          '''An answer to the user question along with justification for the answer.'''\n",
      " |\n",
      " |          answer: str\n",
      " |          # If we provide default values and/or descriptions for fields, these will be passed\n",
      " |          # to the model. This is an important part of improving a model's ability to\n",
      " |          # correctly return structured outputs.\n",
      " |          justification: str | None = Field(default=None, description=\"A justification for the answer.\")\n",
      " |\n",
      " |\n",
      " |      model = ChatGroq(model=\"openai/gpt-oss-120b\", temperature=0)\n",
      " |      structured_model = model.with_structured_output(\n",
      " |          AnswerWithJustification,\n",
      " |          method=\"json_schema\",\n",
      " |      )\n",
      " |\n",
      " |      structured_model.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n",
      " |\n",
      " |      # -> AnswerWithJustification(\n",
      " |      #     answer='They weigh the same',\n",
      " |      #     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'\n",
      " |      # )\n",
      " |      ```\n",
      " |\n",
      " |      Example: schema=Pydantic class, method=\"json_mode\", include_raw=True:\n",
      " |\n",
      " |      ```python\n",
      " |      from langchain_groq import ChatGroq\n",
      " |      from pydantic import BaseModel\n",
      " |\n",
      " |\n",
      " |      class AnswerWithJustification(BaseModel):\n",
      " |          answer: str\n",
      " |          justification: str\n",
      " |\n",
      " |\n",
      " |      model = ChatGroq(model=\"openai/gpt-oss-120b\", temperature=0)\n",
      " |      structured_model = model.with_structured_output(\n",
      " |          AnswerWithJustification, method=\"json_mode\", include_raw=True\n",
      " |      )\n",
      " |\n",
      " |      structured_model.invoke(\n",
      " |          \"Answer the following question. \"\n",
      " |          \"Make sure to return a JSON blob with keys 'answer' and 'justification'.\\n\\n\"\n",
      " |          \"What's heavier a pound of bricks or a pound of feathers?\"\n",
      " |      )\n",
      " |      # -> {\n",
      " |      #     'raw': AIMessage(content='{\\n    \"answer\": \"They are both the same weight.\",\\n    \"justification\": \"Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.\" \\n}'),\n",
      " |      #     'parsed': AnswerWithJustification(answer='They are both the same weight.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.'),\n",
      " |      #     'parsing_error': None\n",
      " |      # }\n",
      " |      ```\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |\n",
      " |  build_extra(values: 'dict[str, Any]') -> 'Any'\n",
      " |      Build extra kwargs from additional params that were passed in.\n",
      " |\n",
      " |  is_lc_serializable() -> 'bool'\n",
      " |      Return whether this model can be serialized by LangChain.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |\n",
      " |  lc_secrets\n",
      " |      Mapping of secret environment variables.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      " |\n",
      " |  __annotations__ = {'async_client': 'Any', 'client': 'Any', 'default_he...\n",
      " |\n",
      " |  __class_vars__ = set()\n",
      " |\n",
      " |  __parameters__ = ()\n",
      " |\n",
      " |  __private_attributes__ = {}\n",
      " |\n",
      " |  __pydantic_complete__ = True\n",
      " |\n",
      " |  __pydantic_computed_fields__ = {}\n",
      " |\n",
      " |  __pydantic_core_schema__ = {'function': {'function': <function ChatGro...\n",
      " |\n",
      " |  __pydantic_custom_init__ = True\n",
      " |\n",
      " |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      " |\n",
      " |  __pydantic_fields__ = {'async_client': FieldInfo(annotation=Any, requi...\n",
      " |\n",
      " |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      " |\n",
      " |  __pydantic_parent_namespace__ = None\n",
      " |\n",
      " |  __pydantic_post_init__ = None\n",
      " |\n",
      " |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      " |      Model...\n",
      " |\n",
      " |  __pydantic_setattr_handlers__ = {'async_client': <function _model_fiel...\n",
      " |\n",
      " |  __pydantic_validator__ = SchemaValidator(title=\"ChatGroq\", validator=F...\n",
      " |\n",
      " |  __signature__ = <Signature (*args: Any, name: str | None = None,..._as...\n",
      " |\n",
      " |  model_config = {'arbitrary_types_allowed': True, 'extra': 'ignore', 'p...\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
      " |\n",
      " |  async agenerate(self, messages: 'list[list[BaseMessage]]', stop: 'list[str] | None' = None, callbacks: 'Callbacks' = None, *, tags: 'list[str] | None' = None, metadata: 'dict[str, Any] | None' = None, run_name: 'str | None' = None, run_id: 'uuid.UUID | None' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Asynchronously pass a sequence of prompts to a model and return generations.\n",
      " |\n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |\n",
      " |      Use this method when you want to:\n",
      " |\n",
      " |      1. Take advantage of batched calls,\n",
      " |      2. Need more output from the model than just the top generated value,\n",
      " |      3. Are building chains that are agnostic to the underlying language model\n",
      " |          type (e.g., pure text completion models vs chat models).\n",
      " |\n",
      " |      Args:\n",
      " |          messages: List of list of messages.\n",
      " |          stop: Stop words to use when generating.\n",
      " |\n",
      " |              Model output is cut off at the first occurrence of any of these\n",
      " |              substrings.\n",
      " |          callbacks: `Callbacks` to pass through.\n",
      " |\n",
      " |              Used for executing additional functionality, such as logging or\n",
      " |              streaming, throughout generation.\n",
      " |          tags: The tags to apply.\n",
      " |          metadata: The metadata to apply.\n",
      " |          run_name: The name of the run.\n",
      " |          run_id: The ID of the run.\n",
      " |          **kwargs: Arbitrary additional keyword arguments.\n",
      " |\n",
      " |              These are usually passed to the model provider API call.\n",
      " |\n",
      " |      Returns:\n",
      " |          An `LLMResult`, which contains a list of candidate `Generations` for each\n",
      " |              input prompt and additional model provider-specific output.\n",
      " |\n",
      " |  async agenerate_prompt(self, prompts: 'list[PromptValue]', stop: 'list[str] | None' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Asynchronously pass a sequence of prompts and return model generations.\n",
      " |\n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |\n",
      " |      Use this method when you want to:\n",
      " |\n",
      " |      1. Take advantage of batched calls,\n",
      " |      2. Need more output from the model than just the top generated value,\n",
      " |      3. Are building chains that are agnostic to the underlying language model\n",
      " |          type (e.g., pure text completion models vs chat models).\n",
      " |\n",
      " |      Args:\n",
      " |          prompts: List of `PromptValue` objects.\n",
      " |\n",
      " |              A `PromptValue` is an object that can be converted to match the format\n",
      " |              of any language model (string for pure text generation models and\n",
      " |              `BaseMessage` objects for chat models).\n",
      " |          stop: Stop words to use when generating.\n",
      " |\n",
      " |              Model output is cut off at the first occurrence of any of these\n",
      " |              substrings.\n",
      " |          callbacks: `Callbacks` to pass through.\n",
      " |\n",
      " |              Used for executing additional functionality, such as logging or\n",
      " |              streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments.\n",
      " |\n",
      " |              These are usually passed to the model provider API call.\n",
      " |\n",
      " |      Returns:\n",
      " |          An `LLMResult`, which contains a list of candidate `Generation` objects for\n",
      " |              each input prompt and additional model provider-specific output.\n",
      " |\n",
      " |  async ainvoke(self, input: 'LanguageModelInput', config: 'RunnableConfig | None' = None, *, stop: 'list[str] | None' = None, **kwargs: 'Any') -> 'AIMessage'\n",
      " |      Transform a single input into an output.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the `Runnable`.\n",
      " |          config: A config to use when invoking the `Runnable`.\n",
      " |\n",
      " |              The config supports standard keys like `'tags'`, `'metadata'` for\n",
      " |              tracing purposes, `'max_concurrency'` for controlling how much work to\n",
      " |              do in parallel, and other keys.\n",
      " |\n",
      " |              Please refer to `RunnableConfig` for more details.\n",
      " |\n",
      " |      Returns:\n",
      " |          The output of the `Runnable`.\n",
      " |\n",
      " |  async astream(self, input: 'LanguageModelInput', config: 'RunnableConfig | None' = None, *, stop: 'list[str] | None' = None, **kwargs: 'Any') -> 'AsyncIterator[AIMessageChunk]'\n",
      " |      Default implementation of `astream`, which calls `ainvoke`.\n",
      " |\n",
      " |      Subclasses must override this method if they support streaming output.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the `Runnable`.\n",
      " |          config: The config to use for the `Runnable`.\n",
      " |          **kwargs: Additional keyword arguments to pass to the `Runnable`.\n",
      " |\n",
      " |      Yields:\n",
      " |          The output of the `Runnable`.\n",
      " |\n",
      " |  dict(self, **kwargs: 'Any') -> 'dict'\n",
      " |      Return a dictionary of the LLM.\n",
      " |\n",
      " |  generate(self, messages: 'list[list[BaseMessage]]', stop: 'list[str] | None' = None, callbacks: 'Callbacks' = None, *, tags: 'list[str] | None' = None, metadata: 'dict[str, Any] | None' = None, run_name: 'str | None' = None, run_id: 'uuid.UUID | None' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Pass a sequence of prompts to the model and return model generations.\n",
      " |\n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |\n",
      " |      Use this method when you want to:\n",
      " |\n",
      " |      1. Take advantage of batched calls,\n",
      " |      2. Need more output from the model than just the top generated value,\n",
      " |      3. Are building chains that are agnostic to the underlying language model\n",
      " |          type (e.g., pure text completion models vs chat models).\n",
      " |\n",
      " |      Args:\n",
      " |          messages: List of list of messages.\n",
      " |          stop: Stop words to use when generating.\n",
      " |\n",
      " |              Model output is cut off at the first occurrence of any of these\n",
      " |              substrings.\n",
      " |          callbacks: `Callbacks` to pass through.\n",
      " |\n",
      " |              Used for executing additional functionality, such as logging or\n",
      " |              streaming, throughout generation.\n",
      " |          tags: The tags to apply.\n",
      " |          metadata: The metadata to apply.\n",
      " |          run_name: The name of the run.\n",
      " |          run_id: The ID of the run.\n",
      " |          **kwargs: Arbitrary additional keyword arguments.\n",
      " |\n",
      " |              These are usually passed to the model provider API call.\n",
      " |\n",
      " |      Returns:\n",
      " |          An `LLMResult`, which contains a list of candidate `Generations` for each\n",
      " |              input prompt and additional model provider-specific output.\n",
      " |\n",
      " |  generate_prompt(self, prompts: 'list[PromptValue]', stop: 'list[str] | None' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Pass a sequence of prompts to the model and return model generations.\n",
      " |\n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |\n",
      " |      Use this method when you want to:\n",
      " |\n",
      " |      1. Take advantage of batched calls,\n",
      " |      2. Need more output from the model than just the top generated value,\n",
      " |      3. Are building chains that are agnostic to the underlying language model\n",
      " |          type (e.g., pure text completion models vs chat models).\n",
      " |\n",
      " |      Args:\n",
      " |          prompts: List of `PromptValue` objects.\n",
      " |\n",
      " |              A `PromptValue` is an object that can be converted to match the format\n",
      " |              of any language model (string for pure text generation models and\n",
      " |              `BaseMessage` objects for chat models).\n",
      " |          stop: Stop words to use when generating.\n",
      " |\n",
      " |              Model output is cut off at the first occurrence of any of these\n",
      " |              substrings.\n",
      " |          callbacks: `Callbacks` to pass through.\n",
      " |\n",
      " |              Used for executing additional functionality, such as logging or\n",
      " |              streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments.\n",
      " |\n",
      " |              These are usually passed to the model provider API call.\n",
      " |\n",
      " |      Returns:\n",
      " |          An `LLMResult`, which contains a list of candidate `Generation` objects for\n",
      " |              each input prompt and additional model provider-specific output.\n",
      " |\n",
      " |  invoke(self, input: 'LanguageModelInput', config: 'RunnableConfig | None' = None, *, stop: 'list[str] | None' = None, **kwargs: 'Any') -> 'AIMessage'\n",
      " |      Transform a single input into an output.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the `Runnable`.\n",
      " |          config: A config to use when invoking the `Runnable`.\n",
      " |\n",
      " |              The config supports standard keys like `'tags'`, `'metadata'` for\n",
      " |              tracing purposes, `'max_concurrency'` for controlling how much work to\n",
      " |              do in parallel, and other keys.\n",
      " |\n",
      " |              Please refer to `RunnableConfig` for more details.\n",
      " |\n",
      " |      Returns:\n",
      " |          The output of the `Runnable`.\n",
      " |\n",
      " |  stream(self, input: 'LanguageModelInput', config: 'RunnableConfig | None' = None, *, stop: 'list[str] | None' = None, **kwargs: 'Any') -> 'Iterator[AIMessageChunk]'\n",
      " |      Default implementation of `stream`, which calls `invoke`.\n",
      " |\n",
      " |      Subclasses must override this method if they support streaming output.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the `Runnable`.\n",
      " |          config: The config to use for the `Runnable`.\n",
      " |          **kwargs: Additional keyword arguments to pass to the `Runnable`.\n",
      " |\n",
      " |      Yields:\n",
      " |          The output of the `Runnable`.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
      " |\n",
      " |  OutputType\n",
      " |      Get the output type for this `Runnable`.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
      " |\n",
      " |  get_num_tokens(self, text: 'str') -> 'int'\n",
      " |      Get the number of tokens present in the text.\n",
      " |\n",
      " |      Useful for checking if an input fits in a model's context window.\n",
      " |\n",
      " |      This should be overridden by model-specific implementations to provide accurate\n",
      " |      token counts via model-specific tokenizers.\n",
      " |\n",
      " |      Args:\n",
      " |          text: The string input to tokenize.\n",
      " |\n",
      " |      Returns:\n",
      " |          The integer number of tokens in the text.\n",
      " |\n",
      " |  get_num_tokens_from_messages(self, messages: 'list[BaseMessage]', tools: 'Sequence | None' = None) -> 'int'\n",
      " |      Get the number of tokens in the messages.\n",
      " |\n",
      " |      Useful for checking if an input fits in a model's context window.\n",
      " |\n",
      " |      This should be overridden by model-specific implementations to provide accurate\n",
      " |      token counts via model-specific tokenizers.\n",
      " |\n",
      " |      !!! note\n",
      " |\n",
      " |          * The base implementation of `get_num_tokens_from_messages` ignores tool\n",
      " |              schemas.\n",
      " |          * The base implementation of `get_num_tokens_from_messages` adds additional\n",
      " |              prefixes to messages in represent user roles, which will add to the\n",
      " |              overall token count. Model-specific implementations may choose to\n",
      " |              handle this differently.\n",
      " |\n",
      " |      Args:\n",
      " |          messages: The message inputs to tokenize.\n",
      " |          tools: If provided, sequence of dict, `BaseModel`, function, or\n",
      " |              `BaseTool` objects to be converted to tool schemas.\n",
      " |\n",
      " |      Returns:\n",
      " |          The sum of the number of tokens across the messages.\n",
      " |\n",
      " |  get_token_ids(self, text: 'str') -> 'list[int]'\n",
      " |      Return the ordered IDs of the tokens in a text.\n",
      " |\n",
      " |      Args:\n",
      " |          text: The string input to tokenize.\n",
      " |\n",
      " |      Returns:\n",
      " |          A list of IDs corresponding to the tokens in the text, in order they occur\n",
      " |              in the text.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
      " |\n",
      " |  set_verbose(verbose: 'bool | None') -> 'bool'\n",
      " |      If verbose is `None`, set it.\n",
      " |\n",
      " |      This allows users to pass in `None` as verbose to access the global setting.\n",
      " |\n",
      " |      Args:\n",
      " |          verbose: The verbosity setting to use.\n",
      " |\n",
      " |      Returns:\n",
      " |          The verbosity setting to use.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
      " |\n",
      " |  InputType\n",
      " |      Get the input type for this `Runnable`.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.RunnableSerializable:\n",
      " |\n",
      " |  configurable_alternatives(self, which: 'ConfigurableField', *, default_key: 'str' = 'default', prefix_keys: 'bool' = False, **kwargs: 'Runnable[Input, Output] | Callable[[], Runnable[Input, Output]]') -> 'RunnableSerializable[Input, Output]'\n",
      " |      Configure alternatives for `Runnable` objects that can be set at runtime.\n",
      " |\n",
      " |      Args:\n",
      " |          which: The `ConfigurableField` instance that will be used to select the\n",
      " |              alternative.\n",
      " |          default_key: The default key to use if no alternative is selected.\n",
      " |          prefix_keys: Whether to prefix the keys with the `ConfigurableField` id.\n",
      " |          **kwargs: A dictionary of keys to `Runnable` instances or callables that\n",
      " |              return `Runnable` instances.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new `Runnable` with the alternatives configured.\n",
      " |\n",
      " |      !!! example\n",
      " |\n",
      " |          ```python\n",
      " |          from langchain_anthropic import ChatAnthropic\n",
      " |          from langchain_core.runnables.utils import ConfigurableField\n",
      " |          from langchain_openai import ChatOpenAI\n",
      " |\n",
      " |          model = ChatAnthropic(\n",
      " |              model_name=\"claude-sonnet-4-5-20250929\"\n",
      " |          ).configurable_alternatives(\n",
      " |              ConfigurableField(id=\"llm\"),\n",
      " |              default_key=\"anthropic\",\n",
      " |              openai=ChatOpenAI(),\n",
      " |          )\n",
      " |\n",
      " |          # uses the default model ChatAnthropic\n",
      " |          print(model.invoke(\"which organization created you?\").content)\n",
      " |\n",
      " |          # uses ChatOpenAI\n",
      " |          print(\n",
      " |              model.with_config(configurable={\"llm\": \"openai\"})\n",
      " |              .invoke(\"which organization created you?\")\n",
      " |              .content\n",
      " |          )\n",
      " |          ```\n",
      " |\n",
      " |  configurable_fields(self, **kwargs: 'AnyConfigurableField') -> 'RunnableSerializable[Input, Output]'\n",
      " |      Configure particular `Runnable` fields at runtime.\n",
      " |\n",
      " |      Args:\n",
      " |          **kwargs: A dictionary of `ConfigurableField` instances to configure.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValueError: If a configuration key is not found in the `Runnable`.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new `Runnable` with the fields configured.\n",
      " |\n",
      " |      !!! example\n",
      " |\n",
      " |          ```python\n",
      " |          from langchain_core.runnables import ConfigurableField\n",
      " |          from langchain_openai import ChatOpenAI\n",
      " |\n",
      " |          model = ChatOpenAI(max_tokens=20).configurable_fields(\n",
      " |              max_tokens=ConfigurableField(\n",
      " |                  id=\"output_token_number\",\n",
      " |                  name=\"Max tokens in the output\",\n",
      " |                  description=\"The maximum number of tokens in the output\",\n",
      " |              )\n",
      " |          )\n",
      " |\n",
      " |          # max_tokens = 20\n",
      " |          print(\n",
      " |              \"max_tokens_20: \", model.invoke(\"tell me something about chess\").content\n",
      " |          )\n",
      " |\n",
      " |          # max_tokens = 200\n",
      " |          print(\n",
      " |              \"max_tokens_200: \",\n",
      " |              model.with_config(configurable={\"output_token_number\": 200})\n",
      " |              .invoke(\"tell me something about chess\")\n",
      " |              .content,\n",
      " |          )\n",
      " |          ```\n",
      " |\n",
      " |  to_json(self) -> 'SerializedConstructor | SerializedNotImplemented'\n",
      " |      Serialize the `Runnable` to JSON.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON-serializable representation of the `Runnable`.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from langchain_core.runnables.base.RunnableSerializable:\n",
      " |\n",
      " |  __orig_bases__ = (<class 'langchain_core.load.serializable.Serializabl...\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |\n",
      " |  __init__(self, *args: Any, **kwargs: Any) -> None\n",
      " |      # Remove default BaseModel init docstring.\n",
      " |\n",
      " |  __repr_args__(self) -> Any\n",
      " |\n",
      " |  to_json_not_implemented(self) -> langchain_core.load.serializable.SerializedNotImplemented\n",
      " |      Serialize a \"not implemented\" object.\n",
      " |\n",
      " |      Returns:\n",
      " |          `SerializedNotImplemented`.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |\n",
      " |  get_lc_namespace() -> list[str]\n",
      " |      Get the namespace of the LangChain object.\n",
      " |\n",
      " |      For example, if the class is [`langchain.llms.openai.OpenAI`][langchain_openai.OpenAI],\n",
      " |      then the namespace is `[\"langchain\", \"llms\", \"openai\"]`\n",
      " |\n",
      " |      Returns:\n",
      " |          The namespace.\n",
      " |\n",
      " |  lc_id() -> list[str]\n",
      " |      Return a unique identifier for this class for serialization purposes.\n",
      " |\n",
      " |      The unique identifier is a list of strings that describes the path\n",
      " |      to the object.\n",
      " |\n",
      " |      For example, for the class `langchain.llms.openai.OpenAI`, the id is\n",
      " |      `[\"langchain\", \"llms\", \"openai\", \"OpenAI\"]`.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.load.serializable.Serializable:\n",
      " |\n",
      " |  lc_attributes\n",
      " |      List of attribute names that should be included in the serialized kwargs.\n",
      " |\n",
      " |      These attributes must be accepted by the constructor.\n",
      " |\n",
      " |      Default is an empty dictionary.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from langchain_core.load.serializable.Serializable:\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __copy__(self) -> 'Self'\n",
      " |      Returns a shallow copy of the model.\n",
      " |\n",
      " |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      " |      Returns a deep copy of the model.\n",
      " |\n",
      " |  __delattr__(self, item: 'str') -> 'Any'\n",
      " |      Implement delattr(self, name).\n",
      " |\n",
      " |  __eq__(self, other: 'Any') -> 'bool'\n",
      " |      Return self==value.\n",
      " |\n",
      " |  __getattr__(self, item: 'str') -> 'Any'\n",
      " |\n",
      " |  __getstate__(self) -> 'dict[Any, Any]'\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      So `dict(model)` works.\n",
      " |\n",
      " |  __pretty__(self, fmt: 'Callable[[Any], Any]', **kwargs: 'Any') -> 'Generator[Any]' from pydantic._internal._repr.Representation\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      " |\n",
      " |  __replace__(self, **changes: 'Any') -> 'Self'\n",
      " |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
      " |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
      " |\n",
      " |  __repr__(self) -> 'str'\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __repr_name__(self) -> 'str' from pydantic._internal._repr.Representation\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |\n",
      " |  __repr_recursion__(self, object: 'Any') -> 'str' from pydantic._internal._repr.Representation\n",
      " |      Returns the string representation of a recursive object.\n",
      " |\n",
      " |  __repr_str__(self, join_str: 'str') -> 'str' from pydantic._internal._repr.Representation\n",
      " |\n",
      " |  __rich_repr__(self) -> 'RichReprResult' from pydantic._internal._repr.Representation\n",
      " |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      " |\n",
      " |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      " |      Implement setattr(self, name, value).\n",
      " |\n",
      " |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      " |\n",
      " |  __str__(self) -> 'str'\n",
      " |      Return str(self).\n",
      " |\n",
      " |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      " |      Returns a copy of the model.\n",
      " |\n",
      " |      !!! warning \"Deprecated\"\n",
      " |          This method is now deprecated; use `model_copy` instead.\n",
      " |\n",
      " |      If you need `include` or `exclude`, use:\n",
      " |\n",
      " |      ```python {test=\"skip\" lint=\"skip\"}\n",
      " |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      " |      data = {**data, **(update or {})}\n",
      " |      copied = self.model_validate(data)\n",
      " |      ```\n",
      " |\n",
      " |      Args:\n",
      " |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      " |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      " |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      " |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      " |\n",
      " |      Returns:\n",
      " |          A copy of the model with included, excluded and updated fields as specified.\n",
      " |\n",
      " |  json(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
      " |\n",
      " |  model_copy(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_copy`](../concepts/models.md#model-copy)\n",
      " |\n",
      " |      Returns a copy of the model.\n",
      " |\n",
      " |      !!! note\n",
      " |          The underlying instance's [`__dict__`][object.__dict__] attribute is copied. This\n",
      " |          might have unexpected side effects if you store anything in it, on top of the model\n",
      " |          fields (e.g. the value of [cached properties][functools.cached_property]).\n",
      " |\n",
      " |      Args:\n",
      " |          update: Values to change/add in the new model. Note: the data is not validated\n",
      " |              before creating the new model. You should trust this data.\n",
      " |          deep: Set to `True` to make a deep copy of the model.\n",
      " |\n",
      " |      Returns:\n",
      " |          New model instance.\n",
      " |\n",
      " |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, exclude_computed_fields: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, fallback: 'Callable[[Any], Any] | None' = None, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_dump`](../concepts/serialization.md#python-mode)\n",
      " |\n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |\n",
      " |      Args:\n",
      " |          mode: The mode in which `to_python` should run.\n",
      " |              If mode is 'json', the output will only contain JSON serializable types.\n",
      " |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      " |          include: A set of fields to include in the output.\n",
      " |          exclude: A set of fields to exclude from the output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          exclude_computed_fields: Whether to exclude computed fields.\n",
      " |              While this can be useful for round-tripping, it is usually recommended to use the dedicated\n",
      " |              `round_trip` parameter instead.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          fallback: A function to call when an unknown value is encountered. If not provided,\n",
      " |              a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |\n",
      " |      Returns:\n",
      " |          A dictionary representation of the model.\n",
      " |\n",
      " |  model_dump_json(self, *, indent: 'int | None' = None, ensure_ascii: 'bool' = False, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, exclude_computed_fields: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, fallback: 'Callable[[Any], Any] | None' = None, serialize_as_any: 'bool' = False) -> 'str'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_dump_json`](../concepts/serialization.md#json-mode)\n",
      " |\n",
      " |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      " |\n",
      " |      Args:\n",
      " |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      " |          ensure_ascii: If `True`, the output is guaranteed to have all incoming non-ASCII characters escaped.\n",
      " |              If `False` (the default), these characters will be output as-is.\n",
      " |          include: Field(s) to include in the JSON output.\n",
      " |          exclude: Field(s) to exclude from the JSON output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to serialize using field aliases.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          exclude_computed_fields: Whether to exclude computed fields.\n",
      " |              While this can be useful for round-tripping, it is usually recommended to use the dedicated\n",
      " |              `round_trip` parameter instead.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          fallback: A function to call when an unknown value is encountered. If not provided,\n",
      " |              a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON string representation of the model.\n",
      " |\n",
      " |  model_post_init(self, context: 'Any', /) -> 'None'\n",
      " |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      " |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef'\n",
      " |      Parameterizes a generic class.\n",
      " |\n",
      " |      At least, parameterizing a generic class is the *main* thing this\n",
      " |      method does. For example, for some generic class `Foo`, this is called\n",
      " |      when we do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      " |\n",
      " |      However, note that this method is also called when defining generic\n",
      " |      classes in the first place with `class Foo[T]: ...`.\n",
      " |\n",
      " |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema'\n",
      " |\n",
      " |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue'\n",
      " |      Hook into generating the model's JSON schema.\n",
      " |\n",
      " |      Args:\n",
      " |          core_schema: A `pydantic-core` CoreSchema.\n",
      " |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      " |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      " |              or just call the handler with the original schema.\n",
      " |          handler: Call into Pydantic's internal JSON schema generation.\n",
      " |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      " |              generation fails.\n",
      " |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      " |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      " |              for a type.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON schema, as a Python object.\n",
      " |\n",
      " |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None'\n",
      " |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      " |      only after basic class initialization is complete. In particular, attributes like `model_fields` will\n",
      " |      be present when this is called, but forward annotations are not guaranteed to be resolved yet,\n",
      " |      meaning that creating an instance of the class may fail.\n",
      " |\n",
      " |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      " |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      " |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      " |\n",
      " |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      " |      any kwargs passed to the class definition that aren't used internally by Pydantic.\n",
      " |\n",
      " |      Args:\n",
      " |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      " |              by Pydantic.\n",
      " |\n",
      " |      Note:\n",
      " |          You may want to override [`__pydantic_on_complete__()`][pydantic.main.BaseModel.__pydantic_on_complete__]\n",
      " |          instead, which is called once the class and its fields are fully initialized and ready for validation.\n",
      " |\n",
      " |  __pydantic_on_complete__() -> 'None'\n",
      " |      This is called once the class and its fields are fully initialized and ready to be used.\n",
      " |\n",
      " |      This typically happens when the class is created (just before\n",
      " |      [`__pydantic_init_subclass__()`][pydantic.main.BaseModel.__pydantic_init_subclass__] is called on the superclass),\n",
      " |      except when forward annotations are used that could not immediately be resolved.\n",
      " |      In that case, it will be called later, when the model is rebuilt automatically or explicitly using\n",
      " |      [`model_rebuild()`][pydantic.main.BaseModel.model_rebuild].\n",
      " |\n",
      " |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |\n",
      " |  from_orm(obj: 'Any') -> 'Self'\n",
      " |\n",
      " |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |      Creates a new instance of the `Model` class with validated data.\n",
      " |\n",
      " |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |\n",
      " |      !!! note\n",
      " |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      " |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      " |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      " |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      " |          an error if extra values are passed, but they will be ignored.\n",
      " |\n",
      " |      Args:\n",
      " |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      " |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      " |              Otherwise, the field names from the `values` argument will be used.\n",
      " |          values: Trusted or pre-validated data dictionary.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new instance of the `Model` class with validated data.\n",
      " |\n",
      " |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation', *, union_format: \"Literal['any_of', 'primitive_type_array']\" = 'any_of') -> 'dict[str, Any]'\n",
      " |      Generates a JSON schema for a model class.\n",
      " |\n",
      " |      Args:\n",
      " |          by_alias: Whether to use attribute aliases or not.\n",
      " |          ref_template: The reference template.\n",
      " |          union_format: The format to use when combining schemas from unions together. Can be one of:\n",
      " |\n",
      " |              - `'any_of'`: Use the [`anyOf`](https://json-schema.org/understanding-json-schema/reference/combining#anyOf)\n",
      " |              keyword to combine schemas (the default).\n",
      " |              - `'primitive_type_array'`: Use the [`type`](https://json-schema.org/understanding-json-schema/reference/type)\n",
      " |              keyword as an array of strings, containing each type of the combination. If any of the schemas is not a primitive\n",
      " |              type (`string`, `boolean`, `null`, `integer` or `number`) or contains constraints/metadata, falls back to\n",
      " |              `any_of`.\n",
      " |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      " |              `GenerateJsonSchema` with your desired modifications\n",
      " |          mode: The mode in which to generate the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          The JSON schema for the given model class.\n",
      " |\n",
      " |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str'\n",
      " |      Compute the class name for parametrizations of generic classes.\n",
      " |\n",
      " |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      " |\n",
      " |      Args:\n",
      " |          params: Tuple of types of the class. Given a generic class\n",
      " |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      " |              the value `(str, int)` would be passed to `params`.\n",
      " |\n",
      " |      Returns:\n",
      " |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      " |\n",
      " |      Raises:\n",
      " |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      " |\n",
      " |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -> 'bool | None'\n",
      " |      Try to rebuild the pydantic-core schema for the model.\n",
      " |\n",
      " |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      " |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      " |\n",
      " |      Args:\n",
      " |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      " |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      " |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      " |          _types_namespace: The types namespace, defaults to `None`.\n",
      " |\n",
      " |      Returns:\n",
      " |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      " |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      " |\n",
      " |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, extra: 'ExtraValues | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, by_name: 'bool | None' = None) -> 'Self'\n",
      " |      Validate a pydantic model instance.\n",
      " |\n",
      " |      Args:\n",
      " |          obj: The object to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          extra: Whether to ignore, allow, or forbid extra data during model validation.\n",
      " |              See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.\n",
      " |          from_attributes: Whether to extract data from object attributes.\n",
      " |          context: Additional context to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValidationError: If the object could not be validated.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated model instance.\n",
      " |\n",
      " |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, extra: 'ExtraValues | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, by_name: 'bool | None' = None) -> 'Self'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [JSON Parsing](../concepts/json.md#json-parsing)\n",
      " |\n",
      " |      Validate the given JSON data against the Pydantic model.\n",
      " |\n",
      " |      Args:\n",
      " |          json_data: The JSON data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          extra: Whether to ignore, allow, or forbid extra data during model validation.\n",
      " |              See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      " |\n",
      " |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, extra: 'ExtraValues | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, by_name: 'bool | None' = None) -> 'Self'\n",
      " |      Validate the given object with string data against the Pydantic model.\n",
      " |\n",
      " |      Args:\n",
      " |          obj: The object containing string data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          extra: Whether to ignore, allow, or forbid extra data during model validation.\n",
      " |              See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |\n",
      " |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self'\n",
      " |\n",
      " |  parse_obj(obj: 'Any') -> 'Self'\n",
      " |\n",
      " |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self'\n",
      " |\n",
      " |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]'\n",
      " |\n",
      " |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str'\n",
      " |\n",
      " |  update_forward_refs(**localns: 'Any') -> 'None'\n",
      " |\n",
      " |  validate(value: 'Any') -> 'Self'\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __fields_set__\n",
      " |\n",
      " |  model_extra\n",
      " |      Get extra fields set during validation.\n",
      " |\n",
      " |      Returns:\n",
      " |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      " |\n",
      " |  model_fields_set\n",
      " |      Returns the set of fields that have been explicitly set on this model instance.\n",
      " |\n",
      " |      Returns:\n",
      " |          A set of strings representing the fields that have been set,\n",
      " |              i.e. that were not filled from defaults.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __pydantic_extra__\n",
      " |\n",
      " |  __pydantic_fields_set__\n",
      " |\n",
      " |  __pydantic_private__\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __hash__ = None\n",
      " |\n",
      " |  __pydantic_root_model__ = False\n",
      " |\n",
      " |  model_computed_fields = {}\n",
      " |\n",
      " |  model_fields = {'async_client': FieldInfo(annotation=Any, required=Fal...\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.Runnable:\n",
      " |\n",
      " |  __or__(self, other: 'Runnable[Any, Other] | Callable[[Iterator[Any]], Iterator[Other]] | Callable[[AsyncIterator[Any]], AsyncIterator[Other]] | Callable[[Any], Other] | Mapping[str, Runnable[Any, Other] | Callable[[Any], Other] | Any]') -> 'RunnableSerializable[Input, Other]'\n",
      " |      Runnable \"or\" operator.\n",
      " |\n",
      " |      Compose this `Runnable` with another object to create a\n",
      " |      `RunnableSequence`.\n",
      " |\n",
      " |      Args:\n",
      " |          other: Another `Runnable` or a `Runnable`-like object.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new `Runnable`.\n",
      " |\n",
      " |  __ror__(self, other: 'Runnable[Other, Any] | Callable[[Iterator[Other]], Iterator[Any]] | Callable[[AsyncIterator[Other]], AsyncIterator[Any]] | Callable[[Other], Any] | Mapping[str, Runnable[Other, Any] | Callable[[Other], Any] | Any]') -> 'RunnableSerializable[Other, Output]'\n",
      " |      Runnable \"reverse-or\" operator.\n",
      " |\n",
      " |      Compose this `Runnable` with another object to create a\n",
      " |      `RunnableSequence`.\n",
      " |\n",
      " |      Args:\n",
      " |          other: Another `Runnable` or a `Runnable`-like object.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new `Runnable`.\n",
      " |\n",
      " |  async abatch(self, inputs: 'list[Input]', config: 'RunnableConfig | list[RunnableConfig] | None' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any | None') -> 'list[Output]'\n",
      " |      Default implementation runs `ainvoke` in parallel using `asyncio.gather`.\n",
      " |\n",
      " |      The default implementation of `batch` works well for IO bound runnables.\n",
      " |\n",
      " |      Subclasses must override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying `Runnable` uses an API which supports a batch mode.\n",
      " |\n",
      " |      Args:\n",
      " |          inputs: A list of inputs to the `Runnable`.\n",
      " |          config: A config to use when invoking the `Runnable`.\n",
      " |\n",
      " |              The config supports standard keys like `'tags'`, `'metadata'` for\n",
      " |              tracing purposes, `'max_concurrency'` for controlling how much work to\n",
      " |              do in parallel, and other keys.\n",
      " |\n",
      " |              Please refer to `RunnableConfig` for more details.\n",
      " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
      " |          **kwargs: Additional keyword arguments to pass to the `Runnable`.\n",
      " |\n",
      " |      Returns:\n",
      " |          A list of outputs from the `Runnable`.\n",
      " |\n",
      " |  async abatch_as_completed(self, inputs: 'Sequence[Input]', config: 'RunnableConfig | Sequence[RunnableConfig] | None' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any | None') -> 'AsyncIterator[tuple[int, Output | Exception]]'\n",
      " |      Run `ainvoke` in parallel on a list of inputs.\n",
      " |\n",
      " |      Yields results as they complete.\n",
      " |\n",
      " |      Args:\n",
      " |          inputs: A list of inputs to the `Runnable`.\n",
      " |          config: A config to use when invoking the `Runnable`.\n",
      " |\n",
      " |              The config supports standard keys like `'tags'`, `'metadata'` for\n",
      " |              tracing purposes, `'max_concurrency'` for controlling how much work to\n",
      " |              do in parallel, and other keys.\n",
      " |\n",
      " |              Please refer to `RunnableConfig` for more details.\n",
      " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
      " |          **kwargs: Additional keyword arguments to pass to the `Runnable`.\n",
      " |\n",
      " |      Yields:\n",
      " |          A tuple of the index of the input and the output from the `Runnable`.\n",
      " |\n",
      " |  as_tool(self, args_schema: 'type[BaseModel] | None' = None, *, name: 'str | None' = None, description: 'str | None' = None, arg_types: 'dict[str, type] | None' = None) -> 'BaseTool'\n",
      " |      .. beta::\n",
      " |         This API is in beta and may change in the future.\n",
      " |\n",
      " |      Create a `BaseTool` from a `Runnable`.\n",
      " |\n",
      " |      `as_tool` will instantiate a `BaseTool` with a name, description, and\n",
      " |      `args_schema` from a `Runnable`. Where possible, schemas are inferred\n",
      " |      from `runnable.get_input_schema`.\n",
      " |\n",
      " |      Alternatively (e.g., if the `Runnable` takes a dict as input and the specific\n",
      " |      `dict` keys are not typed), the schema can be specified directly with\n",
      " |      `args_schema`.\n",
      " |\n",
      " |      You can also pass `arg_types` to just specify the required arguments and their\n",
      " |      types.\n",
      " |\n",
      " |      Args:\n",
      " |          args_schema: The schema for the tool.\n",
      " |          name: The name of the tool.\n",
      " |          description: The description of the tool.\n",
      " |          arg_types: A dictionary of argument names to types.\n",
      " |\n",
      " |      Returns:\n",
      " |          A `BaseTool` instance.\n",
      " |\n",
      " |      !!! example \"`TypedDict` input\"\n",
      " |\n",
      " |          ```python\n",
      " |          from typing_extensions import TypedDict\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |\n",
      " |          class Args(TypedDict):\n",
      " |              a: int\n",
      " |              b: list[int]\n",
      " |\n",
      " |\n",
      " |          def f(x: Args) -> str:\n",
      " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
      " |\n",
      " |\n",
      " |          runnable = RunnableLambda(f)\n",
      " |          as_tool = runnable.as_tool()\n",
      " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
      " |          ```\n",
      " |\n",
      " |      !!! example \"`dict` input, specifying schema via `args_schema`\"\n",
      " |\n",
      " |          ```python\n",
      " |          from typing import Any\n",
      " |          from pydantic import BaseModel, Field\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |          def f(x: dict[str, Any]) -> str:\n",
      " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
      " |\n",
      " |          class FSchema(BaseModel):\n",
      " |              \"\"\"Apply a function to an integer and list of integers.\"\"\"\n",
      " |\n",
      " |              a: int = Field(..., description=\"Integer\")\n",
      " |              b: list[int] = Field(..., description=\"List of ints\")\n",
      " |\n",
      " |          runnable = RunnableLambda(f)\n",
      " |          as_tool = runnable.as_tool(FSchema)\n",
      " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
      " |          ```\n",
      " |\n",
      " |      !!! example \"`dict` input, specifying schema via `arg_types`\"\n",
      " |\n",
      " |          ```python\n",
      " |          from typing import Any\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |\n",
      " |          def f(x: dict[str, Any]) -> str:\n",
      " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
      " |\n",
      " |\n",
      " |          runnable = RunnableLambda(f)\n",
      " |          as_tool = runnable.as_tool(arg_types={\"a\": int, \"b\": list[int]})\n",
      " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
      " |          ```\n",
      " |\n",
      " |      !!! example \"`str` input\"\n",
      " |\n",
      " |          ```python\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |\n",
      " |          def f(x: str) -> str:\n",
      " |              return x + \"a\"\n",
      " |\n",
      " |\n",
      " |          def g(x: str) -> str:\n",
      " |              return x + \"z\"\n",
      " |\n",
      " |\n",
      " |          runnable = RunnableLambda(f) | g\n",
      " |          as_tool = runnable.as_tool()\n",
      " |          as_tool.invoke(\"b\")\n",
      " |          ```\n",
      " |\n",
      " |  assign(self, **kwargs: 'Runnable[dict[str, Any], Any] | Callable[[dict[str, Any]], Any] | Mapping[str, Runnable[dict[str, Any], Any] | Callable[[dict[str, Any]], Any]]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Assigns new fields to the `dict` output of this `Runnable`.\n",
      " |\n",
      " |      ```python\n",
      " |      from langchain_core.language_models.fake import FakeStreamingListLLM\n",
      " |      from langchain_core.output_parsers import StrOutputParser\n",
      " |      from langchain_core.prompts import SystemMessagePromptTemplate\n",
      " |      from langchain_core.runnables import Runnable\n",
      " |      from operator import itemgetter\n",
      " |\n",
      " |      prompt = (\n",
      " |          SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n",
      " |          + \"{question}\"\n",
      " |      )\n",
      " |      model = FakeStreamingListLLM(responses=[\"foo-lish\"])\n",
      " |\n",
      " |      chain: Runnable = prompt | model | {\"str\": StrOutputParser()}\n",
      " |\n",
      " |      chain_with_assign = chain.assign(hello=itemgetter(\"str\") | model)\n",
      " |\n",
      " |      print(chain_with_assign.input_schema.model_json_schema())\n",
      " |      # {'title': 'PromptInput', 'type': 'object', 'properties':\n",
      " |      {'question': {'title': 'Question', 'type': 'string'}}}\n",
      " |      print(chain_with_assign.output_schema.model_json_schema())\n",
      " |      # {'title': 'RunnableSequenceOutput', 'type': 'object', 'properties':\n",
      " |      {'str': {'title': 'Str',\n",
      " |      'type': 'string'}, 'hello': {'title': 'Hello', 'type': 'string'}}}\n",
      " |      ```\n",
      " |\n",
      " |      Args:\n",
      " |          **kwargs: A mapping of keys to `Runnable` or `Runnable`-like objects\n",
      " |              that will be invoked with the entire output dict of this `Runnable`.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new `Runnable`.\n",
      " |\n",
      " |  async astream_events(self, input: 'Any', config: 'RunnableConfig | None' = None, *, version: \"Literal['v1', 'v2']\" = 'v2', include_names: 'Sequence[str] | None' = None, include_types: 'Sequence[str] | None' = None, include_tags: 'Sequence[str] | None' = None, exclude_names: 'Sequence[str] | None' = None, exclude_types: 'Sequence[str] | None' = None, exclude_tags: 'Sequence[str] | None' = None, **kwargs: 'Any') -> 'AsyncIterator[StreamEvent]'\n",
      " |      Generate a stream of events.\n",
      " |\n",
      " |      Use to create an iterator over `StreamEvent` that provide real-time information\n",
      " |      about the progress of the `Runnable`, including `StreamEvent` from intermediate\n",
      " |      results.\n",
      " |\n",
      " |      A `StreamEvent` is a dictionary with the following schema:\n",
      " |\n",
      " |      - `event`: Event names are of the format:\n",
      " |          `on_[runnable_type]_(start|stream|end)`.\n",
      " |      - `name`: The name of the `Runnable` that generated the event.\n",
      " |      - `run_id`: Randomly generated ID associated with the given execution of the\n",
      " |          `Runnable` that emitted the event. A child `Runnable` that gets invoked as\n",
      " |          part of the execution of a parent `Runnable` is assigned its own unique ID.\n",
      " |      - `parent_ids`: The IDs of the parent runnables that generated the event. The\n",
      " |          root `Runnable` will have an empty list. The order of the parent IDs is from\n",
      " |          the root to the immediate parent. Only available for v2 version of the API.\n",
      " |          The v1 version of the API will return an empty list.\n",
      " |      - `tags`: The tags of the `Runnable` that generated the event.\n",
      " |      - `metadata`: The metadata of the `Runnable` that generated the event.\n",
      " |      - `data`: The data associated with the event. The contents of this field\n",
      " |          depend on the type of event. See the table below for more details.\n",
      " |\n",
      " |      Below is a table that illustrates some events that might be emitted by various\n",
      " |      chains. Metadata fields have been omitted from the table for brevity.\n",
      " |      Chain definitions have been included after the table.\n",
      " |\n",
      " |      !!! note\n",
      " |          This reference table is for the v2 version of the schema.\n",
      " |\n",
      " |      | event                  | name                 | chunk                               | input                                             | output                                              |\n",
      " |      | ---------------------- | -------------------- | ----------------------------------- | ------------------------------------------------- | --------------------------------------------------- |\n",
      " |      | `on_chat_model_start`  | `'[model name]'`     |                                     | `{\"messages\": [[SystemMessage, HumanMessage]]}`   |                                                     |\n",
      " |      | `on_chat_model_stream` | `'[model name]'`     | `AIMessageChunk(content=\"hello\")`   |                                                   |                                                     |\n",
      " |      | `on_chat_model_end`    | `'[model name]'`     |                                     | `{\"messages\": [[SystemMessage, HumanMessage]]}`   | `AIMessageChunk(content=\"hello world\")`             |\n",
      " |      | `on_llm_start`         | `'[model name]'`     |                                     | `{'input': 'hello'}`                              |                                                     |\n",
      " |      | `on_llm_stream`        | `'[model name]'`     | `'Hello' `                          |                                                   |                                                     |\n",
      " |      | `on_llm_end`           | `'[model name]'`     |                                     | `'Hello human!'`                                  |                                                     |\n",
      " |      | `on_chain_start`       | `'format_docs'`      |                                     |                                                   |                                                     |\n",
      " |      | `on_chain_stream`      | `'format_docs'`      | `'hello world!, goodbye world!'`    |                                                   |                                                     |\n",
      " |      | `on_chain_end`         | `'format_docs'`      |                                     | `[Document(...)]`                                 | `'hello world!, goodbye world!'`                    |\n",
      " |      | `on_tool_start`        | `'some_tool'`        |                                     | `{\"x\": 1, \"y\": \"2\"}`                              |                                                     |\n",
      " |      | `on_tool_end`          | `'some_tool'`        |                                     |                                                   | `{\"x\": 1, \"y\": \"2\"}`                                |\n",
      " |      | `on_retriever_start`   | `'[retriever name]'` |                                     | `{\"query\": \"hello\"}`                              |                                                     |\n",
      " |      | `on_retriever_end`     | `'[retriever name]'` |                                     | `{\"query\": \"hello\"}`                              | `[Document(...), ..]`                               |\n",
      " |      | `on_prompt_start`      | `'[template_name]'`  |                                     | `{\"question\": \"hello\"}`                           |                                                     |\n",
      " |      | `on_prompt_end`        | `'[template_name]'`  |                                     | `{\"question\": \"hello\"}`                           | `ChatPromptValue(messages: [SystemMessage, ...])`   |\n",
      " |\n",
      " |      In addition to the standard events, users can also dispatch custom events (see example below).\n",
      " |\n",
      " |      Custom events will be only be surfaced with in the v2 version of the API!\n",
      " |\n",
      " |      A custom event has following format:\n",
      " |\n",
      " |      | Attribute   | Type   | Description                                                                                               |\n",
      " |      | ----------- | ------ | --------------------------------------------------------------------------------------------------------- |\n",
      " |      | `name`      | `str`  | A user defined name for the event.                                                                        |\n",
      " |      | `data`      | `Any`  | The data associated with the event. This can be anything, though we suggest making it JSON serializable.  |\n",
      " |\n",
      " |      Here are declarations associated with the standard events shown above:\n",
      " |\n",
      " |      `format_docs`:\n",
      " |\n",
      " |      ```python\n",
      " |      def format_docs(docs: list[Document]) -> str:\n",
      " |          '''Format the docs.'''\n",
      " |          return \", \".join([doc.page_content for doc in docs])\n",
      " |\n",
      " |\n",
      " |      format_docs = RunnableLambda(format_docs)\n",
      " |      ```\n",
      " |\n",
      " |      `some_tool`:\n",
      " |\n",
      " |      ```python\n",
      " |      @tool\n",
      " |      def some_tool(x: int, y: str) -> dict:\n",
      " |          '''Some_tool.'''\n",
      " |          return {\"x\": x, \"y\": y}\n",
      " |      ```\n",
      " |\n",
      " |      `prompt`:\n",
      " |\n",
      " |      ```python\n",
      " |      template = ChatPromptTemplate.from_messages(\n",
      " |          [\n",
      " |              (\"system\", \"You are Cat Agent 007\"),\n",
      " |              (\"human\", \"{question}\"),\n",
      " |          ]\n",
      " |      ).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n",
      " |      ```\n",
      " |\n",
      " |      !!! example\n",
      " |\n",
      " |          ```python\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |\n",
      " |          async def reverse(s: str) -> str:\n",
      " |              return s[::-1]\n",
      " |\n",
      " |\n",
      " |          chain = RunnableLambda(func=reverse)\n",
      " |\n",
      " |          events = [\n",
      " |              event async for event in chain.astream_events(\"hello\", version=\"v2\")\n",
      " |          ]\n",
      " |\n",
      " |          # Will produce the following events\n",
      " |          # (run_id, and parent_ids has been omitted for brevity):\n",
      " |          [\n",
      " |              {\n",
      " |                  \"data\": {\"input\": \"hello\"},\n",
      " |                  \"event\": \"on_chain_start\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"chunk\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_stream\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"output\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_end\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |          ]\n",
      " |          ```\n",
      " |\n",
      " |      ```python title=\"Dispatch custom event\"\n",
      " |      from langchain_core.callbacks.manager import (\n",
      " |          adispatch_custom_event,\n",
      " |      )\n",
      " |      from langchain_core.runnables import RunnableLambda, RunnableConfig\n",
      " |      import asyncio\n",
      " |\n",
      " |\n",
      " |      async def slow_thing(some_input: str, config: RunnableConfig) -> str:\n",
      " |          \"\"\"Do something that takes a long time.\"\"\"\n",
      " |          await asyncio.sleep(1) # Placeholder for some slow operation\n",
      " |          await adispatch_custom_event(\n",
      " |              \"progress_event\",\n",
      " |              {\"message\": \"Finished step 1 of 3\"},\n",
      " |              config=config # Must be included for python < 3.10\n",
      " |          )\n",
      " |          await asyncio.sleep(1) # Placeholder for some slow operation\n",
      " |          await adispatch_custom_event(\n",
      " |              \"progress_event\",\n",
      " |              {\"message\": \"Finished step 2 of 3\"},\n",
      " |              config=config # Must be included for python < 3.10\n",
      " |          )\n",
      " |          await asyncio.sleep(1) # Placeholder for some slow operation\n",
      " |          return \"Done\"\n",
      " |\n",
      " |      slow_thing = RunnableLambda(slow_thing)\n",
      " |\n",
      " |      async for event in slow_thing.astream_events(\"some_input\", version=\"v2\"):\n",
      " |          print(event)\n",
      " |      ```\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the `Runnable`.\n",
      " |          config: The config to use for the `Runnable`.\n",
      " |          version: The version of the schema to use, either `'v2'` or `'v1'`.\n",
      " |\n",
      " |              Users should use `'v2'`.\n",
      " |\n",
      " |              `'v1'` is for backwards compatibility and will be deprecated\n",
      " |              in `0.4.0`.\n",
      " |\n",
      " |              No default will be assigned until the API is stabilized.\n",
      " |              custom events will only be surfaced in `'v2'`.\n",
      " |          include_names: Only include events from `Runnable` objects with matching names.\n",
      " |          include_types: Only include events from `Runnable` objects with matching types.\n",
      " |          include_tags: Only include events from `Runnable` objects with matching tags.\n",
      " |          exclude_names: Exclude events from `Runnable` objects with matching names.\n",
      " |          exclude_types: Exclude events from `Runnable` objects with matching types.\n",
      " |          exclude_tags: Exclude events from `Runnable` objects with matching tags.\n",
      " |          **kwargs: Additional keyword arguments to pass to the `Runnable`.\n",
      " |\n",
      " |              These will be passed to `astream_log` as this implementation\n",
      " |              of `astream_events` is built on top of `astream_log`.\n",
      " |\n",
      " |      Yields:\n",
      " |          An async stream of `StreamEvent`.\n",
      " |\n",
      " |      Raises:\n",
      " |          NotImplementedError: If the version is not `'v1'` or `'v2'`.\n",
      " |\n",
      " |  async astream_log(self, input: 'Any', config: 'RunnableConfig | None' = None, *, diff: 'bool' = True, with_streamed_output_list: 'bool' = True, include_names: 'Sequence[str] | None' = None, include_types: 'Sequence[str] | None' = None, include_tags: 'Sequence[str] | None' = None, exclude_names: 'Sequence[str] | None' = None, exclude_types: 'Sequence[str] | None' = None, exclude_tags: 'Sequence[str] | None' = None, **kwargs: 'Any') -> 'AsyncIterator[RunLogPatch] | AsyncIterator[RunLog]'\n",
      " |      Stream all output from a `Runnable`, as reported to the callback system.\n",
      " |\n",
      " |      This includes all inner runs of LLMs, Retrievers, Tools, etc.\n",
      " |\n",
      " |      Output is streamed as Log objects, which include a list of\n",
      " |      Jsonpatch ops that describe how the state of the run has changed in each\n",
      " |      step, and the final state of the run.\n",
      " |\n",
      " |      The Jsonpatch ops can be applied in order to construct state.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the `Runnable`.\n",
      " |          config: The config to use for the `Runnable`.\n",
      " |          diff: Whether to yield diffs between each step or the current state.\n",
      " |          with_streamed_output_list: Whether to yield the `streamed_output` list.\n",
      " |          include_names: Only include logs with these names.\n",
      " |          include_types: Only include logs with these types.\n",
      " |          include_tags: Only include logs with these tags.\n",
      " |          exclude_names: Exclude logs with these names.\n",
      " |          exclude_types: Exclude logs with these types.\n",
      " |          exclude_tags: Exclude logs with these tags.\n",
      " |          **kwargs: Additional keyword arguments to pass to the `Runnable`.\n",
      " |\n",
      " |      Yields:\n",
      " |          A `RunLogPatch` or `RunLog` object.\n",
      " |\n",
      " |  async atransform(self, input: 'AsyncIterator[Input]', config: 'RunnableConfig | None' = None, **kwargs: 'Any | None') -> 'AsyncIterator[Output]'\n",
      " |      Transform inputs to outputs.\n",
      " |\n",
      " |      Default implementation of atransform, which buffers input and calls `astream`.\n",
      " |\n",
      " |      Subclasses must override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |\n",
      " |      Args:\n",
      " |          input: An async iterator of inputs to the `Runnable`.\n",
      " |          config: The config to use for the `Runnable`.\n",
      " |          **kwargs: Additional keyword arguments to pass to the `Runnable`.\n",
      " |\n",
      " |      Yields:\n",
      " |          The output of the `Runnable`.\n",
      " |\n",
      " |  batch(self, inputs: 'list[Input]', config: 'RunnableConfig | list[RunnableConfig] | None' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any | None') -> 'list[Output]'\n",
      " |      Default implementation runs invoke in parallel using a thread pool executor.\n",
      " |\n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |\n",
      " |      Subclasses must override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying `Runnable` uses an API which supports a batch mode.\n",
      " |\n",
      " |      Args:\n",
      " |          inputs: A list of inputs to the `Runnable`.\n",
      " |          config: A config to use when invoking the `Runnable`. The config supports\n",
      " |              standard keys like `'tags'`, `'metadata'` for\n",
      " |              tracing purposes, `'max_concurrency'` for controlling how much work\n",
      " |              to do in parallel, and other keys.\n",
      " |\n",
      " |              Please refer to `RunnableConfig` for more details.\n",
      " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
      " |          **kwargs: Additional keyword arguments to pass to the `Runnable`.\n",
      " |\n",
      " |      Returns:\n",
      " |          A list of outputs from the `Runnable`.\n",
      " |\n",
      " |  batch_as_completed(self, inputs: 'Sequence[Input]', config: 'RunnableConfig | Sequence[RunnableConfig] | None' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any | None') -> 'Iterator[tuple[int, Output | Exception]]'\n",
      " |      Run `invoke` in parallel on a list of inputs.\n",
      " |\n",
      " |      Yields results as they complete.\n",
      " |\n",
      " |      Args:\n",
      " |          inputs: A list of inputs to the `Runnable`.\n",
      " |          config: A config to use when invoking the `Runnable`.\n",
      " |\n",
      " |              The config supports standard keys like `'tags'`, `'metadata'` for\n",
      " |              tracing purposes, `'max_concurrency'` for controlling how much work to\n",
      " |              do in parallel, and other keys.\n",
      " |\n",
      " |              Please refer to `RunnableConfig` for more details.\n",
      " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
      " |          **kwargs: Additional keyword arguments to pass to the `Runnable`.\n",
      " |\n",
      " |      Yields:\n",
      " |          Tuples of the index of the input and the output from the `Runnable`.\n",
      " |\n",
      " |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind arguments to a `Runnable`, returning a new `Runnable`.\n",
      " |\n",
      " |      Useful when a `Runnable` in a chain requires an argument that is not\n",
      " |      in the output of the previous `Runnable` or included in the user input.\n",
      " |\n",
      " |      Args:\n",
      " |          **kwargs: The arguments to bind to the `Runnable`.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new `Runnable` with the arguments bound.\n",
      " |\n",
      " |      Example:\n",
      " |          ```python\n",
      " |          from langchain_ollama import ChatOllama\n",
      " |          from langchain_core.output_parsers import StrOutputParser\n",
      " |\n",
      " |          model = ChatOllama(model=\"llama3.1\")\n",
      " |\n",
      " |          # Without bind\n",
      " |          chain = model | StrOutputParser()\n",
      " |\n",
      " |          chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
      " |          # Output is 'One two three four five.'\n",
      " |\n",
      " |          # With bind\n",
      " |          chain = model.bind(stop=[\"three\"]) | StrOutputParser()\n",
      " |\n",
      " |          chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
      " |          # Output is 'One two'\n",
      " |          ```\n",
      " |\n",
      " |  config_schema(self, *, include: 'Sequence[str] | None' = None) -> 'type[BaseModel]'\n",
      " |      The type of config this `Runnable` accepts specified as a Pydantic model.\n",
      " |\n",
      " |      To mark a field as configurable, see the `configurable_fields`\n",
      " |      and `configurable_alternatives` methods.\n",
      " |\n",
      " |      Args:\n",
      " |          include: A list of fields to include in the config schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A Pydantic model that can be used to validate config.\n",
      " |\n",
      " |  get_config_jsonschema(self, *, include: 'Sequence[str] | None' = None) -> 'dict[str, Any]'\n",
      " |      Get a JSON schema that represents the config of the `Runnable`.\n",
      " |\n",
      " |      Args:\n",
      " |          include: A list of fields to include in the config schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON schema that represents the config of the `Runnable`.\n",
      " |\n",
      " |      !!! version-added \"Added in `langchain-core` 0.3.0\"\n",
      " |\n",
      " |  get_graph(self, config: 'RunnableConfig | None' = None) -> 'Graph'\n",
      " |      Return a graph representation of this `Runnable`.\n",
      " |\n",
      " |  get_input_jsonschema(self, config: 'RunnableConfig | None' = None) -> 'dict[str, Any]'\n",
      " |      Get a JSON schema that represents the input to the `Runnable`.\n",
      " |\n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON schema that represents the input to the `Runnable`.\n",
      " |\n",
      " |      Example:\n",
      " |          ```python\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |\n",
      " |          def add_one(x: int) -> int:\n",
      " |              return x + 1\n",
      " |\n",
      " |\n",
      " |          runnable = RunnableLambda(add_one)\n",
      " |\n",
      " |          print(runnable.get_input_jsonschema())\n",
      " |          ```\n",
      " |\n",
      " |      !!! version-added \"Added in `langchain-core` 0.3.0\"\n",
      " |\n",
      " |  get_input_schema(self, config: 'RunnableConfig | None' = None) -> 'type[BaseModel]'\n",
      " |      Get a Pydantic model that can be used to validate input to the `Runnable`.\n",
      " |\n",
      " |      `Runnable` objects that leverage the `configurable_fields` and\n",
      " |      `configurable_alternatives` methods will have a dynamic input schema that\n",
      " |      depends on which configuration the `Runnable` is invoked with.\n",
      " |\n",
      " |      This method allows to get an input schema for a specific configuration.\n",
      " |\n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A Pydantic model that can be used to validate input.\n",
      " |\n",
      " |  get_name(self, suffix: 'str | None' = None, *, name: 'str | None' = None) -> 'str'\n",
      " |      Get the name of the `Runnable`.\n",
      " |\n",
      " |      Args:\n",
      " |          suffix: An optional suffix to append to the name.\n",
      " |          name: An optional name to use instead of the `Runnable`'s name.\n",
      " |\n",
      " |      Returns:\n",
      " |          The name of the `Runnable`.\n",
      " |\n",
      " |  get_output_jsonschema(self, config: 'RunnableConfig | None' = None) -> 'dict[str, Any]'\n",
      " |      Get a JSON schema that represents the output of the `Runnable`.\n",
      " |\n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON schema that represents the output of the `Runnable`.\n",
      " |\n",
      " |      Example:\n",
      " |          ```python\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |\n",
      " |          def add_one(x: int) -> int:\n",
      " |              return x + 1\n",
      " |\n",
      " |\n",
      " |          runnable = RunnableLambda(add_one)\n",
      " |\n",
      " |          print(runnable.get_output_jsonschema())\n",
      " |          ```\n",
      " |\n",
      " |      !!! version-added \"Added in `langchain-core` 0.3.0\"\n",
      " |\n",
      " |  get_output_schema(self, config: 'RunnableConfig | None' = None) -> 'type[BaseModel]'\n",
      " |      Get a Pydantic model that can be used to validate output to the `Runnable`.\n",
      " |\n",
      " |      `Runnable` objects that leverage the `configurable_fields` and\n",
      " |      `configurable_alternatives` methods will have a dynamic output schema that\n",
      " |      depends on which configuration the `Runnable` is invoked with.\n",
      " |\n",
      " |      This method allows to get an output schema for a specific configuration.\n",
      " |\n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A Pydantic model that can be used to validate output.\n",
      " |\n",
      " |  get_prompts(self, config: 'RunnableConfig | None' = None) -> 'list[BasePromptTemplate]'\n",
      " |      Return a list of prompts used by this `Runnable`.\n",
      " |\n",
      " |  map(self) -> 'Runnable[list[Input], list[Output]]'\n",
      " |      Return a new `Runnable` that maps a list of inputs to a list of outputs.\n",
      " |\n",
      " |      Calls `invoke` with each input.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new `Runnable` that maps a list of inputs to a list of outputs.\n",
      " |\n",
      " |      Example:\n",
      " |          ```python\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |\n",
      " |          def _lambda(x: int) -> int:\n",
      " |              return x + 1\n",
      " |\n",
      " |\n",
      " |          runnable = RunnableLambda(_lambda)\n",
      " |          print(runnable.map().invoke([1, 2, 3]))  # [2, 3, 4]\n",
      " |          ```\n",
      " |\n",
      " |  pick(self, keys: 'str | list[str]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Pick keys from the output `dict` of this `Runnable`.\n",
      " |\n",
      " |      !!! example \"Pick a single key\"\n",
      " |\n",
      " |          ```python\n",
      " |          import json\n",
      " |\n",
      " |          from langchain_core.runnables import RunnableLambda, RunnableMap\n",
      " |\n",
      " |          as_str = RunnableLambda(str)\n",
      " |          as_json = RunnableLambda(json.loads)\n",
      " |          chain = RunnableMap(str=as_str, json=as_json)\n",
      " |\n",
      " |          chain.invoke(\"[1, 2, 3]\")\n",
      " |          # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3]}\n",
      " |\n",
      " |          json_only_chain = chain.pick(\"json\")\n",
      " |          json_only_chain.invoke(\"[1, 2, 3]\")\n",
      " |          # -> [1, 2, 3]\n",
      " |          ```\n",
      " |\n",
      " |      !!! example \"Pick a list of keys\"\n",
      " |\n",
      " |          ```python\n",
      " |          from typing import Any\n",
      " |\n",
      " |          import json\n",
      " |\n",
      " |          from langchain_core.runnables import RunnableLambda, RunnableMap\n",
      " |\n",
      " |          as_str = RunnableLambda(str)\n",
      " |          as_json = RunnableLambda(json.loads)\n",
      " |\n",
      " |\n",
      " |          def as_bytes(x: Any) -> bytes:\n",
      " |              return bytes(x, \"utf-8\")\n",
      " |\n",
      " |\n",
      " |          chain = RunnableMap(\n",
      " |              str=as_str, json=as_json, bytes=RunnableLambda(as_bytes)\n",
      " |          )\n",
      " |\n",
      " |          chain.invoke(\"[1, 2, 3]\")\n",
      " |          # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n",
      " |\n",
      " |          json_and_bytes_chain = chain.pick([\"json\", \"bytes\"])\n",
      " |          json_and_bytes_chain.invoke(\"[1, 2, 3]\")\n",
      " |          # -> {\"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n",
      " |          ```\n",
      " |\n",
      " |      Args:\n",
      " |          keys: A key or list of keys to pick from the output dict.\n",
      " |\n",
      " |      Returns:\n",
      " |          a new `Runnable`.\n",
      " |\n",
      " |  pipe(self, *others: 'Runnable[Any, Other] | Callable[[Any], Other]', name: 'str | None' = None) -> 'RunnableSerializable[Input, Other]'\n",
      " |      Pipe `Runnable` objects.\n",
      " |\n",
      " |      Compose this `Runnable` with `Runnable`-like objects to make a\n",
      " |      `RunnableSequence`.\n",
      " |\n",
      " |      Equivalent to `RunnableSequence(self, *others)` or `self | others[0] | ...`\n",
      " |\n",
      " |      Example:\n",
      " |          ```python\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |\n",
      " |          def add_one(x: int) -> int:\n",
      " |              return x + 1\n",
      " |\n",
      " |\n",
      " |          def mul_two(x: int) -> int:\n",
      " |              return x * 2\n",
      " |\n",
      " |\n",
      " |          runnable_1 = RunnableLambda(add_one)\n",
      " |          runnable_2 = RunnableLambda(mul_two)\n",
      " |          sequence = runnable_1.pipe(runnable_2)\n",
      " |          # Or equivalently:\n",
      " |          # sequence = runnable_1 | runnable_2\n",
      " |          # sequence = RunnableSequence(first=runnable_1, last=runnable_2)\n",
      " |          sequence.invoke(1)\n",
      " |          await sequence.ainvoke(1)\n",
      " |          # -> 4\n",
      " |\n",
      " |          sequence.batch([1, 2, 3])\n",
      " |          await sequence.abatch([1, 2, 3])\n",
      " |          # -> [4, 6, 8]\n",
      " |          ```\n",
      " |\n",
      " |      Args:\n",
      " |          *others: Other `Runnable` or `Runnable`-like objects to compose\n",
      " |          name: An optional name for the resulting `RunnableSequence`.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new `Runnable`.\n",
      " |\n",
      " |  transform(self, input: 'Iterator[Input]', config: 'RunnableConfig | None' = None, **kwargs: 'Any | None') -> 'Iterator[Output]'\n",
      " |      Transform inputs to outputs.\n",
      " |\n",
      " |      Default implementation of transform, which buffers input and calls `astream`.\n",
      " |\n",
      " |      Subclasses must override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |\n",
      " |      Args:\n",
      " |          input: An iterator of inputs to the `Runnable`.\n",
      " |          config: The config to use for the `Runnable`.\n",
      " |          **kwargs: Additional keyword arguments to pass to the `Runnable`.\n",
      " |\n",
      " |      Yields:\n",
      " |          The output of the `Runnable`.\n",
      " |\n",
      " |  with_alisteners(self, *, on_start: 'AsyncListener | None' = None, on_end: 'AsyncListener | None' = None, on_error: 'AsyncListener | None' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind async lifecycle listeners to a `Runnable`.\n",
      " |\n",
      " |      Returns a new `Runnable`.\n",
      " |\n",
      " |      The Run object contains information about the run, including its `id`,\n",
      " |      `type`, `input`, `output`, `error`, `start_time`, `end_time`, and\n",
      " |      any tags or metadata added to the run.\n",
      " |\n",
      " |      Args:\n",
      " |          on_start: Called asynchronously before the `Runnable` starts running,\n",
      " |              with the `Run` object.\n",
      " |          on_end: Called asynchronously after the `Runnable` finishes running,\n",
      " |              with the `Run` object.\n",
      " |          on_error: Called asynchronously if the `Runnable` throws an error,\n",
      " |              with the `Run` object.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new `Runnable` with the listeners bound.\n",
      " |\n",
      " |      Example:\n",
      " |          ```python\n",
      " |          from langchain_core.runnables import RunnableLambda, Runnable\n",
      " |          from datetime import datetime, timezone\n",
      " |          import time\n",
      " |          import asyncio\n",
      " |\n",
      " |\n",
      " |          def format_t(timestamp: float) -> str:\n",
      " |              return datetime.fromtimestamp(timestamp, tz=timezone.utc).isoformat()\n",
      " |\n",
      " |\n",
      " |          async def test_runnable(time_to_sleep: int):\n",
      " |              print(f\"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}\")\n",
      " |              await asyncio.sleep(time_to_sleep)\n",
      " |              print(f\"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}\")\n",
      " |\n",
      " |\n",
      " |          async def fn_start(run_obj: Runnable):\n",
      " |              print(f\"on start callback starts at {format_t(time.time())}\")\n",
      " |              await asyncio.sleep(3)\n",
      " |              print(f\"on start callback ends at {format_t(time.time())}\")\n",
      " |\n",
      " |\n",
      " |          async def fn_end(run_obj: Runnable):\n",
      " |              print(f\"on end callback starts at {format_t(time.time())}\")\n",
      " |              await asyncio.sleep(2)\n",
      " |              print(f\"on end callback ends at {format_t(time.time())}\")\n",
      " |\n",
      " |\n",
      " |          runnable = RunnableLambda(test_runnable).with_alisteners(\n",
      " |              on_start=fn_start, on_end=fn_end\n",
      " |          )\n",
      " |\n",
      " |\n",
      " |          async def concurrent_runs():\n",
      " |              await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))\n",
      " |\n",
      " |\n",
      " |          asyncio.run(concurrent_runs())\n",
      " |          # Result:\n",
      " |          # on start callback starts at 2025-03-01T07:05:22.875378+00:00\n",
      " |          # on start callback starts at 2025-03-01T07:05:22.875495+00:00\n",
      " |          # on start callback ends at 2025-03-01T07:05:25.878862+00:00\n",
      " |          # on start callback ends at 2025-03-01T07:05:25.878947+00:00\n",
      " |          # Runnable[2s]: starts at 2025-03-01T07:05:25.879392+00:00\n",
      " |          # Runnable[3s]: starts at 2025-03-01T07:05:25.879804+00:00\n",
      " |          # Runnable[2s]: ends at 2025-03-01T07:05:27.881998+00:00\n",
      " |          # on end callback starts at 2025-03-01T07:05:27.882360+00:00\n",
      " |          # Runnable[3s]: ends at 2025-03-01T07:05:28.881737+00:00\n",
      " |          # on end callback starts at 2025-03-01T07:05:28.882428+00:00\n",
      " |          # on end callback ends at 2025-03-01T07:05:29.883893+00:00\n",
      " |          # on end callback ends at 2025-03-01T07:05:30.884831+00:00\n",
      " |          ```\n",
      " |\n",
      " |  with_config(self, config: 'RunnableConfig | None' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind config to a `Runnable`, returning a new `Runnable`.\n",
      " |\n",
      " |      Args:\n",
      " |          config: The config to bind to the `Runnable`.\n",
      " |          **kwargs: Additional keyword arguments to pass to the `Runnable`.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new `Runnable` with the config bound.\n",
      " |\n",
      " |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'tuple[type[BaseException], ...]' = (<class 'Exception'>,), exception_key: 'str | None' = None) -> 'RunnableWithFallbacksT[Input, Output]'\n",
      " |      Add fallbacks to a `Runnable`, returning a new `Runnable`.\n",
      " |\n",
      " |      The new `Runnable` will try the original `Runnable`, and then each fallback\n",
      " |      in order, upon failures.\n",
      " |\n",
      " |      Args:\n",
      " |          fallbacks: A sequence of runnables to try if the original `Runnable`\n",
      " |              fails.\n",
      " |          exceptions_to_handle: A tuple of exception types to handle.\n",
      " |          exception_key: If `string` is specified then handled exceptions will be\n",
      " |              passed to fallbacks as part of the input under the specified key.\n",
      " |\n",
      " |              If `None`, exceptions will not be passed to fallbacks.\n",
      " |\n",
      " |              If used, the base `Runnable` and its fallbacks must accept a\n",
      " |              dictionary as input.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new `Runnable` that will try the original `Runnable`, and then each\n",
      " |              Fallback in order, upon failures.\n",
      " |\n",
      " |      Example:\n",
      " |          ```python\n",
      " |          from typing import Iterator\n",
      " |\n",
      " |          from langchain_core.runnables import RunnableGenerator\n",
      " |\n",
      " |\n",
      " |          def _generate_immediate_error(input: Iterator) -> Iterator[str]:\n",
      " |              raise ValueError()\n",
      " |              yield \"\"\n",
      " |\n",
      " |\n",
      " |          def _generate(input: Iterator) -> Iterator[str]:\n",
      " |              yield from \"foo bar\"\n",
      " |\n",
      " |\n",
      " |          runnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(\n",
      " |              [RunnableGenerator(_generate)]\n",
      " |          )\n",
      " |          print(\"\".join(runnable.stream({})))  # foo bar\n",
      " |          ```\n",
      " |\n",
      " |      Args:\n",
      " |          fallbacks: A sequence of runnables to try if the original `Runnable`\n",
      " |              fails.\n",
      " |          exceptions_to_handle: A tuple of exception types to handle.\n",
      " |          exception_key: If `string` is specified then handled exceptions will be\n",
      " |              passed to fallbacks as part of the input under the specified key.\n",
      " |\n",
      " |              If `None`, exceptions will not be passed to fallbacks.\n",
      " |\n",
      " |              If used, the base `Runnable` and its fallbacks must accept a\n",
      " |              dictionary as input.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new `Runnable` that will try the original `Runnable`, and then each\n",
      " |              Fallback in order, upon failures.\n",
      " |\n",
      " |  with_listeners(self, *, on_start: 'Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None' = None, on_end: 'Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None' = None, on_error: 'Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind lifecycle listeners to a `Runnable`, returning a new `Runnable`.\n",
      " |\n",
      " |      The Run object contains information about the run, including its `id`,\n",
      " |      `type`, `input`, `output`, `error`, `start_time`, `end_time`, and\n",
      " |      any tags or metadata added to the run.\n",
      " |\n",
      " |      Args:\n",
      " |          on_start: Called before the `Runnable` starts running, with the `Run`\n",
      " |              object.\n",
      " |          on_end: Called after the `Runnable` finishes running, with the `Run`\n",
      " |              object.\n",
      " |          on_error: Called if the `Runnable` throws an error, with the `Run`\n",
      " |              object.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new `Runnable` with the listeners bound.\n",
      " |\n",
      " |      Example:\n",
      " |          ```python\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |          from langchain_core.tracers.schemas import Run\n",
      " |\n",
      " |          import time\n",
      " |\n",
      " |\n",
      " |          def test_runnable(time_to_sleep: int):\n",
      " |              time.sleep(time_to_sleep)\n",
      " |\n",
      " |\n",
      " |          def fn_start(run_obj: Run):\n",
      " |              print(\"start_time:\", run_obj.start_time)\n",
      " |\n",
      " |\n",
      " |          def fn_end(run_obj: Run):\n",
      " |              print(\"end_time:\", run_obj.end_time)\n",
      " |\n",
      " |\n",
      " |          chain = RunnableLambda(test_runnable).with_listeners(\n",
      " |              on_start=fn_start, on_end=fn_end\n",
      " |          )\n",
      " |          chain.invoke(2)\n",
      " |          ```\n",
      " |\n",
      " |  with_retry(self, *, retry_if_exception_type: 'tuple[type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, exponential_jitter_params: 'ExponentialJitterParams | None' = None, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      " |      Create a new `Runnable` that retries the original `Runnable` on exceptions.\n",
      " |\n",
      " |      Args:\n",
      " |          retry_if_exception_type: A tuple of exception types to retry on.\n",
      " |          wait_exponential_jitter: Whether to add jitter to the wait\n",
      " |              time between retries.\n",
      " |          stop_after_attempt: The maximum number of attempts to make before\n",
      " |              giving up.\n",
      " |          exponential_jitter_params: Parameters for\n",
      " |              `tenacity.wait_exponential_jitter`. Namely: `initial`, `max`,\n",
      " |              `exp_base`, and `jitter` (all `float` values).\n",
      " |\n",
      " |      Returns:\n",
      " |          A new `Runnable` that retries the original `Runnable` on exceptions.\n",
      " |\n",
      " |      Example:\n",
      " |          ```python\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |          count = 0\n",
      " |\n",
      " |\n",
      " |          def _lambda(x: int) -> None:\n",
      " |              global count\n",
      " |              count = count + 1\n",
      " |              if x == 1:\n",
      " |                  raise ValueError(\"x is 1\")\n",
      " |              else:\n",
      " |                  pass\n",
      " |\n",
      " |\n",
      " |          runnable = RunnableLambda(_lambda)\n",
      " |          try:\n",
      " |              runnable.with_retry(\n",
      " |                  stop_after_attempt=2,\n",
      " |                  retry_if_exception_type=(ValueError,),\n",
      " |              ).invoke(1)\n",
      " |          except ValueError:\n",
      " |              pass\n",
      " |\n",
      " |          assert count == 2\n",
      " |          ```\n",
      " |\n",
      " |  with_types(self, *, input_type: 'type[Input] | None' = None, output_type: 'type[Output] | None' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind input and output types to a `Runnable`, returning a new `Runnable`.\n",
      " |\n",
      " |      Args:\n",
      " |          input_type: The input type to bind to the `Runnable`.\n",
      " |          output_type: The output type to bind to the `Runnable`.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new `Runnable` with the types bound.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.runnables.base.Runnable:\n",
      " |\n",
      " |  config_specs\n",
      " |      List configurable fields for this `Runnable`.\n",
      " |\n",
      " |  input_schema\n",
      " |      The type of input this `Runnable` accepts specified as a Pydantic model.\n",
      " |\n",
      " |  output_schema\n",
      " |      Output schema.\n",
      " |\n",
      " |      The type of output this `Runnable` produces specified as a Pydantic model.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |\n",
      " |  __init_subclass__(...)\n",
      " |      Function to initialize subclasses.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(llm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe56c056",
   "metadata": {},
   "source": [
    "answer of 2nd we called dir() to describe the object and all its methord then help will give me enitre documentation for the object\\\n",
    "for 3rd just use any methord\\\n",
    "for 5th we will use inspect to know the signature of our object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca230443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signature:\n",
      "(*args: Any, name: str | None = None, cache: langchain_core.caches.BaseCache | bool | None = None, verbose: bool = <factory>, callbacks: list[langchain_core.callbacks.base.BaseCallbackHandler] | langchain_core.callbacks.base.BaseCallbackManager | None = None, tags: list[str] | None = None, metadata: dict[str, typing.Any] | None = None, custom_get_token_ids: collections.abc.Callable[[str], list[int]] | None = None, rate_limiter: langchain_core.rate_limiters.BaseRateLimiter | None = None, disable_streaming: Union[bool, Literal['tool_calling']] = False, output_version: str | None = <factory>, profile: langchain_core.language_models.model_profile.ModelProfile | None = None, client: Any = None, async_client: Any = None, model: str, temperature: float = 0.7, stop_sequences: list[str] | str | None = None, reasoning_format: Optional[Literal['parsed', 'raw', 'hidden']] = None, reasoning_effort: str | None = None, model_kwargs: dict[str, typing.Any] = <factory>, api_key: pydantic.types.SecretStr | None = <factory>, base_url: str | None = <factory>, groq_proxy: str | None = <factory>, timeout: float | tuple[float, float] | typing.Any | None = None, max_retries: int = 2, streaming: bool = False, n: int = 1, max_tokens: int | None = None, service_tier: Literal['on_demand', 'flex', 'auto'] = 'on_demand', default_headers: collections.abc.Mapping[str, str] | None = None, default_query: collections.abc.Mapping[str, object] | None = None, http_client: typing.Any | None = None, http_async_client: typing.Any | None = None) -> None\n",
      "\n",
      "Methods:\n",
      "['InputType', 'OutputType', 'abatch', 'abatch_as_completed', 'agenerate', 'agenerate_prompt', 'ainvoke', 'as_tool', 'assign', 'astream', 'astream_events', 'astream_log', 'async_client', 'atransform', 'batch', 'batch_as_completed', 'bind', 'bind_tools', 'build_extra', 'cache', 'callbacks', 'client', 'config_schema', 'config_specs', 'configurable_alternatives', 'configurable_fields', 'construct', 'copy', 'custom_get_token_ids', 'default_headers', 'default_query', 'dict', 'disable_streaming', 'from_orm', 'generate', 'generate_prompt', 'get_config_jsonschema', 'get_graph', 'get_input_jsonschema', 'get_input_schema', 'get_lc_namespace', 'get_name', 'get_num_tokens', 'get_num_tokens_from_messages', 'get_output_jsonschema', 'get_output_schema', 'get_prompts', 'get_token_ids', 'groq_api_base', 'groq_api_key', 'groq_proxy', 'http_async_client', 'http_client', 'input_schema', 'invoke', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'map', 'max_retries', 'max_tokens', 'metadata', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_kwargs', 'model_name', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'n', 'name', 'output_schema', 'output_version', 'parse_file', 'parse_obj', 'parse_raw', 'pick', 'pipe', 'profile', 'rate_limiter', 'reasoning_effort', 'reasoning_format', 'request_timeout', 'schema', 'schema_json', 'service_tier', 'set_verbose', 'stop', 'stream', 'streaming', 'tags', 'temperature', 'to_json', 'to_json_not_implemented', 'transform', 'update_forward_refs', 'validate', 'validate_environment', 'verbose', 'with_alisteners', 'with_config', 'with_fallbacks', 'with_listeners', 'with_retry', 'with_structured_output', 'with_types']\n",
      "\n",
      "Response type: <class 'langchain_core.messages.ai.AIMessage'>\n",
      "Response content: <think>\n",
      "Okay, the user asked \"Hello what is 5+5?\" So first, I need to respond appropriately. Let me start by acknowledging their greeting. Then, the math part. 5 plus 5 is a basic arithmetic problem. I should make sure I calculate it correctly. 5 plus 5 equals 10. That's straightforward. Maybe the user is testing if I can handle simple math or just wants a quick answer. I should present the answer clearly. Also, consider if they need further help with math problems. Maybe add a friendly offer to assist with more questions. Keep the tone conversational and helpful. Let me put that together.\n",
      "</think>\n",
      "\n",
      "Hello! 5 + 5 equals **10**. Let me know if you need help with anything else! 😊\n",
      "Extra: {}\n",
      "Modelname: qwen/qwen3-32b\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage\n",
    "import inspect\n",
    "\n",
    "llm = ChatGroq(model=\"qwen/qwen3-32b\", temperature=0)\n",
    "\n",
    "print(\"Signature:\")\n",
    "print(inspect.signature(ChatGroq))\n",
    "\n",
    "print(\"\\nMethods:\")\n",
    "print([m for m in dir(llm) if not m.startswith(\"_\")])\n",
    "\n",
    "response = llm.invoke([HumanMessage(\"Hello what is 5+5?\")])\n",
    "\n",
    "print(\"\\nResponse type:\", type(response))\n",
    "print(\"Response content:\", response.content)\n",
    "print(\"Extra:\", response.additional_kwargs)\n",
    "print(\"Modelname:\",response.response_metadata[\"model_name\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b335977",
   "metadata": {},
   "source": [
    "## **Creating a Very basic agent with single tool call**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "442fc6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langgraph.graph.state.CompiledStateGraph'>\n",
      "\n",
      "Methods:\n",
      "['InputType', 'OutputType', 'abatch', 'abatch_as_completed', 'abulk_update_state', 'aclear_cache', 'aget_graph', 'aget_state', 'aget_state_history', 'aget_subgraphs', 'ainvoke', 'as_tool', 'assign', 'astream', 'astream_events', 'astream_log', 'atransform', 'attach_branch', 'attach_edge', 'attach_node', 'aupdate_state', 'batch', 'batch_as_completed', 'bind', 'builder', 'bulk_update_state', 'cache', 'cache_policy', 'channels', 'checkpointer', 'clear_cache', 'config', 'config_schema', 'config_specs', 'context_schema', 'copy', 'debug', 'get_config_jsonschema', 'get_context_jsonschema', 'get_graph', 'get_input_jsonschema', 'get_input_schema', 'get_name', 'get_output_jsonschema', 'get_output_schema', 'get_prompts', 'get_state', 'get_state_history', 'get_subgraphs', 'input_channels', 'input_schema', 'interrupt_after_nodes', 'interrupt_before_nodes', 'invoke', 'map', 'name', 'nodes', 'output_channels', 'output_schema', 'pick', 'pipe', 'retry_policy', 'schema_to_mapper', 'step_timeout', 'store', 'stream', 'stream_channels', 'stream_channels_asis', 'stream_channels_list', 'stream_eager', 'stream_mode', 'transform', 'trigger_to_nodes', 'update_state', 'validate', 'with_alisteners', 'with_config', 'with_fallbacks', 'with_listeners', 'with_retry', 'with_types']\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[get_weather],\n",
    "    system_prompt=\"You are a helpful assistant\",\n",
    ")\n",
    "\n",
    "print(type(agent))\n",
    "\n",
    "print(\"\\nMethods:\")\n",
    "print([m for m in dir(agent) if not m.startswith(\"_\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6af68982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on CompiledStateGraph in module langgraph.graph.state object:\n",
      "\n",
      "class CompiledStateGraph(langgraph.pregel.main.Pregel, typing.Generic)\n",
      " |  CompiledStateGraph(*, builder: 'StateGraph[StateT, ContextT, InputT, OutputT]', schema_to_mapper: 'dict[type[Any], Callable[[Any], Any] | None]', **kwargs: 'Any') -> 'None'\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      CompiledStateGraph\n",
      " |      langgraph.pregel.main.Pregel\n",
      " |      langgraph.pregel.protocol.PregelProtocol\n",
      " |      langchain_core.runnables.base.Runnable\n",
      " |      abc.ABC\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, *, builder: 'StateGraph[StateT, ContextT, InputT, OutputT]', schema_to_mapper: 'dict[type[Any], Callable[[Any], Any] | None]', **kwargs: 'Any') -> 'None'\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  attach_branch(self, start: 'str', name: 'str', branch: 'BranchSpec', *, with_reader: 'bool' = True) -> 'None'\n",
      " |\n",
      " |  attach_edge(self, starts: 'str | Sequence[str]', end: 'str') -> 'None'\n",
      " |\n",
      " |  attach_node(self, key: 'str', node: 'StateNodeSpec[Any, ContextT] | None') -> 'None'\n",
      " |\n",
      " |  get_input_jsonschema(self, config: 'RunnableConfig | None' = None) -> 'dict[str, Any]'\n",
      " |      Get a JSON schema that represents the input to the `Runnable`.\n",
      " |\n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON schema that represents the input to the `Runnable`.\n",
      " |\n",
      " |      Example:\n",
      " |          ```python\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |\n",
      " |          def add_one(x: int) -> int:\n",
      " |              return x + 1\n",
      " |\n",
      " |\n",
      " |          runnable = RunnableLambda(add_one)\n",
      " |\n",
      " |          print(runnable.get_input_jsonschema())\n",
      " |          ```\n",
      " |\n",
      " |      !!! version-added \"Added in `langchain-core` 0.3.0\"\n",
      " |\n",
      " |  get_output_jsonschema(self, config: 'RunnableConfig | None' = None) -> 'dict[str, Any]'\n",
      " |      Get a JSON schema that represents the output of the `Runnable`.\n",
      " |\n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON schema that represents the output of the `Runnable`.\n",
      " |\n",
      " |      Example:\n",
      " |          ```python\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |\n",
      " |          def add_one(x: int) -> int:\n",
      " |              return x + 1\n",
      " |\n",
      " |\n",
      " |          runnable = RunnableLambda(add_one)\n",
      " |\n",
      " |          print(runnable.get_output_jsonschema())\n",
      " |          ```\n",
      " |\n",
      " |      !!! version-added \"Added in `langchain-core` 0.3.0\"\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      " |\n",
      " |  __annotations__ = {'builder': 'StateGraph[StateT, ContextT, InputT, Ou...\n",
      " |\n",
      " |  __orig_bases__ = (langgraph.pregel.main.Pregel[~StateT, ~ContextT, ~In...\n",
      " |\n",
      " |  __parameters__ = (~StateT, ~ContextT, ~InputT, ~OutputT)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langgraph.pregel.main.Pregel:\n",
      " |\n",
      " |  async abulk_update_state(self, config: 'RunnableConfig', supersteps: 'Sequence[Sequence[StateUpdate]]') -> 'RunnableConfig'\n",
      " |      Asynchronously apply updates to the graph state in bulk. Requires a checkpointer to be set.\n",
      " |\n",
      " |      Args:\n",
      " |          config: The config to apply the updates to.\n",
      " |          supersteps: A list of supersteps, each including a list of updates to apply sequentially to a graph state.\n",
      " |\n",
      " |              Each update is a tuple of the form `(values, as_node, task_id)` where `task_id` is optional.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValueError: If no checkpointer is set or no updates are provided.\n",
      " |          InvalidUpdateError: If an invalid update is provided.\n",
      " |\n",
      " |      Returns:\n",
      " |          RunnableConfig: The updated config.\n",
      " |\n",
      " |  async aclear_cache(self, nodes: 'Sequence[str] | None' = None) -> 'None'\n",
      " |      Asynchronously clear the cache for the given nodes.\n",
      " |\n",
      " |  async aget_graph(self, config: 'RunnableConfig | None' = None, *, xray: 'int | bool' = False) -> 'Graph'\n",
      " |      Return a drawable representation of the computation graph.\n",
      " |\n",
      " |  async aget_state(self, config: 'RunnableConfig', *, subgraphs: 'bool' = False) -> 'StateSnapshot'\n",
      " |      Get the current state of the graph.\n",
      " |\n",
      " |  async aget_state_history(self, config: 'RunnableConfig', *, filter: 'dict[str, Any] | None' = None, before: 'RunnableConfig | None' = None, limit: 'int | None' = None) -> 'AsyncIterator[StateSnapshot]'\n",
      " |      Asynchronously get the history of the state of the graph.\n",
      " |\n",
      " |  async aget_subgraphs(self, *, namespace: 'str | None' = None, recurse: 'bool' = False) -> 'AsyncIterator[tuple[str, PregelProtocol]]'\n",
      " |      Get the subgraphs of the graph.\n",
      " |\n",
      " |      Args:\n",
      " |          namespace: The namespace to filter the subgraphs by.\n",
      " |          recurse: Whether to recurse into the subgraphs.\n",
      " |              If `False`, only the immediate subgraphs will be returned.\n",
      " |\n",
      " |      Returns:\n",
      " |          An iterator of the `(namespace, subgraph)` pairs.\n",
      " |\n",
      " |  async ainvoke(self, input: 'InputT | Command | None', config: 'RunnableConfig | None' = None, *, context: 'ContextT | None' = None, stream_mode: 'StreamMode' = 'values', print_mode: 'StreamMode | Sequence[StreamMode]' = (), output_keys: 'str | Sequence[str] | None' = None, interrupt_before: 'All | Sequence[str] | None' = None, interrupt_after: 'All | Sequence[str] | None' = None, durability: 'Durability | None' = None, **kwargs: 'Any') -> 'dict[str, Any] | Any'\n",
      " |      Asynchronously run the graph with a single input and config.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input data for the graph. It can be a dictionary or any other type.\n",
      " |          config: The configuration for the graph run.\n",
      " |          context: The static context to use for the run.\n",
      " |              !!! version-added \"Added in version 0.6.0\"\n",
      " |          stream_mode: The stream mode for the graph run.\n",
      " |          print_mode: Accepts the same values as `stream_mode`, but only prints the output to the console, for debugging purposes.\n",
      " |\n",
      " |              Does not affect the output of the graph in any way.\n",
      " |          output_keys: The output keys to retrieve from the graph run.\n",
      " |          interrupt_before: The nodes to interrupt the graph run before.\n",
      " |          interrupt_after: The nodes to interrupt the graph run after.\n",
      " |          durability: The durability mode for the graph execution, defaults to `\"async\"`.\n",
      " |\n",
      " |              Options are:\n",
      " |\n",
      " |              - `\"sync\"`: Changes are persisted synchronously before the next step starts.\n",
      " |              - `\"async\"`: Changes are persisted asynchronously while the next step executes.\n",
      " |              - `\"exit\"`: Changes are persisted only when the graph exits.\n",
      " |          **kwargs: Additional keyword arguments to pass to the graph run.\n",
      " |\n",
      " |      Returns:\n",
      " |          The output of the graph run. If `stream_mode` is `\"values\"`, it returns the latest output.\n",
      " |          If `stream_mode` is not `\"values\"`, it returns a list of output chunks.\n",
      " |\n",
      " |  async astream(self, input: 'InputT | Command | None', config: 'RunnableConfig | None' = None, *, context: 'ContextT | None' = None, stream_mode: 'StreamMode | Sequence[StreamMode] | None' = None, print_mode: 'StreamMode | Sequence[StreamMode]' = (), output_keys: 'str | Sequence[str] | None' = None, interrupt_before: 'All | Sequence[str] | None' = None, interrupt_after: 'All | Sequence[str] | None' = None, durability: 'Durability | None' = None, subgraphs: 'bool' = False, debug: 'bool | None' = None, **kwargs: 'Unpack[DeprecatedKwargs]') -> 'AsyncIterator[dict[str, Any] | Any]'\n",
      " |      Asynchronously stream graph steps for a single input.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the graph.\n",
      " |          config: The configuration to use for the run.\n",
      " |          context: The static context to use for the run.\n",
      " |              !!! version-added \"Added in version 0.6.0\"\n",
      " |          stream_mode: The mode to stream output, defaults to `self.stream_mode`.\n",
      " |\n",
      " |              Options are:\n",
      " |\n",
      " |              - `\"values\"`: Emit all values in the state after each step, including interrupts.\n",
      " |                  When used with functional API, values are emitted once at the end of the workflow.\n",
      " |              - `\"updates\"`: Emit only the node or task names and updates returned by the nodes or tasks after each step.\n",
      " |                  If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.\n",
      " |              - `\"custom\"`: Emit custom data from inside nodes or tasks using `StreamWriter`.\n",
      " |              - `\"messages\"`: Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.\n",
      " |                  - Will be emitted as 2-tuples `(LLM token, metadata)`.\n",
      " |              - `\"checkpoints\"`: Emit an event when a checkpoint is created, in the same format as returned by `get_state()`.\n",
      " |              - `\"tasks\"`: Emit events when tasks start and finish, including their results and errors.\n",
      " |              - `\"debug\"`: Emit debug events with as much information as possible for each step.\n",
      " |\n",
      " |              You can pass a list as the `stream_mode` parameter to stream multiple modes at once.\n",
      " |              The streamed outputs will be tuples of `(mode, data)`.\n",
      " |\n",
      " |              See [LangGraph streaming guide](https://docs.langchain.com/oss/python/langgraph/streaming) for more details.\n",
      " |          print_mode: Accepts the same values as `stream_mode`, but only prints the output to the console, for debugging purposes.\n",
      " |\n",
      " |              Does not affect the output of the graph in any way.\n",
      " |          output_keys: The keys to stream, defaults to all non-context channels.\n",
      " |          interrupt_before: Nodes to interrupt before, defaults to all nodes in the graph.\n",
      " |          interrupt_after: Nodes to interrupt after, defaults to all nodes in the graph.\n",
      " |          durability: The durability mode for the graph execution, defaults to `\"async\"`.\n",
      " |\n",
      " |              Options are:\n",
      " |\n",
      " |              - `\"sync\"`: Changes are persisted synchronously before the next step starts.\n",
      " |              - `\"async\"`: Changes are persisted asynchronously while the next step executes.\n",
      " |              - `\"exit\"`: Changes are persisted only when the graph exits.\n",
      " |          subgraphs: Whether to stream events from inside subgraphs, defaults to `False`.\n",
      " |\n",
      " |              If `True`, the events will be emitted as tuples `(namespace, data)`,\n",
      " |              or `(namespace, mode, data)` if `stream_mode` is a list,\n",
      " |              where `namespace` is a tuple with the path to the node where a subgraph is invoked,\n",
      " |              e.g. `(\"parent_node:<task_id>\", \"child_node:<task_id>\")`.\n",
      " |\n",
      " |              See [LangGraph streaming guide](https://docs.langchain.com/oss/python/langgraph/streaming) for more details.\n",
      " |\n",
      " |      Yields:\n",
      " |          The output of each step in the graph. The output shape depends on the `stream_mode`.\n",
      " |\n",
      " |  async aupdate_state(self, config: 'RunnableConfig', values: 'dict[str, Any] | Any', as_node: 'str | None' = None, task_id: 'str | None' = None) -> 'RunnableConfig'\n",
      " |      Asynchronously update the state of the graph with the given values, as if they came from\n",
      " |      node `as_node`. If `as_node` is not provided, it will be set to the last node\n",
      " |      that updated the state, if not ambiguous.\n",
      " |\n",
      " |  bulk_update_state(self, config: 'RunnableConfig', supersteps: 'Sequence[Sequence[StateUpdate]]') -> 'RunnableConfig'\n",
      " |      Apply updates to the graph state in bulk. Requires a checkpointer to be set.\n",
      " |\n",
      " |      Args:\n",
      " |          config: The config to apply the updates to.\n",
      " |          supersteps: A list of supersteps, each including a list of updates to apply sequentially to a graph state.\n",
      " |\n",
      " |              Each update is a tuple of the form `(values, as_node, task_id)` where `task_id` is optional.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValueError: If no checkpointer is set or no updates are provided.\n",
      " |          InvalidUpdateError: If an invalid update is provided.\n",
      " |\n",
      " |      Returns:\n",
      " |          RunnableConfig: The updated config.\n",
      " |\n",
      " |  clear_cache(self, nodes: 'Sequence[str] | None' = None) -> 'None'\n",
      " |      Clear the cache for the given nodes.\n",
      " |\n",
      " |  config_schema(self, *, include: 'Sequence[str] | None' = None) -> 'type[BaseModel]'\n",
      " |      The type of config this `Runnable` accepts specified as a Pydantic model.\n",
      " |\n",
      " |      To mark a field as configurable, see the `configurable_fields`\n",
      " |      and `configurable_alternatives` methods.\n",
      " |\n",
      " |      Args:\n",
      " |          include: A list of fields to include in the config schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A Pydantic model that can be used to validate config.\n",
      " |\n",
      " |  copy(self, update: 'dict[str, Any] | None' = None) -> 'Self'\n",
      " |\n",
      " |  get_config_jsonschema(self, *, include: 'Sequence[str] | None' = None) -> 'dict[str, Any]'\n",
      " |      Get a JSON schema that represents the config of the `Runnable`.\n",
      " |\n",
      " |      Args:\n",
      " |          include: A list of fields to include in the config schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON schema that represents the config of the `Runnable`.\n",
      " |\n",
      " |      !!! version-added \"Added in `langchain-core` 0.3.0\"\n",
      " |\n",
      " |  get_context_jsonschema(self) -> 'dict[str, Any] | None'\n",
      " |\n",
      " |  get_graph(self, config: 'RunnableConfig | None' = None, *, xray: 'int | bool' = False) -> 'Graph'\n",
      " |      Return a drawable representation of the computation graph.\n",
      " |\n",
      " |  get_input_schema(self, config: 'RunnableConfig | None' = None) -> 'type[BaseModel]'\n",
      " |      Get a Pydantic model that can be used to validate input to the `Runnable`.\n",
      " |\n",
      " |      `Runnable` objects that leverage the `configurable_fields` and\n",
      " |      `configurable_alternatives` methods will have a dynamic input schema that\n",
      " |      depends on which configuration the `Runnable` is invoked with.\n",
      " |\n",
      " |      This method allows to get an input schema for a specific configuration.\n",
      " |\n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A Pydantic model that can be used to validate input.\n",
      " |\n",
      " |  get_output_schema(self, config: 'RunnableConfig | None' = None) -> 'type[BaseModel]'\n",
      " |      Get a Pydantic model that can be used to validate output to the `Runnable`.\n",
      " |\n",
      " |      `Runnable` objects that leverage the `configurable_fields` and\n",
      " |      `configurable_alternatives` methods will have a dynamic output schema that\n",
      " |      depends on which configuration the `Runnable` is invoked with.\n",
      " |\n",
      " |      This method allows to get an output schema for a specific configuration.\n",
      " |\n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A Pydantic model that can be used to validate output.\n",
      " |\n",
      " |  get_state(self, config: 'RunnableConfig', *, subgraphs: 'bool' = False) -> 'StateSnapshot'\n",
      " |      Get the current state of the graph.\n",
      " |\n",
      " |  get_state_history(self, config: 'RunnableConfig', *, filter: 'dict[str, Any] | None' = None, before: 'RunnableConfig | None' = None, limit: 'int | None' = None) -> 'Iterator[StateSnapshot]'\n",
      " |      Get the history of the state of the graph.\n",
      " |\n",
      " |  get_subgraphs(self, *, namespace: 'str | None' = None, recurse: 'bool' = False) -> 'Iterator[tuple[str, PregelProtocol]]'\n",
      " |      Get the subgraphs of the graph.\n",
      " |\n",
      " |      Args:\n",
      " |          namespace: The namespace to filter the subgraphs by.\n",
      " |          recurse: Whether to recurse into the subgraphs.\n",
      " |              If `False`, only the immediate subgraphs will be returned.\n",
      " |\n",
      " |      Returns:\n",
      " |          An iterator of the `(namespace, subgraph)` pairs.\n",
      " |\n",
      " |  invoke(self, input: 'InputT | Command | None', config: 'RunnableConfig | None' = None, *, context: 'ContextT | None' = None, stream_mode: 'StreamMode' = 'values', print_mode: 'StreamMode | Sequence[StreamMode]' = (), output_keys: 'str | Sequence[str] | None' = None, interrupt_before: 'All | Sequence[str] | None' = None, interrupt_after: 'All | Sequence[str] | None' = None, durability: 'Durability | None' = None, **kwargs: 'Any') -> 'dict[str, Any] | Any'\n",
      " |      Run the graph with a single input and config.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input data for the graph. It can be a dictionary or any other type.\n",
      " |          config: The configuration for the graph run.\n",
      " |          context: The static context to use for the run.\n",
      " |              !!! version-added \"Added in version 0.6.0\"\n",
      " |          stream_mode: The stream mode for the graph run.\n",
      " |          print_mode: Accepts the same values as `stream_mode`, but only prints the output to the console, for debugging purposes.\n",
      " |\n",
      " |              Does not affect the output of the graph in any way.\n",
      " |          output_keys: The output keys to retrieve from the graph run.\n",
      " |          interrupt_before: The nodes to interrupt the graph run before.\n",
      " |          interrupt_after: The nodes to interrupt the graph run after.\n",
      " |          durability: The durability mode for the graph execution, defaults to `\"async\"`.\n",
      " |\n",
      " |              Options are:\n",
      " |\n",
      " |              - `\"sync\"`: Changes are persisted synchronously before the next step starts.\n",
      " |              - `\"async\"`: Changes are persisted asynchronously while the next step executes.\n",
      " |              - `\"exit\"`: Changes are persisted only when the graph exits.\n",
      " |          **kwargs: Additional keyword arguments to pass to the graph run.\n",
      " |\n",
      " |      Returns:\n",
      " |          The output of the graph run. If `stream_mode` is `\"values\"`, it returns the latest output.\n",
      " |          If `stream_mode` is not `\"values\"`, it returns a list of output chunks.\n",
      " |\n",
      " |  stream(self, input: 'InputT | Command | None', config: 'RunnableConfig | None' = None, *, context: 'ContextT | None' = None, stream_mode: 'StreamMode | Sequence[StreamMode] | None' = None, print_mode: 'StreamMode | Sequence[StreamMode]' = (), output_keys: 'str | Sequence[str] | None' = None, interrupt_before: 'All | Sequence[str] | None' = None, interrupt_after: 'All | Sequence[str] | None' = None, durability: 'Durability | None' = None, subgraphs: 'bool' = False, debug: 'bool | None' = None, **kwargs: 'Unpack[DeprecatedKwargs]') -> 'Iterator[dict[str, Any] | Any]'\n",
      " |      Stream graph steps for a single input.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the graph.\n",
      " |          config: The configuration to use for the run.\n",
      " |          context: The static context to use for the run.\n",
      " |              !!! version-added \"Added in version 0.6.0\"\n",
      " |          stream_mode: The mode to stream output, defaults to `self.stream_mode`.\n",
      " |\n",
      " |              Options are:\n",
      " |\n",
      " |              - `\"values\"`: Emit all values in the state after each step, including interrupts.\n",
      " |                  When used with functional API, values are emitted once at the end of the workflow.\n",
      " |              - `\"updates\"`: Emit only the node or task names and updates returned by the nodes or tasks after each step.\n",
      " |                  If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.\n",
      " |              - `\"custom\"`: Emit custom data from inside nodes or tasks using `StreamWriter`.\n",
      " |              - `\"messages\"`: Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.\n",
      " |                  - Will be emitted as 2-tuples `(LLM token, metadata)`.\n",
      " |              - `\"checkpoints\"`: Emit an event when a checkpoint is created, in the same format as returned by `get_state()`.\n",
      " |              - `\"tasks\"`: Emit events when tasks start and finish, including their results and errors.\n",
      " |              - `\"debug\"`: Emit debug events with as much information as possible for each step.\n",
      " |\n",
      " |              You can pass a list as the `stream_mode` parameter to stream multiple modes at once.\n",
      " |              The streamed outputs will be tuples of `(mode, data)`.\n",
      " |\n",
      " |              See [LangGraph streaming guide](https://docs.langchain.com/oss/python/langgraph/streaming) for more details.\n",
      " |          print_mode: Accepts the same values as `stream_mode`, but only prints the output to the console, for debugging purposes.\n",
      " |\n",
      " |              Does not affect the output of the graph in any way.\n",
      " |          output_keys: The keys to stream, defaults to all non-context channels.\n",
      " |          interrupt_before: Nodes to interrupt before, defaults to all nodes in the graph.\n",
      " |          interrupt_after: Nodes to interrupt after, defaults to all nodes in the graph.\n",
      " |          durability: The durability mode for the graph execution, defaults to `\"async\"`.\n",
      " |\n",
      " |              Options are:\n",
      " |\n",
      " |              - `\"sync\"`: Changes are persisted synchronously before the next step starts.\n",
      " |              - `\"async\"`: Changes are persisted asynchronously while the next step executes.\n",
      " |              - `\"exit\"`: Changes are persisted only when the graph exits.\n",
      " |          subgraphs: Whether to stream events from inside subgraphs, defaults to `False`.\n",
      " |\n",
      " |              If `True`, the events will be emitted as tuples `(namespace, data)`,\n",
      " |              or `(namespace, mode, data)` if `stream_mode` is a list,\n",
      " |              where `namespace` is a tuple with the path to the node where a subgraph is invoked,\n",
      " |              e.g. `(\"parent_node:<task_id>\", \"child_node:<task_id>\")`.\n",
      " |\n",
      " |              See [LangGraph streaming guide](https://docs.langchain.com/oss/python/langgraph/streaming) for more details.\n",
      " |\n",
      " |      Yields:\n",
      " |          The output of each step in the graph. The output shape depends on the `stream_mode`.\n",
      " |\n",
      " |  update_state(self, config: 'RunnableConfig', values: 'dict[str, Any] | Any | None', as_node: 'str | None' = None, task_id: 'str | None' = None) -> 'RunnableConfig'\n",
      " |      Update the state of the graph with the given values, as if they came from\n",
      " |      node `as_node`. If `as_node` is not provided, it will be set to the last node\n",
      " |      that updated the state, if not ambiguous.\n",
      " |\n",
      " |  validate(self) -> 'Self'\n",
      " |\n",
      " |  with_config(self, config: 'RunnableConfig | None' = None, **kwargs: 'Any') -> 'Self'\n",
      " |      Create a copy of the Pregel object with an updated config.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langgraph.pregel.main.Pregel:\n",
      " |\n",
      " |  InputType\n",
      " |      Input type.\n",
      " |\n",
      " |      The type of input this `Runnable` accepts specified as a type annotation.\n",
      " |\n",
      " |      Raises:\n",
      " |          TypeError: If the input type cannot be inferred.\n",
      " |\n",
      " |  OutputType\n",
      " |      Output Type.\n",
      " |\n",
      " |      The type of output this `Runnable` produces specified as a type annotation.\n",
      " |\n",
      " |      Raises:\n",
      " |          TypeError: If the output type cannot be inferred.\n",
      " |\n",
      " |  stream_channels_asis\n",
      " |\n",
      " |  stream_channels_list\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from langgraph.pregel.main.Pregel:\n",
      " |\n",
      " |  cache = None\n",
      " |\n",
      " |  cache_policy = None\n",
      " |\n",
      " |  checkpointer = None\n",
      " |\n",
      " |  config = None\n",
      " |\n",
      " |  context_schema = None\n",
      " |\n",
      " |  name = 'LangGraph'\n",
      " |\n",
      " |  retry_policy = ()\n",
      " |\n",
      " |  step_timeout = None\n",
      " |\n",
      " |  store = None\n",
      " |\n",
      " |  stream_channels = None\n",
      " |\n",
      " |  stream_eager = False\n",
      " |\n",
      " |  stream_mode = 'values'\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.Runnable:\n",
      " |\n",
      " |  __or__(self, other: 'Runnable[Any, Other] | Callable[[Iterator[Any]], Iterator[Other]] | Callable[[AsyncIterator[Any]], AsyncIterator[Other]] | Callable[[Any], Other] | Mapping[str, Runnable[Any, Other] | Callable[[Any], Other] | Any]') -> 'RunnableSerializable[Input, Other]'\n",
      " |      Runnable \"or\" operator.\n",
      " |\n",
      " |      Compose this `Runnable` with another object to create a\n",
      " |      `RunnableSequence`.\n",
      " |\n",
      " |      Args:\n",
      " |          other: Another `Runnable` or a `Runnable`-like object.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new `Runnable`.\n",
      " |\n",
      " |  __ror__(self, other: 'Runnable[Other, Any] | Callable[[Iterator[Other]], Iterator[Any]] | Callable[[AsyncIterator[Other]], AsyncIterator[Any]] | Callable[[Other], Any] | Mapping[str, Runnable[Other, Any] | Callable[[Other], Any] | Any]') -> 'RunnableSerializable[Other, Output]'\n",
      " |      Runnable \"reverse-or\" operator.\n",
      " |\n",
      " |      Compose this `Runnable` with another object to create a\n",
      " |      `RunnableSequence`.\n",
      " |\n",
      " |      Args:\n",
      " |          other: Another `Runnable` or a `Runnable`-like object.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new `Runnable`.\n",
      " |\n",
      " |  async abatch(self, inputs: 'list[Input]', config: 'RunnableConfig | list[RunnableConfig] | None' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any | None') -> 'list[Output]'\n",
      " |      Default implementation runs `ainvoke` in parallel using `asyncio.gather`.\n",
      " |\n",
      " |      The default implementation of `batch` works well for IO bound runnables.\n",
      " |\n",
      " |      Subclasses must override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying `Runnable` uses an API which supports a batch mode.\n",
      " |\n",
      " |      Args:\n",
      " |          inputs: A list of inputs to the `Runnable`.\n",
      " |          config: A config to use when invoking the `Runnable`.\n",
      " |\n",
      " |              The config supports standard keys like `'tags'`, `'metadata'` for\n",
      " |              tracing purposes, `'max_concurrency'` for controlling how much work to\n",
      " |              do in parallel, and other keys.\n",
      " |\n",
      " |              Please refer to `RunnableConfig` for more details.\n",
      " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
      " |          **kwargs: Additional keyword arguments to pass to the `Runnable`.\n",
      " |\n",
      " |      Returns:\n",
      " |          A list of outputs from the `Runnable`.\n",
      " |\n",
      " |  async abatch_as_completed(self, inputs: 'Sequence[Input]', config: 'RunnableConfig | Sequence[RunnableConfig] | None' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any | None') -> 'AsyncIterator[tuple[int, Output | Exception]]'\n",
      " |      Run `ainvoke` in parallel on a list of inputs.\n",
      " |\n",
      " |      Yields results as they complete.\n",
      " |\n",
      " |      Args:\n",
      " |          inputs: A list of inputs to the `Runnable`.\n",
      " |          config: A config to use when invoking the `Runnable`.\n",
      " |\n",
      " |              The config supports standard keys like `'tags'`, `'metadata'` for\n",
      " |              tracing purposes, `'max_concurrency'` for controlling how much work to\n",
      " |              do in parallel, and other keys.\n",
      " |\n",
      " |              Please refer to `RunnableConfig` for more details.\n",
      " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
      " |          **kwargs: Additional keyword arguments to pass to the `Runnable`.\n",
      " |\n",
      " |      Yields:\n",
      " |          A tuple of the index of the input and the output from the `Runnable`.\n",
      " |\n",
      " |  as_tool(self, args_schema: 'type[BaseModel] | None' = None, *, name: 'str | None' = None, description: 'str | None' = None, arg_types: 'dict[str, type] | None' = None) -> 'BaseTool'\n",
      " |      .. beta::\n",
      " |         This API is in beta and may change in the future.\n",
      " |\n",
      " |      Create a `BaseTool` from a `Runnable`.\n",
      " |\n",
      " |      `as_tool` will instantiate a `BaseTool` with a name, description, and\n",
      " |      `args_schema` from a `Runnable`. Where possible, schemas are inferred\n",
      " |      from `runnable.get_input_schema`.\n",
      " |\n",
      " |      Alternatively (e.g., if the `Runnable` takes a dict as input and the specific\n",
      " |      `dict` keys are not typed), the schema can be specified directly with\n",
      " |      `args_schema`.\n",
      " |\n",
      " |      You can also pass `arg_types` to just specify the required arguments and their\n",
      " |      types.\n",
      " |\n",
      " |      Args:\n",
      " |          args_schema: The schema for the tool.\n",
      " |          name: The name of the tool.\n",
      " |          description: The description of the tool.\n",
      " |          arg_types: A dictionary of argument names to types.\n",
      " |\n",
      " |      Returns:\n",
      " |          A `BaseTool` instance.\n",
      " |\n",
      " |      !!! example \"`TypedDict` input\"\n",
      " |\n",
      " |          ```python\n",
      " |          from typing_extensions import TypedDict\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |\n",
      " |          class Args(TypedDict):\n",
      " |              a: int\n",
      " |              b: list[int]\n",
      " |\n",
      " |\n",
      " |          def f(x: Args) -> str:\n",
      " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
      " |\n",
      " |\n",
      " |          runnable = RunnableLambda(f)\n",
      " |          as_tool = runnable.as_tool()\n",
      " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
      " |          ```\n",
      " |\n",
      " |      !!! example \"`dict` input, specifying schema via `args_schema`\"\n",
      " |\n",
      " |          ```python\n",
      " |          from typing import Any\n",
      " |          from pydantic import BaseModel, Field\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |          def f(x: dict[str, Any]) -> str:\n",
      " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
      " |\n",
      " |          class FSchema(BaseModel):\n",
      " |              \"\"\"Apply a function to an integer and list of integers.\"\"\"\n",
      " |\n",
      " |              a: int = Field(..., description=\"Integer\")\n",
      " |              b: list[int] = Field(..., description=\"List of ints\")\n",
      " |\n",
      " |          runnable = RunnableLambda(f)\n",
      " |          as_tool = runnable.as_tool(FSchema)\n",
      " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
      " |          ```\n",
      " |\n",
      " |      !!! example \"`dict` input, specifying schema via `arg_types`\"\n",
      " |\n",
      " |          ```python\n",
      " |          from typing import Any\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |\n",
      " |          def f(x: dict[str, Any]) -> str:\n",
      " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
      " |\n",
      " |\n",
      " |          runnable = RunnableLambda(f)\n",
      " |          as_tool = runnable.as_tool(arg_types={\"a\": int, \"b\": list[int]})\n",
      " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
      " |          ```\n",
      " |\n",
      " |      !!! example \"`str` input\"\n",
      " |\n",
      " |          ```python\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |\n",
      " |          def f(x: str) -> str:\n",
      " |              return x + \"a\"\n",
      " |\n",
      " |\n",
      " |          def g(x: str) -> str:\n",
      " |              return x + \"z\"\n",
      " |\n",
      " |\n",
      " |          runnable = RunnableLambda(f) | g\n",
      " |          as_tool = runnable.as_tool()\n",
      " |          as_tool.invoke(\"b\")\n",
      " |          ```\n",
      " |\n",
      " |  assign(self, **kwargs: 'Runnable[dict[str, Any], Any] | Callable[[dict[str, Any]], Any] | Mapping[str, Runnable[dict[str, Any], Any] | Callable[[dict[str, Any]], Any]]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Assigns new fields to the `dict` output of this `Runnable`.\n",
      " |\n",
      " |      ```python\n",
      " |      from langchain_core.language_models.fake import FakeStreamingListLLM\n",
      " |      from langchain_core.output_parsers import StrOutputParser\n",
      " |      from langchain_core.prompts import SystemMessagePromptTemplate\n",
      " |      from langchain_core.runnables import Runnable\n",
      " |      from operator import itemgetter\n",
      " |\n",
      " |      prompt = (\n",
      " |          SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n",
      " |          + \"{question}\"\n",
      " |      )\n",
      " |      model = FakeStreamingListLLM(responses=[\"foo-lish\"])\n",
      " |\n",
      " |      chain: Runnable = prompt | model | {\"str\": StrOutputParser()}\n",
      " |\n",
      " |      chain_with_assign = chain.assign(hello=itemgetter(\"str\") | model)\n",
      " |\n",
      " |      print(chain_with_assign.input_schema.model_json_schema())\n",
      " |      # {'title': 'PromptInput', 'type': 'object', 'properties':\n",
      " |      {'question': {'title': 'Question', 'type': 'string'}}}\n",
      " |      print(chain_with_assign.output_schema.model_json_schema())\n",
      " |      # {'title': 'RunnableSequenceOutput', 'type': 'object', 'properties':\n",
      " |      {'str': {'title': 'Str',\n",
      " |      'type': 'string'}, 'hello': {'title': 'Hello', 'type': 'string'}}}\n",
      " |      ```\n",
      " |\n",
      " |      Args:\n",
      " |          **kwargs: A mapping of keys to `Runnable` or `Runnable`-like objects\n",
      " |              that will be invoked with the entire output dict of this `Runnable`.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new `Runnable`.\n",
      " |\n",
      " |  async astream_events(self, input: 'Any', config: 'RunnableConfig | None' = None, *, version: \"Literal['v1', 'v2']\" = 'v2', include_names: 'Sequence[str] | None' = None, include_types: 'Sequence[str] | None' = None, include_tags: 'Sequence[str] | None' = None, exclude_names: 'Sequence[str] | None' = None, exclude_types: 'Sequence[str] | None' = None, exclude_tags: 'Sequence[str] | None' = None, **kwargs: 'Any') -> 'AsyncIterator[StreamEvent]'\n",
      " |      Generate a stream of events.\n",
      " |\n",
      " |      Use to create an iterator over `StreamEvent` that provide real-time information\n",
      " |      about the progress of the `Runnable`, including `StreamEvent` from intermediate\n",
      " |      results.\n",
      " |\n",
      " |      A `StreamEvent` is a dictionary with the following schema:\n",
      " |\n",
      " |      - `event`: Event names are of the format:\n",
      " |          `on_[runnable_type]_(start|stream|end)`.\n",
      " |      - `name`: The name of the `Runnable` that generated the event.\n",
      " |      - `run_id`: Randomly generated ID associated with the given execution of the\n",
      " |          `Runnable` that emitted the event. A child `Runnable` that gets invoked as\n",
      " |          part of the execution of a parent `Runnable` is assigned its own unique ID.\n",
      " |      - `parent_ids`: The IDs of the parent runnables that generated the event. The\n",
      " |          root `Runnable` will have an empty list. The order of the parent IDs is from\n",
      " |          the root to the immediate parent. Only available for v2 version of the API.\n",
      " |          The v1 version of the API will return an empty list.\n",
      " |      - `tags`: The tags of the `Runnable` that generated the event.\n",
      " |      - `metadata`: The metadata of the `Runnable` that generated the event.\n",
      " |      - `data`: The data associated with the event. The contents of this field\n",
      " |          depend on the type of event. See the table below for more details.\n",
      " |\n",
      " |      Below is a table that illustrates some events that might be emitted by various\n",
      " |      chains. Metadata fields have been omitted from the table for brevity.\n",
      " |      Chain definitions have been included after the table.\n",
      " |\n",
      " |      !!! note\n",
      " |          This reference table is for the v2 version of the schema.\n",
      " |\n",
      " |      | event                  | name                 | chunk                               | input                                             | output                                              |\n",
      " |      | ---------------------- | -------------------- | ----------------------------------- | ------------------------------------------------- | --------------------------------------------------- |\n",
      " |      | `on_chat_model_start`  | `'[model name]'`     |                                     | `{\"messages\": [[SystemMessage, HumanMessage]]}`   |                                                     |\n",
      " |      | `on_chat_model_stream` | `'[model name]'`     | `AIMessageChunk(content=\"hello\")`   |                                                   |                                                     |\n",
      " |      | `on_chat_model_end`    | `'[model name]'`     |                                     | `{\"messages\": [[SystemMessage, HumanMessage]]}`   | `AIMessageChunk(content=\"hello world\")`             |\n",
      " |      | `on_llm_start`         | `'[model name]'`     |                                     | `{'input': 'hello'}`                              |                                                     |\n",
      " |      | `on_llm_stream`        | `'[model name]'`     | `'Hello' `                          |                                                   |                                                     |\n",
      " |      | `on_llm_end`           | `'[model name]'`     |                                     | `'Hello human!'`                                  |                                                     |\n",
      " |      | `on_chain_start`       | `'format_docs'`      |                                     |                                                   |                                                     |\n",
      " |      | `on_chain_stream`      | `'format_docs'`      | `'hello world!, goodbye world!'`    |                                                   |                                                     |\n",
      " |      | `on_chain_end`         | `'format_docs'`      |                                     | `[Document(...)]`                                 | `'hello world!, goodbye world!'`                    |\n",
      " |      | `on_tool_start`        | `'some_tool'`        |                                     | `{\"x\": 1, \"y\": \"2\"}`                              |                                                     |\n",
      " |      | `on_tool_end`          | `'some_tool'`        |                                     |                                                   | `{\"x\": 1, \"y\": \"2\"}`                                |\n",
      " |      | `on_retriever_start`   | `'[retriever name]'` |                                     | `{\"query\": \"hello\"}`                              |                                                     |\n",
      " |      | `on_retriever_end`     | `'[retriever name]'` |                                     | `{\"query\": \"hello\"}`                              | `[Document(...), ..]`                               |\n",
      " |      | `on_prompt_start`      | `'[template_name]'`  |                                     | `{\"question\": \"hello\"}`                           |                                                     |\n",
      " |      | `on_prompt_end`        | `'[template_name]'`  |                                     | `{\"question\": \"hello\"}`                           | `ChatPromptValue(messages: [SystemMessage, ...])`   |\n",
      " |\n",
      " |      In addition to the standard events, users can also dispatch custom events (see example below).\n",
      " |\n",
      " |      Custom events will be only be surfaced with in the v2 version of the API!\n",
      " |\n",
      " |      A custom event has following format:\n",
      " |\n",
      " |      | Attribute   | Type   | Description                                                                                               |\n",
      " |      | ----------- | ------ | --------------------------------------------------------------------------------------------------------- |\n",
      " |      | `name`      | `str`  | A user defined name for the event.                                                                        |\n",
      " |      | `data`      | `Any`  | The data associated with the event. This can be anything, though we suggest making it JSON serializable.  |\n",
      " |\n",
      " |      Here are declarations associated with the standard events shown above:\n",
      " |\n",
      " |      `format_docs`:\n",
      " |\n",
      " |      ```python\n",
      " |      def format_docs(docs: list[Document]) -> str:\n",
      " |          '''Format the docs.'''\n",
      " |          return \", \".join([doc.page_content for doc in docs])\n",
      " |\n",
      " |\n",
      " |      format_docs = RunnableLambda(format_docs)\n",
      " |      ```\n",
      " |\n",
      " |      `some_tool`:\n",
      " |\n",
      " |      ```python\n",
      " |      @tool\n",
      " |      def some_tool(x: int, y: str) -> dict:\n",
      " |          '''Some_tool.'''\n",
      " |          return {\"x\": x, \"y\": y}\n",
      " |      ```\n",
      " |\n",
      " |      `prompt`:\n",
      " |\n",
      " |      ```python\n",
      " |      template = ChatPromptTemplate.from_messages(\n",
      " |          [\n",
      " |              (\"system\", \"You are Cat Agent 007\"),\n",
      " |              (\"human\", \"{question}\"),\n",
      " |          ]\n",
      " |      ).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n",
      " |      ```\n",
      " |\n",
      " |      !!! example\n",
      " |\n",
      " |          ```python\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |\n",
      " |          async def reverse(s: str) -> str:\n",
      " |              return s[::-1]\n",
      " |\n",
      " |\n",
      " |          chain = RunnableLambda(func=reverse)\n",
      " |\n",
      " |          events = [\n",
      " |              event async for event in chain.astream_events(\"hello\", version=\"v2\")\n",
      " |          ]\n",
      " |\n",
      " |          # Will produce the following events\n",
      " |          # (run_id, and parent_ids has been omitted for brevity):\n",
      " |          [\n",
      " |              {\n",
      " |                  \"data\": {\"input\": \"hello\"},\n",
      " |                  \"event\": \"on_chain_start\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"chunk\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_stream\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"output\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_end\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |          ]\n",
      " |          ```\n",
      " |\n",
      " |      ```python title=\"Dispatch custom event\"\n",
      " |      from langchain_core.callbacks.manager import (\n",
      " |          adispatch_custom_event,\n",
      " |      )\n",
      " |      from langchain_core.runnables import RunnableLambda, RunnableConfig\n",
      " |      import asyncio\n",
      " |\n",
      " |\n",
      " |      async def slow_thing(some_input: str, config: RunnableConfig) -> str:\n",
      " |          \"\"\"Do something that takes a long time.\"\"\"\n",
      " |          await asyncio.sleep(1) # Placeholder for some slow operation\n",
      " |          await adispatch_custom_event(\n",
      " |              \"progress_event\",\n",
      " |              {\"message\": \"Finished step 1 of 3\"},\n",
      " |              config=config # Must be included for python < 3.10\n",
      " |          )\n",
      " |          await asyncio.sleep(1) # Placeholder for some slow operation\n",
      " |          await adispatch_custom_event(\n",
      " |              \"progress_event\",\n",
      " |              {\"message\": \"Finished step 2 of 3\"},\n",
      " |              config=config # Must be included for python < 3.10\n",
      " |          )\n",
      " |          await asyncio.sleep(1) # Placeholder for some slow operation\n",
      " |          return \"Done\"\n",
      " |\n",
      " |      slow_thing = RunnableLambda(slow_thing)\n",
      " |\n",
      " |      async for event in slow_thing.astream_events(\"some_input\", version=\"v2\"):\n",
      " |          print(event)\n",
      " |      ```\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the `Runnable`.\n",
      " |          config: The config to use for the `Runnable`.\n",
      " |          version: The version of the schema to use, either `'v2'` or `'v1'`.\n",
      " |\n",
      " |              Users should use `'v2'`.\n",
      " |\n",
      " |              `'v1'` is for backwards compatibility and will be deprecated\n",
      " |              in `0.4.0`.\n",
      " |\n",
      " |              No default will be assigned until the API is stabilized.\n",
      " |              custom events will only be surfaced in `'v2'`.\n",
      " |          include_names: Only include events from `Runnable` objects with matching names.\n",
      " |          include_types: Only include events from `Runnable` objects with matching types.\n",
      " |          include_tags: Only include events from `Runnable` objects with matching tags.\n",
      " |          exclude_names: Exclude events from `Runnable` objects with matching names.\n",
      " |          exclude_types: Exclude events from `Runnable` objects with matching types.\n",
      " |          exclude_tags: Exclude events from `Runnable` objects with matching tags.\n",
      " |          **kwargs: Additional keyword arguments to pass to the `Runnable`.\n",
      " |\n",
      " |              These will be passed to `astream_log` as this implementation\n",
      " |              of `astream_events` is built on top of `astream_log`.\n",
      " |\n",
      " |      Yields:\n",
      " |          An async stream of `StreamEvent`.\n",
      " |\n",
      " |      Raises:\n",
      " |          NotImplementedError: If the version is not `'v1'` or `'v2'`.\n",
      " |\n",
      " |  async astream_log(self, input: 'Any', config: 'RunnableConfig | None' = None, *, diff: 'bool' = True, with_streamed_output_list: 'bool' = True, include_names: 'Sequence[str] | None' = None, include_types: 'Sequence[str] | None' = None, include_tags: 'Sequence[str] | None' = None, exclude_names: 'Sequence[str] | None' = None, exclude_types: 'Sequence[str] | None' = None, exclude_tags: 'Sequence[str] | None' = None, **kwargs: 'Any') -> 'AsyncIterator[RunLogPatch] | AsyncIterator[RunLog]'\n",
      " |      Stream all output from a `Runnable`, as reported to the callback system.\n",
      " |\n",
      " |      This includes all inner runs of LLMs, Retrievers, Tools, etc.\n",
      " |\n",
      " |      Output is streamed as Log objects, which include a list of\n",
      " |      Jsonpatch ops that describe how the state of the run has changed in each\n",
      " |      step, and the final state of the run.\n",
      " |\n",
      " |      The Jsonpatch ops can be applied in order to construct state.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the `Runnable`.\n",
      " |          config: The config to use for the `Runnable`.\n",
      " |          diff: Whether to yield diffs between each step or the current state.\n",
      " |          with_streamed_output_list: Whether to yield the `streamed_output` list.\n",
      " |          include_names: Only include logs with these names.\n",
      " |          include_types: Only include logs with these types.\n",
      " |          include_tags: Only include logs with these tags.\n",
      " |          exclude_names: Exclude logs with these names.\n",
      " |          exclude_types: Exclude logs with these types.\n",
      " |          exclude_tags: Exclude logs with these tags.\n",
      " |          **kwargs: Additional keyword arguments to pass to the `Runnable`.\n",
      " |\n",
      " |      Yields:\n",
      " |          A `RunLogPatch` or `RunLog` object.\n",
      " |\n",
      " |  async atransform(self, input: 'AsyncIterator[Input]', config: 'RunnableConfig | None' = None, **kwargs: 'Any | None') -> 'AsyncIterator[Output]'\n",
      " |      Transform inputs to outputs.\n",
      " |\n",
      " |      Default implementation of atransform, which buffers input and calls `astream`.\n",
      " |\n",
      " |      Subclasses must override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |\n",
      " |      Args:\n",
      " |          input: An async iterator of inputs to the `Runnable`.\n",
      " |          config: The config to use for the `Runnable`.\n",
      " |          **kwargs: Additional keyword arguments to pass to the `Runnable`.\n",
      " |\n",
      " |      Yields:\n",
      " |          The output of the `Runnable`.\n",
      " |\n",
      " |  batch(self, inputs: 'list[Input]', config: 'RunnableConfig | list[RunnableConfig] | None' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any | None') -> 'list[Output]'\n",
      " |      Default implementation runs invoke in parallel using a thread pool executor.\n",
      " |\n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |\n",
      " |      Subclasses must override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying `Runnable` uses an API which supports a batch mode.\n",
      " |\n",
      " |      Args:\n",
      " |          inputs: A list of inputs to the `Runnable`.\n",
      " |          config: A config to use when invoking the `Runnable`. The config supports\n",
      " |              standard keys like `'tags'`, `'metadata'` for\n",
      " |              tracing purposes, `'max_concurrency'` for controlling how much work\n",
      " |              to do in parallel, and other keys.\n",
      " |\n",
      " |              Please refer to `RunnableConfig` for more details.\n",
      " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
      " |          **kwargs: Additional keyword arguments to pass to the `Runnable`.\n",
      " |\n",
      " |      Returns:\n",
      " |          A list of outputs from the `Runnable`.\n",
      " |\n",
      " |  batch_as_completed(self, inputs: 'Sequence[Input]', config: 'RunnableConfig | Sequence[RunnableConfig] | None' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any | None') -> 'Iterator[tuple[int, Output | Exception]]'\n",
      " |      Run `invoke` in parallel on a list of inputs.\n",
      " |\n",
      " |      Yields results as they complete.\n",
      " |\n",
      " |      Args:\n",
      " |          inputs: A list of inputs to the `Runnable`.\n",
      " |          config: A config to use when invoking the `Runnable`.\n",
      " |\n",
      " |              The config supports standard keys like `'tags'`, `'metadata'` for\n",
      " |              tracing purposes, `'max_concurrency'` for controlling how much work to\n",
      " |              do in parallel, and other keys.\n",
      " |\n",
      " |              Please refer to `RunnableConfig` for more details.\n",
      " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
      " |          **kwargs: Additional keyword arguments to pass to the `Runnable`.\n",
      " |\n",
      " |      Yields:\n",
      " |          Tuples of the index of the input and the output from the `Runnable`.\n",
      " |\n",
      " |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind arguments to a `Runnable`, returning a new `Runnable`.\n",
      " |\n",
      " |      Useful when a `Runnable` in a chain requires an argument that is not\n",
      " |      in the output of the previous `Runnable` or included in the user input.\n",
      " |\n",
      " |      Args:\n",
      " |          **kwargs: The arguments to bind to the `Runnable`.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new `Runnable` with the arguments bound.\n",
      " |\n",
      " |      Example:\n",
      " |          ```python\n",
      " |          from langchain_ollama import ChatOllama\n",
      " |          from langchain_core.output_parsers import StrOutputParser\n",
      " |\n",
      " |          model = ChatOllama(model=\"llama3.1\")\n",
      " |\n",
      " |          # Without bind\n",
      " |          chain = model | StrOutputParser()\n",
      " |\n",
      " |          chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
      " |          # Output is 'One two three four five.'\n",
      " |\n",
      " |          # With bind\n",
      " |          chain = model.bind(stop=[\"three\"]) | StrOutputParser()\n",
      " |\n",
      " |          chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
      " |          # Output is 'One two'\n",
      " |          ```\n",
      " |\n",
      " |  get_name(self, suffix: 'str | None' = None, *, name: 'str | None' = None) -> 'str'\n",
      " |      Get the name of the `Runnable`.\n",
      " |\n",
      " |      Args:\n",
      " |          suffix: An optional suffix to append to the name.\n",
      " |          name: An optional name to use instead of the `Runnable`'s name.\n",
      " |\n",
      " |      Returns:\n",
      " |          The name of the `Runnable`.\n",
      " |\n",
      " |  get_prompts(self, config: 'RunnableConfig | None' = None) -> 'list[BasePromptTemplate]'\n",
      " |      Return a list of prompts used by this `Runnable`.\n",
      " |\n",
      " |  map(self) -> 'Runnable[list[Input], list[Output]]'\n",
      " |      Return a new `Runnable` that maps a list of inputs to a list of outputs.\n",
      " |\n",
      " |      Calls `invoke` with each input.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new `Runnable` that maps a list of inputs to a list of outputs.\n",
      " |\n",
      " |      Example:\n",
      " |          ```python\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |\n",
      " |          def _lambda(x: int) -> int:\n",
      " |              return x + 1\n",
      " |\n",
      " |\n",
      " |          runnable = RunnableLambda(_lambda)\n",
      " |          print(runnable.map().invoke([1, 2, 3]))  # [2, 3, 4]\n",
      " |          ```\n",
      " |\n",
      " |  pick(self, keys: 'str | list[str]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Pick keys from the output `dict` of this `Runnable`.\n",
      " |\n",
      " |      !!! example \"Pick a single key\"\n",
      " |\n",
      " |          ```python\n",
      " |          import json\n",
      " |\n",
      " |          from langchain_core.runnables import RunnableLambda, RunnableMap\n",
      " |\n",
      " |          as_str = RunnableLambda(str)\n",
      " |          as_json = RunnableLambda(json.loads)\n",
      " |          chain = RunnableMap(str=as_str, json=as_json)\n",
      " |\n",
      " |          chain.invoke(\"[1, 2, 3]\")\n",
      " |          # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3]}\n",
      " |\n",
      " |          json_only_chain = chain.pick(\"json\")\n",
      " |          json_only_chain.invoke(\"[1, 2, 3]\")\n",
      " |          # -> [1, 2, 3]\n",
      " |          ```\n",
      " |\n",
      " |      !!! example \"Pick a list of keys\"\n",
      " |\n",
      " |          ```python\n",
      " |          from typing import Any\n",
      " |\n",
      " |          import json\n",
      " |\n",
      " |          from langchain_core.runnables import RunnableLambda, RunnableMap\n",
      " |\n",
      " |          as_str = RunnableLambda(str)\n",
      " |          as_json = RunnableLambda(json.loads)\n",
      " |\n",
      " |\n",
      " |          def as_bytes(x: Any) -> bytes:\n",
      " |              return bytes(x, \"utf-8\")\n",
      " |\n",
      " |\n",
      " |          chain = RunnableMap(\n",
      " |              str=as_str, json=as_json, bytes=RunnableLambda(as_bytes)\n",
      " |          )\n",
      " |\n",
      " |          chain.invoke(\"[1, 2, 3]\")\n",
      " |          # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n",
      " |\n",
      " |          json_and_bytes_chain = chain.pick([\"json\", \"bytes\"])\n",
      " |          json_and_bytes_chain.invoke(\"[1, 2, 3]\")\n",
      " |          # -> {\"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n",
      " |          ```\n",
      " |\n",
      " |      Args:\n",
      " |          keys: A key or list of keys to pick from the output dict.\n",
      " |\n",
      " |      Returns:\n",
      " |          a new `Runnable`.\n",
      " |\n",
      " |  pipe(self, *others: 'Runnable[Any, Other] | Callable[[Any], Other]', name: 'str | None' = None) -> 'RunnableSerializable[Input, Other]'\n",
      " |      Pipe `Runnable` objects.\n",
      " |\n",
      " |      Compose this `Runnable` with `Runnable`-like objects to make a\n",
      " |      `RunnableSequence`.\n",
      " |\n",
      " |      Equivalent to `RunnableSequence(self, *others)` or `self | others[0] | ...`\n",
      " |\n",
      " |      Example:\n",
      " |          ```python\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |\n",
      " |          def add_one(x: int) -> int:\n",
      " |              return x + 1\n",
      " |\n",
      " |\n",
      " |          def mul_two(x: int) -> int:\n",
      " |              return x * 2\n",
      " |\n",
      " |\n",
      " |          runnable_1 = RunnableLambda(add_one)\n",
      " |          runnable_2 = RunnableLambda(mul_two)\n",
      " |          sequence = runnable_1.pipe(runnable_2)\n",
      " |          # Or equivalently:\n",
      " |          # sequence = runnable_1 | runnable_2\n",
      " |          # sequence = RunnableSequence(first=runnable_1, last=runnable_2)\n",
      " |          sequence.invoke(1)\n",
      " |          await sequence.ainvoke(1)\n",
      " |          # -> 4\n",
      " |\n",
      " |          sequence.batch([1, 2, 3])\n",
      " |          await sequence.abatch([1, 2, 3])\n",
      " |          # -> [4, 6, 8]\n",
      " |          ```\n",
      " |\n",
      " |      Args:\n",
      " |          *others: Other `Runnable` or `Runnable`-like objects to compose\n",
      " |          name: An optional name for the resulting `RunnableSequence`.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new `Runnable`.\n",
      " |\n",
      " |  transform(self, input: 'Iterator[Input]', config: 'RunnableConfig | None' = None, **kwargs: 'Any | None') -> 'Iterator[Output]'\n",
      " |      Transform inputs to outputs.\n",
      " |\n",
      " |      Default implementation of transform, which buffers input and calls `astream`.\n",
      " |\n",
      " |      Subclasses must override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |\n",
      " |      Args:\n",
      " |          input: An iterator of inputs to the `Runnable`.\n",
      " |          config: The config to use for the `Runnable`.\n",
      " |          **kwargs: Additional keyword arguments to pass to the `Runnable`.\n",
      " |\n",
      " |      Yields:\n",
      " |          The output of the `Runnable`.\n",
      " |\n",
      " |  with_alisteners(self, *, on_start: 'AsyncListener | None' = None, on_end: 'AsyncListener | None' = None, on_error: 'AsyncListener | None' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind async lifecycle listeners to a `Runnable`.\n",
      " |\n",
      " |      Returns a new `Runnable`.\n",
      " |\n",
      " |      The Run object contains information about the run, including its `id`,\n",
      " |      `type`, `input`, `output`, `error`, `start_time`, `end_time`, and\n",
      " |      any tags or metadata added to the run.\n",
      " |\n",
      " |      Args:\n",
      " |          on_start: Called asynchronously before the `Runnable` starts running,\n",
      " |              with the `Run` object.\n",
      " |          on_end: Called asynchronously after the `Runnable` finishes running,\n",
      " |              with the `Run` object.\n",
      " |          on_error: Called asynchronously if the `Runnable` throws an error,\n",
      " |              with the `Run` object.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new `Runnable` with the listeners bound.\n",
      " |\n",
      " |      Example:\n",
      " |          ```python\n",
      " |          from langchain_core.runnables import RunnableLambda, Runnable\n",
      " |          from datetime import datetime, timezone\n",
      " |          import time\n",
      " |          import asyncio\n",
      " |\n",
      " |\n",
      " |          def format_t(timestamp: float) -> str:\n",
      " |              return datetime.fromtimestamp(timestamp, tz=timezone.utc).isoformat()\n",
      " |\n",
      " |\n",
      " |          async def test_runnable(time_to_sleep: int):\n",
      " |              print(f\"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}\")\n",
      " |              await asyncio.sleep(time_to_sleep)\n",
      " |              print(f\"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}\")\n",
      " |\n",
      " |\n",
      " |          async def fn_start(run_obj: Runnable):\n",
      " |              print(f\"on start callback starts at {format_t(time.time())}\")\n",
      " |              await asyncio.sleep(3)\n",
      " |              print(f\"on start callback ends at {format_t(time.time())}\")\n",
      " |\n",
      " |\n",
      " |          async def fn_end(run_obj: Runnable):\n",
      " |              print(f\"on end callback starts at {format_t(time.time())}\")\n",
      " |              await asyncio.sleep(2)\n",
      " |              print(f\"on end callback ends at {format_t(time.time())}\")\n",
      " |\n",
      " |\n",
      " |          runnable = RunnableLambda(test_runnable).with_alisteners(\n",
      " |              on_start=fn_start, on_end=fn_end\n",
      " |          )\n",
      " |\n",
      " |\n",
      " |          async def concurrent_runs():\n",
      " |              await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))\n",
      " |\n",
      " |\n",
      " |          asyncio.run(concurrent_runs())\n",
      " |          # Result:\n",
      " |          # on start callback starts at 2025-03-01T07:05:22.875378+00:00\n",
      " |          # on start callback starts at 2025-03-01T07:05:22.875495+00:00\n",
      " |          # on start callback ends at 2025-03-01T07:05:25.878862+00:00\n",
      " |          # on start callback ends at 2025-03-01T07:05:25.878947+00:00\n",
      " |          # Runnable[2s]: starts at 2025-03-01T07:05:25.879392+00:00\n",
      " |          # Runnable[3s]: starts at 2025-03-01T07:05:25.879804+00:00\n",
      " |          # Runnable[2s]: ends at 2025-03-01T07:05:27.881998+00:00\n",
      " |          # on end callback starts at 2025-03-01T07:05:27.882360+00:00\n",
      " |          # Runnable[3s]: ends at 2025-03-01T07:05:28.881737+00:00\n",
      " |          # on end callback starts at 2025-03-01T07:05:28.882428+00:00\n",
      " |          # on end callback ends at 2025-03-01T07:05:29.883893+00:00\n",
      " |          # on end callback ends at 2025-03-01T07:05:30.884831+00:00\n",
      " |          ```\n",
      " |\n",
      " |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'tuple[type[BaseException], ...]' = (<class 'Exception'>,), exception_key: 'str | None' = None) -> 'RunnableWithFallbacksT[Input, Output]'\n",
      " |      Add fallbacks to a `Runnable`, returning a new `Runnable`.\n",
      " |\n",
      " |      The new `Runnable` will try the original `Runnable`, and then each fallback\n",
      " |      in order, upon failures.\n",
      " |\n",
      " |      Args:\n",
      " |          fallbacks: A sequence of runnables to try if the original `Runnable`\n",
      " |              fails.\n",
      " |          exceptions_to_handle: A tuple of exception types to handle.\n",
      " |          exception_key: If `string` is specified then handled exceptions will be\n",
      " |              passed to fallbacks as part of the input under the specified key.\n",
      " |\n",
      " |              If `None`, exceptions will not be passed to fallbacks.\n",
      " |\n",
      " |              If used, the base `Runnable` and its fallbacks must accept a\n",
      " |              dictionary as input.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new `Runnable` that will try the original `Runnable`, and then each\n",
      " |              Fallback in order, upon failures.\n",
      " |\n",
      " |      Example:\n",
      " |          ```python\n",
      " |          from typing import Iterator\n",
      " |\n",
      " |          from langchain_core.runnables import RunnableGenerator\n",
      " |\n",
      " |\n",
      " |          def _generate_immediate_error(input: Iterator) -> Iterator[str]:\n",
      " |              raise ValueError()\n",
      " |              yield \"\"\n",
      " |\n",
      " |\n",
      " |          def _generate(input: Iterator) -> Iterator[str]:\n",
      " |              yield from \"foo bar\"\n",
      " |\n",
      " |\n",
      " |          runnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(\n",
      " |              [RunnableGenerator(_generate)]\n",
      " |          )\n",
      " |          print(\"\".join(runnable.stream({})))  # foo bar\n",
      " |          ```\n",
      " |\n",
      " |      Args:\n",
      " |          fallbacks: A sequence of runnables to try if the original `Runnable`\n",
      " |              fails.\n",
      " |          exceptions_to_handle: A tuple of exception types to handle.\n",
      " |          exception_key: If `string` is specified then handled exceptions will be\n",
      " |              passed to fallbacks as part of the input under the specified key.\n",
      " |\n",
      " |              If `None`, exceptions will not be passed to fallbacks.\n",
      " |\n",
      " |              If used, the base `Runnable` and its fallbacks must accept a\n",
      " |              dictionary as input.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new `Runnable` that will try the original `Runnable`, and then each\n",
      " |              Fallback in order, upon failures.\n",
      " |\n",
      " |  with_listeners(self, *, on_start: 'Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None' = None, on_end: 'Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None' = None, on_error: 'Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind lifecycle listeners to a `Runnable`, returning a new `Runnable`.\n",
      " |\n",
      " |      The Run object contains information about the run, including its `id`,\n",
      " |      `type`, `input`, `output`, `error`, `start_time`, `end_time`, and\n",
      " |      any tags or metadata added to the run.\n",
      " |\n",
      " |      Args:\n",
      " |          on_start: Called before the `Runnable` starts running, with the `Run`\n",
      " |              object.\n",
      " |          on_end: Called after the `Runnable` finishes running, with the `Run`\n",
      " |              object.\n",
      " |          on_error: Called if the `Runnable` throws an error, with the `Run`\n",
      " |              object.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new `Runnable` with the listeners bound.\n",
      " |\n",
      " |      Example:\n",
      " |          ```python\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |          from langchain_core.tracers.schemas import Run\n",
      " |\n",
      " |          import time\n",
      " |\n",
      " |\n",
      " |          def test_runnable(time_to_sleep: int):\n",
      " |              time.sleep(time_to_sleep)\n",
      " |\n",
      " |\n",
      " |          def fn_start(run_obj: Run):\n",
      " |              print(\"start_time:\", run_obj.start_time)\n",
      " |\n",
      " |\n",
      " |          def fn_end(run_obj: Run):\n",
      " |              print(\"end_time:\", run_obj.end_time)\n",
      " |\n",
      " |\n",
      " |          chain = RunnableLambda(test_runnable).with_listeners(\n",
      " |              on_start=fn_start, on_end=fn_end\n",
      " |          )\n",
      " |          chain.invoke(2)\n",
      " |          ```\n",
      " |\n",
      " |  with_retry(self, *, retry_if_exception_type: 'tuple[type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, exponential_jitter_params: 'ExponentialJitterParams | None' = None, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      " |      Create a new `Runnable` that retries the original `Runnable` on exceptions.\n",
      " |\n",
      " |      Args:\n",
      " |          retry_if_exception_type: A tuple of exception types to retry on.\n",
      " |          wait_exponential_jitter: Whether to add jitter to the wait\n",
      " |              time between retries.\n",
      " |          stop_after_attempt: The maximum number of attempts to make before\n",
      " |              giving up.\n",
      " |          exponential_jitter_params: Parameters for\n",
      " |              `tenacity.wait_exponential_jitter`. Namely: `initial`, `max`,\n",
      " |              `exp_base`, and `jitter` (all `float` values).\n",
      " |\n",
      " |      Returns:\n",
      " |          A new `Runnable` that retries the original `Runnable` on exceptions.\n",
      " |\n",
      " |      Example:\n",
      " |          ```python\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |          count = 0\n",
      " |\n",
      " |\n",
      " |          def _lambda(x: int) -> None:\n",
      " |              global count\n",
      " |              count = count + 1\n",
      " |              if x == 1:\n",
      " |                  raise ValueError(\"x is 1\")\n",
      " |              else:\n",
      " |                  pass\n",
      " |\n",
      " |\n",
      " |          runnable = RunnableLambda(_lambda)\n",
      " |          try:\n",
      " |              runnable.with_retry(\n",
      " |                  stop_after_attempt=2,\n",
      " |                  retry_if_exception_type=(ValueError,),\n",
      " |              ).invoke(1)\n",
      " |          except ValueError:\n",
      " |              pass\n",
      " |\n",
      " |          assert count == 2\n",
      " |          ```\n",
      " |\n",
      " |  with_types(self, *, input_type: 'type[Input] | None' = None, output_type: 'type[Output] | None' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind input and output types to a `Runnable`, returning a new `Runnable`.\n",
      " |\n",
      " |      Args:\n",
      " |          input_type: The input type to bind to the `Runnable`.\n",
      " |          output_type: The output type to bind to the `Runnable`.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new `Runnable` with the types bound.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.runnables.base.Runnable:\n",
      " |\n",
      " |  config_specs\n",
      " |      List configurable fields for this `Runnable`.\n",
      " |\n",
      " |  input_schema\n",
      " |      The type of input this `Runnable` accepts specified as a Pydantic model.\n",
      " |\n",
      " |  output_schema\n",
      " |      Output schema.\n",
      " |\n",
      " |      The type of output this `Runnable` produces specified as a Pydantic model.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from langchain_core.runnables.base.Runnable:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |\n",
      " |  __class_getitem__(...)\n",
      " |      Parameterizes a generic class.\n",
      " |\n",
      " |      At least, parameterizing a generic class is the *main* thing this\n",
      " |      method does. For example, for some generic class `Foo`, this is called\n",
      " |      when we do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      " |\n",
      " |      However, note that this method is also called when defining generic\n",
      " |      classes in the first place with `class Foo[T]: ...`.\n",
      " |\n",
      " |  __init_subclass__(...)\n",
      " |      Function to initialize subclasses.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(agent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c873e2",
   "metadata": {},
   "source": [
    "I did all the exploraton again , i want to run invoke methord i opened the documentation and read about invoke.It told me it gets a input this can be a dictonary on which the agent will run then there are other parameters that are for advanced control of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9316b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'messages': [HumanMessage(content='what is the weather in sf', additional_kwargs={}, response_metadata={}, id='169166c2-8645-41b8-ae48-2f6c61af65e3'), AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking for the weather in \"sf\". I need to figure out what \"sf\" stands for. Well, \"sf\" is a common abbreviation for San Francisco. So I should use the get_weather function and input the city as San Francisco. Let me check if there\\'s any other possible meaning for \"sf\", but I think in this context, it\\'s safe to assume they mean San Francisco. I\\'ll proceed with that.\\n', 'tool_calls': [{'id': 'xr4x774da', 'function': {'arguments': '{\"city\":\"San Francisco\"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 116, 'prompt_tokens': 158, 'total_tokens': 274, 'completion_time': 0.207793839, 'completion_tokens_details': {'reasoning_tokens': 91}, 'prompt_time': 0.008948965, 'prompt_tokens_details': None, 'queue_time': 0.067921524, 'total_time': 0.216742804}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bb240-e86a-76b2-9e5a-988f1307f426-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'xr4x774da', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 158, 'output_tokens': 116, 'total_tokens': 274, 'output_token_details': {'reasoning': 91}}), ToolMessage(content=\"It's always sunny in San Francisco!\", name='get_weather', id='f1a97a07-94f7-43b6-9911-4049dee121e4', tool_call_id='xr4x774da'), AIMessage(content='The weather in San Francisco (SF) is always sunny! 😊 Let me know if you need more details.', additional_kwargs={'reasoning_content': \"Okay, the user asked about the weather in SF, and I called the get_weather function with San Francisco as the city. The response came back saying it's always sunny there. Now I need to relay that information back to the user in a friendly way.\\n\\nFirst, I should confirm the city they asked about to make sure there's no confusion. Then, present the weather information clearly. Since the response is straightforward, I don't need to add much extra info. Maybe a smiley emoji to keep it light and positive. Let me check for any typos or formatting issues. Alright, that should do it.\\n\"}, response_metadata={'token_usage': {'completion_tokens': 152, 'prompt_tokens': 199, 'total_tokens': 351, 'completion_time': 0.312173611, 'completion_tokens_details': {'reasoning_tokens': 124}, 'prompt_time': 0.007875141, 'prompt_tokens_details': None, 'queue_time': 0.055871518, 'total_time': 0.320048752}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bb240-ead3-75d2-9a9e-723cd01f79ca-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 199, 'output_tokens': 152, 'total_tokens': 351, 'output_token_details': {'reasoning': 124}})]}\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n",
      "<class 'langchain_core.messages.tool.ToolMessage'>\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "agent_response=agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]})\n",
    "\n",
    "print(type(agent_response)) ## its a dictionary with a lot of key and value pairs\n",
    "print(agent_response) ## I explored what this dictionary contains and it has a lot of things from the agent thinking and evalutaiong to use the tools , the tool id , tool response etc\n",
    "print(type(agent_response['messages'][0])) ## the value of key message is a humanmessage object\n",
    "print(type(agent_response['messages'][2])) ## the value of key message is an toolmessage object\n",
    "print(type(agent_response['messages'][3])) ## the value of key message is an AIMessage object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3fc391ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what is the weather in sf\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_weather (mbsfvsen4)\n",
      " Call ID: mbsfvsen4\n",
      "  Args:\n",
      "    city: San Francisco\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_weather\n",
      "\n",
      "It's always sunny in San Francisco!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The weather in San Francisco (SF) is always sunny! 😊 Let me know if you need more details.\n"
     ]
    }
   ],
   "source": [
    "for message in agent_response['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c8dd9a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in San Francisco (SF) is always sunny! 😊 Let me know if you need more details.\n"
     ]
    }
   ],
   "source": [
    "print(agent_response['messages'][3].content) ## to acceess thing from that object i need to use . thingy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf8115e",
   "metadata": {},
   "source": [
    "If i was making some chatbot i would have printed this on the frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85304449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5de1dd68",
   "metadata": {},
   "source": [
    "## **Creating a real life agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cf683dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an expert weather forecaster, who speaks in puns.\n",
    "\n",
    "You have access to two tools:\n",
    "\n",
    "- get_weather_for_location: use this to get the weather for a specific location\n",
    "- get_user_location: use this to get the user's location\n",
    "\n",
    "If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4238719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "@tool\n",
    "def get_weather_for_location(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "@dataclass\n",
    "class Context:\n",
    "    \"\"\"Custom runtime context schema.\"\"\"\n",
    "    user_id: str\n",
    "\n",
    "@tool\n",
    "def get_user_location(runtime: ToolRuntime[Context]) -> str:\n",
    "    \"\"\"Retrieve user information based on user ID.\"\"\"\n",
    "    user_id = runtime.context.user_id\n",
    "    return \"Florida\" if user_id == \"1\" else \"SF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5e00d7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'function'>\n",
      "(cls=None, /, *, init=True, repr=True, eq=True, order=False, unsafe_hash=False, frozen=False, match_args=True, kw_only=False, slots=False, weakref_slot=False)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import inspect\n",
    "\n",
    "@dataclass\n",
    "class Context:\n",
    "    \"\"\"Custom runtime context schema.\"\"\"\n",
    "    user_id: str\n",
    "\n",
    "print(type(dataclass))\n",
    "print(inspect.signature(dataclass))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4bda2d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function dataclass in module dataclasses:\n",
      "\n",
      "dataclass(cls=None, /, *, init=True, repr=True, eq=True, order=False, unsafe_hash=False, frozen=False, match_args=True, kw_only=False, slots=False, weakref_slot=False)\n",
      "    Add dunder methods based on the fields defined in the class.\n",
      "\n",
      "    Examines PEP 526 __annotations__ to determine fields.\n",
      "\n",
      "    If init is true, an __init__() method is added to the class. If repr\n",
      "    is true, a __repr__() method is added. If order is true, rich\n",
      "    comparison dunder methods are added. If unsafe_hash is true, a\n",
      "    __hash__() method is added. If frozen is true, fields may not be\n",
      "    assigned to after instance creation. If match_args is true, the\n",
      "    __match_args__ tuple is added. If kw_only is true, then by default\n",
      "    all fields are keyword-only. If slots is true, a new class with a\n",
      "    __slots__ attribute is returned.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dataclass)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e9a02a",
   "metadata": {},
   "source": [
    "What does dataclass do this is a function that is used to creat boiler plate code for classes.With out this i have to define init methord by myself but this dataclass do that job for me.\\\n",
    "for exmaple without dataclass i will write like this.\\\n",
    "**class User:**\n",
    "\n",
    "    def __init__(self, name, age, active=True):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "        self.active = active\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"User(name={self.name}, age={self.age}, active={self.active})\"\n",
    "but with dataclass i will write like this.\\\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "**\"@dataclass\\n\"**\\\n",
    "**\"class User:**\\\n",
    "\n",
    "    name: str\n",
    "    age: int\n",
    "    active: bool = True\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d11e30e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context(user_id='1')\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "ctx= Context(user_id=\"1\")\n",
    "print(ctx)\n",
    "print(ctx.user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dcc79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'type'>\n",
      "(state: 'StateT', context: 'ContextT', config: 'RunnableConfig', stream_writer: 'StreamWriter', tool_call_id: 'str | None', store: 'BaseStore | None') -> None\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "@tool\n",
    "def get_weather_for_location(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "@tool\n",
    "def get_user_location(runtime: ToolRuntime[Context]) -> str:   ## tool define krte time runtime is a parameter for this function\n",
    "    \"\"\"Retreive user information based on user_id\"\"\"\n",
    "    user_id = runtime.context.user_id\n",
    "    return \"Florida\" if user_id == 1 else \"SF\"\n",
    "\n",
    "print(type(ToolRuntime))\n",
    "print(inspect.signature(ToolRuntime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a7ede12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ToolRuntime in module langgraph.prebuilt.tool_node:\n",
      "\n",
      "class ToolRuntime(langchain_core.tools.base._DirectlyInjectedToolArg, typing.Generic)\n",
      " |  ToolRuntime(state: 'StateT', context: 'ContextT', config: 'RunnableConfig', stream_writer: 'StreamWriter', tool_call_id: 'str | None', store: 'BaseStore | None') -> None\n",
      " |\n",
      " |  Runtime context automatically injected into tools.\n",
      " |\n",
      " |  When a tool function has a parameter named `tool_runtime` with type hint\n",
      " |  `ToolRuntime`, the tool execution system will automatically inject an instance\n",
      " |  containing:\n",
      " |\n",
      " |  - `state`: The current graph state\n",
      " |  - `tool_call_id`: The ID of the current tool call\n",
      " |  - `config`: `RunnableConfig` for the current execution\n",
      " |  - `context`: Runtime context (from langgraph `Runtime`)\n",
      " |  - `store`: `BaseStore` instance for persistent storage (from langgraph `Runtime`)\n",
      " |  - `stream_writer`: `StreamWriter` for streaming output (from langgraph `Runtime`)\n",
      " |\n",
      " |  No `Annotated` wrapper is needed - just use `runtime: ToolRuntime`\n",
      " |  as a parameter.\n",
      " |\n",
      " |  Example:\n",
      " |      ```python\n",
      " |      from langchain_core.tools import tool\n",
      " |      from langchain.tools import ToolRuntime\n",
      " |\n",
      " |      @tool\n",
      " |      def my_tool(x: int, runtime: ToolRuntime) -> str:\n",
      " |          \"\"\"Tool that accesses runtime context.\"\"\"\n",
      " |          # Access state\n",
      " |          messages = tool_runtime.state[\"messages\"]\n",
      " |\n",
      " |          # Access tool_call_id\n",
      " |          print(f\"Tool call ID: {tool_runtime.tool_call_id}\")\n",
      " |\n",
      " |          # Access config\n",
      " |          print(f\"Run ID: {tool_runtime.config.get('run_id')}\")\n",
      " |\n",
      " |          # Access runtime context\n",
      " |          user_id = tool_runtime.context.get(\"user_id\")\n",
      " |\n",
      " |          # Access store\n",
      " |          tool_runtime.store.put((\"metrics\",), \"count\", 1)\n",
      " |\n",
      " |          # Stream output\n",
      " |          tool_runtime.stream_writer.write(\"Processing...\")\n",
      " |\n",
      " |          return f\"Processed {x}\"\n",
      " |      ```\n",
      " |\n",
      " |  !!! note\n",
      " |      This is a marker class used for type checking and detection.\n",
      " |      The actual runtime object will be constructed during tool execution.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      ToolRuntime\n",
      " |      langchain_core.tools.base._DirectlyInjectedToolArg\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |\n",
      " |  __init__(self, state: 'StateT', context: 'ContextT', config: 'RunnableConfig', stream_writer: 'StreamWriter', tool_call_id: 'str | None', store: 'BaseStore | None') -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __annotations__ = {'config': 'RunnableConfig', 'context': 'ContextT', ...\n",
      " |\n",
      " |  __dataclass_fields__ = {'config': Field(name='config',type='RunnableCo...\n",
      " |\n",
      " |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      " |\n",
      " |  __hash__ = None\n",
      " |\n",
      " |  __match_args__ = ('state', 'context', 'config', 'stream_writer', 'tool...\n",
      " |\n",
      " |  __orig_bases__ = (<class 'langchain_core.tools.base._DirectlyInjectedT...\n",
      " |\n",
      " |  __parameters__ = (~ContextT, ~StateT)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from langchain_core.tools.base._DirectlyInjectedToolArg:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |\n",
      " |  __class_getitem__(...)\n",
      " |      Parameterizes a generic class.\n",
      " |\n",
      " |      At least, parameterizing a generic class is the *main* thing this\n",
      " |      method does. For example, for some generic class `Foo`, this is called\n",
      " |      when we do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      " |\n",
      " |      However, note that this method is also called when defining generic\n",
      " |      classes in the first place with `class Foo[T]: ...`.\n",
      " |\n",
      " |  __init_subclass__(...)\n",
      " |      Function to initialize subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ToolRuntime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17147ba",
   "metadata": {},
   "source": [
    "Why do we use ToolRuntime? As the name suggest this contains all the information that tool might need during running of the tool that are hidden from llms.example tool_call_id,config,context,it also help us inject databases during execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1a70bf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "# We use a dataclass here, but Pydantic models are also supported.\n",
    "@dataclass\n",
    "class ResponseFormat:\n",
    "    \"\"\"Response schema for the agent.\"\"\"\n",
    "    # A punny response (always required)\n",
    "    punny_response: str\n",
    "    # Any interesting information about the weather if available\n",
    "    weather_conditions: str | None = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "042a486f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langgraph.checkpoint.memory.InMemorySaver'>\n",
      "['__abstractmethods__', '__aenter__', '__aexit__', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_load_blobs', 'adelete_thread', 'aget', 'aget_tuple', 'alist', 'aput', 'aput_writes', 'blobs', 'config_specs', 'delete_thread', 'get', 'get_next_version', 'get_tuple', 'list', 'put', 'put_writes', 'serde', 'stack', 'storage', 'writes']\n"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "print(type(checkpointer))\n",
    "print(dir(checkpointer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dbb04929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "['__class__', '__class_getitem__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__ior__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__or__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__ror__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'clear', 'copy', 'fromkeys', 'get', 'items', 'keys', 'pop', 'popitem', 'setdefault', 'update', 'values']\n",
      "{'messages': [HumanMessage(content='what is the weather outside?', additional_kwargs={}, response_metadata={}, id='43acee6c-e5d6-408e-9e1a-2d35c4bb6b83'), AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking, \"what is the weather outside?\" They probably want to know the current conditions at their location. Since they didn\\'t specify a city, I should use the get_user_location tool to find out where they are. Once I have their location, I can then call get_weather_for_location with their city. Let me check if I have the right tools. Yep, get_user_location doesn\\'t need any parameters, so I\\'ll start with that. Then, after getting their city, I\\'ll fetch the weather. I need to make sure I follow the tool calls properly and then format the response with a punny message and the weather details.\\n', 'tool_calls': [{'id': 'nt68h9n6g', 'function': {'arguments': '{}', 'name': 'get_user_location'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 155, 'prompt_tokens': 376, 'total_tokens': 531, 'completion_time': 0.314629422, 'completion_tokens_details': {'reasoning_tokens': 134}, 'prompt_time': 0.016280293, 'prompt_tokens_details': None, 'queue_time': 0.0562925, 'total_time': 0.330909715}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bb619-9489-7f62-85be-1c300bb3dcde-0', tool_calls=[{'name': 'get_user_location', 'args': {}, 'id': 'nt68h9n6g', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 376, 'output_tokens': 155, 'total_tokens': 531, 'output_token_details': {'reasoning': 134}}), ToolMessage(content='SF', name='get_user_location', id='55bee69c-db7a-49a6-b2bb-e925b2b513e8', tool_call_id='nt68h9n6g'), AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user asked, \"what is the weather outside?\" and I used the get_user_location tool to find out they\\'re in SF. Now I need to get the weather for San Francisco. Let me call the get_weather_for_location function with the city parameter set to \"SF\". Wait, should it be \"San Francisco\" instead? The tool might expect the full city name. But the user\\'s location was given as \"SF\", so maybe that\\'s the correct parameter. I\\'ll proceed with \"SF\" and see. If there\\'s an error, I can adjust. Once I get the weather data, I\\'ll need to craft a punny response. Let\\'s make sure to include the weather conditions in the response using the ResponseFormat function. Alright, let\\'s make the call.\\n', 'tool_calls': [{'id': '7qa25tv56', 'function': {'arguments': '{\"city\":\"San Francisco\"}', 'name': 'get_weather_for_location'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 187, 'prompt_tokens': 407, 'total_tokens': 594, 'completion_time': 0.365811256, 'completion_tokens_details': {'reasoning_tokens': 160}, 'prompt_time': 0.017835429, 'prompt_tokens_details': None, 'queue_time': 0.057321944, 'total_time': 0.383646685}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bb619-96b9-7e71-adde-60b94ccbcf58-0', tool_calls=[{'name': 'get_weather_for_location', 'args': {'city': 'San Francisco'}, 'id': '7qa25tv56', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 407, 'output_tokens': 187, 'total_tokens': 594, 'output_token_details': {'reasoning': 160}}), ToolMessage(content=\"It's always sunny in San Francisco!\", name='get_weather_for_location', id='30b8fb52-b2da-4d0e-9c86-837c4280cdba', tool_call_id='7qa25tv56'), HumanMessage(content='what is the weather outside?', additional_kwargs={}, response_metadata={}, id='c0b820e7-b248-44d3-b873-0963462dd332'), AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user asked, \"what is the weather outside?\" again. Let me check the history. Previously, they asked the same question, and I used get_user_location to find they were in San Francisco. Then I called get_weather_for_location for San Francisco and got the response \"It\\'s always sunny in San Francisco!\" Now, they\\'re asking again. Since their location hasn\\'t changed, I can reuse the previous weather info. But wait, maybe the weather changed? Hmm, but the tool response was a fixed statement. Since the user is still in SF, and the tool\\'s response is the same, I should just repeat the punny answer. Let me make sure I follow the format. The response should include the punny_response and weather_conditions. The weather_conditions here is the tool\\'s response. Wait, the tool\\'s response was \"It\\'s always sunny in San Francisco!\" So I need to structure the ResponseFormat with that. But the user\\'s current query doesn\\'t mention a location, so I need to confirm if they\\'re still in SF. Wait, no, the user hasn\\'t provided a new location. The previous get_user_location was used, and the response was SF. Since the user is the same, maybe the location is still SF. So I can proceed to use the previous weather data. Therefore, the answer should be the punny response based on the weather. Since it\\'s sunny, maybe a sun-related pun. Let me think... \"Looks like the sun\\'s out, so you can shine too! It\\'s sunny in San Francisco!\" Then include the weather_conditions as \"It\\'s always sunny in San Francisco!\" in the ResponseFormat.\\n', 'tool_calls': [{'id': 'y8v75tmt6', 'function': {'arguments': '{\"punny_response\":\"Looks like the sun\\'s out, so you can shine too! It\\'s sunny in San Francisco!\",\"weather_conditions\":\"It\\'s always sunny in San Francisco!\"}', 'name': 'ResponseFormat'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 396, 'prompt_tokens': 461, 'total_tokens': 857, 'completion_time': 0.861617176, 'completion_tokens_details': {'reasoning_tokens': 338}, 'prompt_time': 0.02094622, 'prompt_tokens_details': None, 'queue_time': 0.052925203, 'total_time': 0.882563396}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bb61a-4be6-7da2-9738-077ac76dc979-0', tool_calls=[{'name': 'ResponseFormat', 'args': {'punny_response': \"Looks like the sun's out, so you can shine too! It's sunny in San Francisco!\", 'weather_conditions': \"It's always sunny in San Francisco!\"}, 'id': 'y8v75tmt6', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 461, 'output_tokens': 396, 'total_tokens': 857, 'output_token_details': {'reasoning': 338}}), ToolMessage(content='Returning structured response: ResponseFormat(punny_response=\"Looks like the sun\\'s out, so you can shine too! It\\'s sunny in San Francisco!\", weather_conditions=\"It\\'s always sunny in San Francisco!\")', name='ResponseFormat', id='8ba810c2-42ab-4709-ba19-36c670ab566c', tool_call_id='y8v75tmt6'), HumanMessage(content='what is the weather outside?', additional_kwargs={}, response_metadata={}, id='ecf82c55-78ba-4c2f-bc7c-3bc450e50006'), AIMessage(content='', additional_kwargs={'reasoning_content': \"Okay, the user is asking again about the weather outside. Let me check the history. Previously, they asked, and I used get_user_location to find they were in San Francisco. Then I called get_weather_for_location for San Francisco and got the response that it's always sunny there. The next step was formatting the response with a pun, which was successful.\\n\\nNow, the user is asking the same question again. Since the last interaction resulted in a successful response, and there's no indication that the user has moved or the weather has changed, I can assume the weather is still sunny in San Francisco. I should reuse the previous response to maintain consistency and avoid unnecessary API calls. The user might just want a quick confirmation without any new information. So, I'll use the ResponseFormat function again with the same punny response and weather conditions as before. That should cover it efficiently.\\n\", 'tool_calls': [{'id': 'zp9twbkh0', 'function': {'arguments': '{\"punny_response\":\"Looks like the sun\\'s out, so you can shine too! It\\'s sunny in San Francisco!\",\"weather_conditions\":\"It\\'s always sunny in San Francisco!\"}', 'name': 'ResponseFormat'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 236, 'prompt_tokens': 579, 'total_tokens': 815, 'completion_time': 0.460767163, 'completion_tokens_details': {'reasoning_tokens': 178}, 'prompt_time': 0.026864876, 'prompt_tokens_details': None, 'queue_time': 0.066844214, 'total_time': 0.487632039}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bb66e-0655-7f70-b378-73d12d8d9994-0', tool_calls=[{'name': 'ResponseFormat', 'args': {'punny_response': \"Looks like the sun's out, so you can shine too! It's sunny in San Francisco!\", 'weather_conditions': \"It's always sunny in San Francisco!\"}, 'id': 'zp9twbkh0', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 579, 'output_tokens': 236, 'total_tokens': 815, 'output_token_details': {'reasoning': 178}}), ToolMessage(content='Returning structured response: ResponseFormat(punny_response=\"Looks like the sun\\'s out, so you can shine too! It\\'s sunny in San Francisco!\", weather_conditions=\"It\\'s always sunny in San Francisco!\")', name='ResponseFormat', id='49ccfc86-5bb1-4eef-adb8-300d5286a382', tool_call_id='zp9twbkh0'), HumanMessage(content='what is the weather outside?', additional_kwargs={}, response_metadata={}, id='cc69b38b-d7fc-44f2-8c4c-01b796b68b46'), AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking \"what is the weather outside?\" again. Let me check the history.\\n\\nPreviously, they asked the same question, and I used get_user_location to find they\\'re in San Francisco. Then I called get_weather_for_location for San Francisco and got the response \"It\\'s always sunny in San Francisco!\" Each time they ask, I provide the same punny response. Since the weather hasn\\'t changed, I should just return the same structured response again. No need to call tools again because the information is the same as before. So, use ResponseFormat with the punny message and weather conditions as before.\\n', 'tool_calls': [{'id': 't5c66dasq', 'function': {'arguments': '{\"punny_response\":\"Looks like the sun\\'s out, so you can shine too! It\\'s sunny in San Francisco!\",\"weather_conditions\":\"It\\'s always sunny in San Francisco!\"}', 'name': 'ResponseFormat'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 185, 'prompt_tokens': 697, 'total_tokens': 882, 'completion_time': 0.353046382, 'completion_tokens_details': {'reasoning_tokens': 127}, 'prompt_time': 0.031670293, 'prompt_tokens_details': None, 'queue_time': 0.056176407, 'total_time': 0.384716675}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bb66e-2a35-7ac0-ac4d-c58ea71ccc40-0', tool_calls=[{'name': 'ResponseFormat', 'args': {'punny_response': \"Looks like the sun's out, so you can shine too! It's sunny in San Francisco!\", 'weather_conditions': \"It's always sunny in San Francisco!\"}, 'id': 't5c66dasq', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 697, 'output_tokens': 185, 'total_tokens': 882, 'output_token_details': {'reasoning': 127}}), ToolMessage(content='Returning structured response: ResponseFormat(punny_response=\"Looks like the sun\\'s out, so you can shine too! It\\'s sunny in San Francisco!\", weather_conditions=\"It\\'s always sunny in San Francisco!\")', name='ResponseFormat', id='3e17796e-9bbe-499d-ba4d-886c04ec60b1', tool_call_id='t5c66dasq'), HumanMessage(content='what is the weather outside?', additional_kwargs={}, response_metadata={}, id='3d5da1a8-82a5-4482-846d-ab91a8d8a354'), AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking \"what is the weather outside?\" again. Let me check the history. They first asked this, and I used get_user_location to find they\\'re in San Francisco. Then I called get_weather_for_location for San Francisco and got the response that it\\'s always sunny there. I formatted the answer with a pun, mentioning the sun shining and them shining too.\\n\\nNow, they\\'re asking the same question again. The previous responses were structured using ResponseFormat with the same punny message. Since the weather in San Francisco is reported as always sunny, there\\'s no new data here. The user might be testing if I remember the location or just wants a consistent answer. \\n\\nI need to make sure I don\\'t call any functions unnecessarily. Since the weather data hasn\\'t changed, I can reuse the previous response. The tool calls are consistent, so I should just return the same structured response again. No need to fetch the location or weather again because the user hasn\\'t indicated any change. So, the best approach is to repeat the same punny response with the weather conditions as before.\\n', 'tool_calls': [{'id': 'j1zfyrnr1', 'function': {'arguments': '{\"punny_response\":\"Looks like the sun\\'s out, so you can shine too! It\\'s sunny in San Francisco!\",\"weather_conditions\":\"It\\'s always sunny in San Francisco!\"}', 'name': 'ResponseFormat'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 281, 'prompt_tokens': 815, 'total_tokens': 1096, 'completion_time': 0.577777615, 'completion_tokens_details': {'reasoning_tokens': 223}, 'prompt_time': 0.028715864, 'prompt_tokens_details': {'cached_tokens': 512}, 'queue_time': 0.046845436, 'total_time': 0.606493479}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bb66e-79d4-7283-82fa-9f80bb7a63f9-0', tool_calls=[{'name': 'ResponseFormat', 'args': {'punny_response': \"Looks like the sun's out, so you can shine too! It's sunny in San Francisco!\", 'weather_conditions': \"It's always sunny in San Francisco!\"}, 'id': 'j1zfyrnr1', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 815, 'output_tokens': 281, 'total_tokens': 1096, 'input_token_details': {'cache_read': 512}, 'output_token_details': {'reasoning': 223}}), ToolMessage(content='Returning structured response: ResponseFormat(punny_response=\"Looks like the sun\\'s out, so you can shine too! It\\'s sunny in San Francisco!\", weather_conditions=\"It\\'s always sunny in San Francisco!\")', name='ResponseFormat', id='229bc379-f6e5-4b58-80ff-ae3a775fd1e6', tool_call_id='j1zfyrnr1'), HumanMessage(content='thank you!', additional_kwargs={}, response_metadata={}, id='1a6441e7-b68b-43e7-9f0b-b968afb65f72'), HumanMessage(content='what is the weather outside?', additional_kwargs={}, response_metadata={}, id='e9594272-c7d7-453c-b2da-69e563d5be34'), AIMessage(content='', additional_kwargs={'reasoning_content': \"Okay, the user is asking for the weather outside again. Let me check the history. Previously, they asked and I determined their location was San Francisco using get_user_location. Then I used get_weather_for_location for San Francisco and got the response that it's always sunny there. Since then, the user has asked the same question multiple times, and each time I provided the same punny response about San Francisco being sunny.\\n\\nNow, the user is asking again. Since their location hasn't changed (they're still in San Francisco as per the last interaction), and the weather data hasn't changed either (still sunny), I should repeat the same response. The tools available don't require any new function calls here because the information is already cached from previous interactions. So, I'll use the ResponseFormat function again with the same punny message and weather conditions. No need to call get_user_location or get_weather_for_location again unless there's an indication that the location or weather has changed, which there isn't. So, just structure the response as before.\\n\", 'tool_calls': [{'id': 'c8agg9wv2', 'function': {'arguments': '{\"punny_response\":\"Looks like the sun\\'s out, so you can shine too! It\\'s sunny in San Francisco!\",\"weather_conditions\":\"It\\'s always sunny in San Francisco!\"}', 'name': 'ResponseFormat'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 270, 'prompt_tokens': 946, 'total_tokens': 1216, 'completion_time': 0.538338447, 'completion_tokens_details': {'reasoning_tokens': 212}, 'prompt_time': 0.047364215, 'prompt_tokens_details': None, 'queue_time': 0.159382495, 'total_time': 0.585702662}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bb66f-b46e-7283-8a3c-9b6e82d43720-0', tool_calls=[{'name': 'ResponseFormat', 'args': {'punny_response': \"Looks like the sun's out, so you can shine too! It's sunny in San Francisco!\", 'weather_conditions': \"It's always sunny in San Francisco!\"}, 'id': 'c8agg9wv2', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 946, 'output_tokens': 270, 'total_tokens': 1216, 'output_token_details': {'reasoning': 212}}), ToolMessage(content='Returning structured response: ResponseFormat(punny_response=\"Looks like the sun\\'s out, so you can shine too! It\\'s sunny in San Francisco!\", weather_conditions=\"It\\'s always sunny in San Francisco!\")', name='ResponseFormat', id='3239cb8f-26a4-4172-8cfc-30ac6017464a', tool_call_id='c8agg9wv2'), HumanMessage(content='thank you!', additional_kwargs={}, response_metadata={}, id='8424176f-7f6a-4fdc-8a07-cf032341d1b3'), HumanMessage(content='thank you!', additional_kwargs={}, response_metadata={}, id='b310dec2-4107-4f9e-b6c0-b0066c0a1df5'), HumanMessage(content='what is the weather outside?', additional_kwargs={}, response_metadata={}, id='1c8b34d2-8433-4a67-9248-3973581f9a2a'), AIMessage(content='', additional_kwargs={'reasoning_content': \"Okay, the user is asking for the weather outside again. Let me check the history. They've asked this multiple times, and each time I provided the same response about San Francisco being sunny. Since their location is set to SF, and the tool responses keep saying it's always sunny there, I should stick with that. No need to call any functions again because the information hasn't changed. Just repeat the punny response and weather conditions as before.\\n\", 'tool_calls': [{'id': 'ryh66cx8r', 'function': {'arguments': '{\"punny_response\":\"Looks like the sun\\'s out, so you can shine too! It\\'s sunny in San Francisco!\",\"weather_conditions\":\"It\\'s always sunny in San Francisco!\"}', 'name': 'ResponseFormat'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 149, 'prompt_tokens': 1090, 'total_tokens': 1239, 'completion_time': 0.256952757, 'completion_tokens_details': {'reasoning_tokens': 91}, 'prompt_time': 0.051836985, 'prompt_tokens_details': None, 'queue_time': 0.173243984, 'total_time': 0.308789742}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bb675-0d37-74d0-84ae-a669e9828c5a-0', tool_calls=[{'name': 'ResponseFormat', 'args': {'punny_response': \"Looks like the sun's out, so you can shine too! It's sunny in San Francisco!\", 'weather_conditions': \"It's always sunny in San Francisco!\"}, 'id': 'ryh66cx8r', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 1090, 'output_tokens': 149, 'total_tokens': 1239, 'output_token_details': {'reasoning': 91}}), ToolMessage(content='Returning structured response: ResponseFormat(punny_response=\"Looks like the sun\\'s out, so you can shine too! It\\'s sunny in San Francisco!\", weather_conditions=\"It\\'s always sunny in San Francisco!\")', name='ResponseFormat', id='1dc4d801-dab7-416f-9069-186c16ce57de', tool_call_id='ryh66cx8r'), HumanMessage(content='thank you!', additional_kwargs={}, response_metadata={}, id='fee7b1a4-6ec7-42e1-9dbb-322aa1fa8b19'), HumanMessage(content='what is the weather outside?', additional_kwargs={}, response_metadata={}, id='20753a39-e3f9-4e83-b166-f11b8066b7b3'), AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking for the weather outside again. Let me check the history. They\\'ve been asking repeatedly, and each time I\\'ve been using the ResponseFormat tool with the same message about San Francisco being sunny. The user\\'s location was determined as San Francisco earlier, and the tool response said it\\'s always sunny there.\\n\\nWait, the user just said \"thank you!\" after the last response. Maybe they\\'re acknowledging the previous answer. But now they\\'re asking again. Since the weather hasn\\'t changed, I should probably give the same response. But maybe I should check if there\\'s a need to refresh the data? However, the tools available don\\'t have a function to get real-time updates unless the user specifies a new location or there\\'s a change. \\n\\nThe user hasn\\'t provided any new information, so the location is still San Francisco. The previous tool calls used get_user_location and then get_weather_for_location. But after that, all subsequent responses used ResponseFormat with the cached data. Since the user keeps asking, maybe they want the same info, so repeating the punny response is okay. \\n\\nI need to make sure I\\'m using the correct tool. The ResponseFormat requires a punny_response and optionally weather_conditions. The last correct response was \"Looks like the sun\\'s out, so you can shine too! It\\'s sunny in San Francisco!\" with the weather_conditions from the tool response. \\n\\nNo errors in the previous steps. Just need to stick with the same answer since there\\'s no new data. The user might be testing if the response changes, but according to the tools, it\\'s always sunny in SF here. So the next step is to call ResponseFormat again with the same parameters.\\n', 'tool_calls': [{'id': 'tb02k71vm', 'function': {'arguments': '{\"punny_response\":\"Looks like the sun\\'s out, so you can shine too! It\\'s sunny in San Francisco!\",\"weather_conditions\":\"It\\'s always sunny in San Francisco!\"}', 'name': 'ResponseFormat'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 406, 'prompt_tokens': 1221, 'total_tokens': 1627, 'completion_time': 0.881627682, 'completion_tokens_details': {'reasoning_tokens': 348}, 'prompt_time': 0.053986188, 'prompt_tokens_details': None, 'queue_time': 0.158889422, 'total_time': 0.93561387}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bb675-485b-7c41-a40a-870116a28f02-0', tool_calls=[{'name': 'ResponseFormat', 'args': {'punny_response': \"Looks like the sun's out, so you can shine too! It's sunny in San Francisco!\", 'weather_conditions': \"It's always sunny in San Francisco!\"}, 'id': 'tb02k71vm', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 1221, 'output_tokens': 406, 'total_tokens': 1627, 'output_token_details': {'reasoning': 348}}), ToolMessage(content='Returning structured response: ResponseFormat(punny_response=\"Looks like the sun\\'s out, so you can shine too! It\\'s sunny in San Francisco!\", weather_conditions=\"It\\'s always sunny in San Francisco!\")', name='ResponseFormat', id='a5c9106a-5bca-4717-aa7a-d112d863f5ca', tool_call_id='tb02k71vm'), HumanMessage(content='thank you!', additional_kwargs={}, response_metadata={}, id='be989dfb-dd6f-41d2-b938-435d62d4123d'), HumanMessage(content='what is the weather outside?', additional_kwargs={}, response_metadata={}, id='b2d66306-cf43-4e03-a5e0-c21eaaea8722'), AIMessage(content='', additional_kwargs={'reasoning_content': \"Okay, the user is asking for the weather outside again. Let me check the history. They've been asking repeatedly, and each time I've been using the ResponseFormat tool with the same message about San Francisco being sunny. The user keeps thanking me, so maybe they just want confirmation again.\\n\\nWait, the user's location was determined as San Francisco earlier. The tool responses keep saying it's always sunny there. But maybe the user wants more details or a different pun? Or perhaps they're testing if the response changes. Since the tool's response is static, I need to stick with the same answer. Let me make sure to use the same punny response again. No need to call any other tools since the info hasn't changed. Just return the standard ResponseFormat with the sunny message.\\n\", 'tool_calls': [{'id': 'r7n5xbwwh', 'function': {'arguments': '{\"punny_response\":\"Looks like the sun\\'s out, so you can shine too! It\\'s sunny in San Francisco!\",\"weather_conditions\":\"It\\'s always sunny in San Francisco!\"}', 'name': 'ResponseFormat'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 218, 'prompt_tokens': 1352, 'total_tokens': 1570, 'completion_time': 0.479320466, 'completion_tokens_details': {'reasoning_tokens': 160}, 'prompt_time': 0.060854844, 'prompt_tokens_details': None, 'queue_time': 0.051811106, 'total_time': 0.54017531}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bb68b-7572-7851-bb9b-f054d0b9f79d-0', tool_calls=[{'name': 'ResponseFormat', 'args': {'punny_response': \"Looks like the sun's out, so you can shine too! It's sunny in San Francisco!\", 'weather_conditions': \"It's always sunny in San Francisco!\"}, 'id': 'r7n5xbwwh', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 1352, 'output_tokens': 218, 'total_tokens': 1570, 'output_token_details': {'reasoning': 160}}), ToolMessage(content='Returning structured response: ResponseFormat(punny_response=\"Looks like the sun\\'s out, so you can shine too! It\\'s sunny in San Francisco!\", weather_conditions=\"It\\'s always sunny in San Francisco!\")', name='ResponseFormat', id='e147c99c-179e-4fbe-9d74-9dc3edae4ec5', tool_call_id='r7n5xbwwh')], 'structured_response': ResponseFormat(punny_response=\"Looks like the sun's out, so you can shine too! It's sunny in San Francisco!\", weather_conditions=\"It's always sunny in San Francisco!\")}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents.structured_output import ToolStrategy\n",
    "\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    tools=[get_user_location, get_weather_for_location],\n",
    "    context_schema=Context,\n",
    "    middleware=[handle_tool_errors],\n",
    "    response_format=ToolStrategy(ResponseFormat),\n",
    "    checkpointer=checkpointer\n",
    ")\n",
    "\n",
    "# `thread_id` is a unique identifier for a given conversation.\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather outside?\"}]},\n",
    "    config=config,\n",
    "    context=Context(user_id=\"1\")\n",
    ")\n",
    "print(type(response))\n",
    "print(dir(response))\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d9246933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6c215ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import wrap_tool_call\n",
    "from langchain.messages import ToolMessage\n",
    "\n",
    "\n",
    "@wrap_tool_call\n",
    "def handle_tool_errors(request, handler):\n",
    "    \"\"\"Handle tool execution errors with custom messages.\"\"\"\n",
    "    try:\n",
    "        return handler(request)\n",
    "    except Exception as e:\n",
    "        # Return a custom error message to the model\n",
    "        return ToolMessage(\n",
    "            content=f\"Tool error: Please check your input and try again. ({str(e)})\",\n",
    "            tool_call_id=request.tool_call[\"id\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "41576c85",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': \"You're welcome! If you need more sunshine or a伞 (umbrella) for shade, I'm here for you! ☀️🌧️\"}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthank you!\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ARJIT SHUKLA\\Desktop\\IIT\\ML Books\\.venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:3068\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3065\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3066\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3068\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3069\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ARJIT SHUKLA\\Desktop\\IIT\\ML Books\\.venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:2643\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2641\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2642\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2643\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2644\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2645\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2653\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ARJIT SHUKLA\\Desktop\\IIT\\ML Books\\.venv\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ARJIT SHUKLA\\Desktop\\IIT\\ML Books\\.venv\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ARJIT SHUKLA\\Desktop\\IIT\\ML Books\\.venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ARJIT SHUKLA\\Desktop\\IIT\\ML Books\\.venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ARJIT SHUKLA\\Desktop\\IIT\\ML Books\\.venv\\Lib\\site-packages\\langchain\\agents\\factory.py:1130\u001b[39m, in \u001b[36mcreate_agent.<locals>.model_node\u001b[39m\u001b[34m(state, runtime)\u001b[39m\n\u001b[32m   1117\u001b[39m request = ModelRequest(\n\u001b[32m   1118\u001b[39m     model=model,\n\u001b[32m   1119\u001b[39m     tools=default_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1125\u001b[39m     runtime=runtime,\n\u001b[32m   1126\u001b[39m )\n\u001b[32m   1128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m wrap_model_call_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1129\u001b[39m     \u001b[38;5;66;03m# No handlers - execute directly\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1130\u001b[39m     response = \u001b[43m_execute_model_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1132\u001b[39m     \u001b[38;5;66;03m# Call composed handler with base handler\u001b[39;00m\n\u001b[32m   1133\u001b[39m     response = wrap_model_call_handler(request, _execute_model_sync)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ARJIT SHUKLA\\Desktop\\IIT\\ML Books\\.venv\\Lib\\site-packages\\langchain\\agents\\factory.py:1101\u001b[39m, in \u001b[36mcreate_agent.<locals>._execute_model_sync\u001b[39m\u001b[34m(request)\u001b[39m\n\u001b[32m   1098\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request.system_message:\n\u001b[32m   1099\u001b[39m     messages = [request.system_message, *messages]\n\u001b[32m-> \u001b[39m\u001b[32m1101\u001b[39m output = \u001b[43mmodel_\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name:\n\u001b[32m   1103\u001b[39m     output.name = name\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ARJIT SHUKLA\\Desktop\\IIT\\ML Books\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5557\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5550\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5551\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5552\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5555\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5556\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5557\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5558\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5559\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5560\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5561\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ARJIT SHUKLA\\Desktop\\IIT\\ML Books\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:402\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m     **kwargs: Any,\n\u001b[32m    396\u001b[39m ) -> AIMessage:\n\u001b[32m    397\u001b[39m     config = ensure_config(config)\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    400\u001b[39m         cast(\n\u001b[32m    401\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    412\u001b[39m         ).message,\n\u001b[32m    413\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ARJIT SHUKLA\\Desktop\\IIT\\ML Books\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1121\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1114\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1118\u001b[39m     **kwargs: Any,\n\u001b[32m   1119\u001b[39m ) -> LLMResult:\n\u001b[32m   1120\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ARJIT SHUKLA\\Desktop\\IIT\\ML Books\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:931\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    930\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    937\u001b[39m         )\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    939\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ARJIT SHUKLA\\Desktop\\IIT\\ML Books\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1233\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1231\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1232\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1237\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ARJIT SHUKLA\\Desktop\\IIT\\ML Books\\.venv\\Lib\\site-packages\\langchain_groq\\chat_models.py:593\u001b[39m, in \u001b[36mChatGroq._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    588\u001b[39m message_dicts, params = \u001b[38;5;28mself\u001b[39m._create_message_dicts(messages, stop)\n\u001b[32m    589\u001b[39m params = {\n\u001b[32m    590\u001b[39m     **params,\n\u001b[32m    591\u001b[39m     **kwargs,\n\u001b[32m    592\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ARJIT SHUKLA\\Desktop\\IIT\\ML Books\\.venv\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:461\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    242\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    243\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    300\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m    301\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    302\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    303\u001b[39m \u001b[33;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[32m    304\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    459\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    460\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/openai/v1/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcitation_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcitation_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompound_custom\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompound_custom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdisable_tool_validation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_tool_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_reasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_reasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msearch_settings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ARJIT SHUKLA\\Desktop\\IIT\\ML Books\\.venv\\Lib\\site-packages\\groq\\_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ARJIT SHUKLA\\Desktop\\IIT\\ML Books\\.venv\\Lib\\site-packages\\groq\\_base_client.py:1044\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1041\u001b[39m             err.response.read()\n\u001b[32m   1043\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1046\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': \"You're welcome! If you need more sunshine or a伞 (umbrella) for shade, I'm here for you! ☀️🌧️\"}}",
      "During task with name 'model' and id '0ab1c83e-268e-124c-0adb-65afff3bc4e5'"
     ]
    }
   ],
   "source": [
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]},\n",
    "    config=config,\n",
    "    context=Context(user_id=\"1\")\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c99e5e",
   "metadata": {},
   "source": [
    "This error due to not updation of langchain documentaion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe6736e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
